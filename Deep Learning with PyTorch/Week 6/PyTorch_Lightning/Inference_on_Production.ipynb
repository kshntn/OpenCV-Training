{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rD2rpKceMM0z",
    "toc": true
   },
   "source": [
    "<h1>Inference on Production<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Lightning-Module\" data-toc-modified-id=\"Lightning-Module-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Lightning Module</a></span></li><li><span><a href=\"#Get-the-Checkpoint\" data-toc-modified-id=\"Get-the-Checkpoint-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Get the Checkpoint</a></span></li><li><span><a href=\"#Convert-to-ONNX-Format\" data-toc-modified-id=\"Convert-to-ONNX-Format-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Convert to ONNX Format</a></span></li><li><span><a href=\"#Sample-Inference\" data-toc-modified-id=\"Sample-Inference-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Sample Inference</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIHzzFQoMM00"
   },
   "source": [
    "1. Assume your team is working on a project where you need to work on some ML problems.\n",
    "2. What if you pick one issue and solve it with the help of the PyTorch framework while your colleague does the same, using Tensorflow.\n",
    "3. Both problems we know are part of a bigger project. Now, how to arrive at a common format to share the ML models.\n",
    "\n",
    "\n",
    "<a target=\"_blank\" href=\"https://onnx.ai/\">ONNX: Open Neural Network Exchange</a> is one such open format that allows model interchange between various <a target=\"_blank\" href=\"https://onnx.ai/supported-tools\">ML frameworks and tools</a>.\n",
    "\n",
    "\n",
    "**In this notebook, we will see how to convert a PyTorch Lightning saved checkpoint to the ONNX model.  Let's take the example of the checkpoint saved by the notebook on MNIST training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install onnxruntime onnx -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AGnsJmC6MM01"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy\n",
    "from torchmetrics import MeanMetric\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # filter UserWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kS3O4IKUMM01"
   },
   "source": [
    "## Lightning Module\n",
    "\n",
    "The Lighting module gives us the model definition to load a model from checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Cevo_53HMM01"
   },
   "outputs": [],
   "source": [
    "class LeNet5(pl.LightningModule):  # here nn.Module is replaced by LightningModule\n",
    "    def __init__(self, learning_rate=0.01, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save the arguments as hyperparameters.\n",
    "        self.save_hyperparameters()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2-d\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weights in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=16 * 5 * 5, out_features=120),\n",
    "\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120\n",
    "            nn.Linear(in_features=120, out_features=84),\n",
    "\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Third fully connected layer. It is also the output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=84, out_features=self.num_classes))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weights_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OrabAEyMM02"
   },
   "source": [
    "## Get the Checkpoint\n",
    "\n",
    "We are going to load one of the checkpoints saved during the last training.\n",
    "\n",
    "We have written a helper function for it. It takes the log directory of PyTorch Lighting training and runs the version number to return the corresponding `.ckpt` path.\n",
    "\n",
    "Youâ€™ll be familiar with  this function from the last unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ht9ZQKynMM02"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_latest_run_version_ckpt_epoch_no(lightning_logs_dir='lightning_logs', run_version=None):\n",
    "    if run_version is None:\n",
    "        run_version = 0\n",
    "        for dir_name in os.listdir(lightning_logs_dir):\n",
    "            if 'version' in dir_name:\n",
    "                if int(dir_name.split('_')[1]) > run_version:\n",
    "                    run_version = int(dir_name.split('_')[1])\n",
    "\n",
    "    checkpoints_dir = os.path.join(lightning_logs_dir, 'version_{}'.format(run_version), 'checkpoints')\n",
    "\n",
    "    files = os.listdir(checkpoints_dir)\n",
    "    ckpt_filename = None\n",
    "    for file in files:\n",
    "        if file.endswith('.ckpt'):\n",
    "            ckpt_filename = file\n",
    "\n",
    "    if ckpt_filename is not None:\n",
    "        ckpt_path = os.path.join(checkpoints_dir, ckpt_filename)\n",
    "    else:\n",
    "        print('CKPT file is not present')\n",
    "\n",
    "    return ckpt_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba_V0txyMM03"
   },
   "source": [
    "**Let us get the `.ckpt` model path.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9m2E3eszMM03",
    "outputId": "7d8c46c5-b373-440e-ff8e-13fb18251178"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt_path: lightning_logs/version_0/checkpoints/ckpt_009.ckpt\n"
     ]
    }
   ],
   "source": [
    "# get checkpoint path\n",
    "ckpt_path = get_latest_run_version_ckpt_epoch_no(run_version=0)\n",
    "print('ckpt_path: {}'.format(ckpt_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTUKqHM8MM03"
   },
   "source": [
    "## Convert to ONNX Format\n",
    "\n",
    "We have written a function to convert `.ckpt` model to `.onnx` model.\n",
    "\n",
    "The function takes the model definition, `.ckpt` path, and the `.onnx` file path as arguments. And convert the `.ckpt` file to `.onnx` and return the `.onnx` path.\n",
    "\n",
    "We find an `input_sample` being  used with `.ckpt` to `.onnx` conversion method `to_onnx`. This sample input fixes the input size, binding us to use the same  at the time of inference.\n",
    "\n",
    "Get details <a target=\"_blank\" href=\"https://pytorch-lightning.readthedocs.io/en/stable/common/production_inference.html\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Oab1XtAkMM04"
   },
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "def convert_to_onnx_model(model, ckpt_path, onnx_path=None):\n",
    "\n",
    "    # ONNX filename\n",
    "    if onnx_path is None:\n",
    "        onnx_path = ckpt_path[:-4] + 'onnx'\n",
    "\n",
    "    # Load the checkpoint\n",
    "    ckpt_model = LeNet5.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "    # Freeze the network\n",
    "    ckpt_model.freeze()\n",
    "\n",
    "    ckpt_model.eval()\n",
    "\n",
    "    # Add a sample input. Here input shape = (batch_size, num_channel, height, width)\n",
    "    input_sample = torch.randn((1, 1, 32, 32))\n",
    "\n",
    "    # convert to ONNX model\n",
    "    ckpt_model.to_onnx(onnx_path, input_sample, export_params=True)\n",
    "\n",
    "    return onnx_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JsroSJHMM04"
   },
   "source": [
    "**Let us convert `.ckpt` to `.onnx`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "njSGlCUzMM04",
    "outputId": "34a57e5c-b1cc-4a9f-a854-679d396f9ec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx_model_path: lightning_logs/version_0/checkpoints/ckpt_009.onnx\n"
     ]
    }
   ],
   "source": [
    "# initiate the model\n",
    "model = LeNet5()\n",
    "\n",
    "# convert the checkpoint to onnx format\n",
    "onnx_model_path = convert_to_onnx_model(model, ckpt_path)\n",
    "print('onnx_model_path: {}'.format(onnx_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQc8fj83MM04"
   },
   "source": [
    "## Sample Inference\n",
    "\n",
    "**Steps for the inference with `.onnx` model:**\n",
    "\n",
    "- Init the session. This is a one-time operation.\n",
    "\n",
    "\n",
    "- Get the input name from the session. Again, one-time operation.\n",
    "\n",
    "\n",
    "- Prepare input.\n",
    "\n",
    "\n",
    "- Run the session with the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset and preprocess it (resize and normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize to 32x32 as required by LeNet5\n",
    "    transforms.ToTensor(),        # Convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Download and load the MNIST dataset\n",
    "mnist_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "rD4A0rFcMM04",
    "outputId": "0033ca7e-a951-4b15-80c7-adee109f7338"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a single image and label from MNIST\n",
    "mnist_image, mnist_label = mnist_data[0]  # Take the first image and its true label\n",
    "\n",
    "# The model expects a batch, so unsqueeze to add the batch dimension\n",
    "mnist_image = mnist_image.unsqueeze(0)  # Shape: [1, 1, 32, 32]\n",
    "\n",
    "# Convert the image to a NumPy array as expected by ONNX Runtime\n",
    "mnist_image_np = mnist_image.numpy().astype(np.float32)\n",
    "\n",
    "# Load the ONNX model and set up an inference session\n",
    "sess = onnxruntime.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Get input name for the model\n",
    "input_name = sess.get_inputs()[0].name\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = {input_name: mnist_image_np}\n",
    "\n",
    "# Perform inference with ONNX model\n",
    "outputs = sess.run(None, inputs)\n",
    "\n",
    "# The model output is a list of class logits. Get the predicted class by finding the index of the maximum logit.\n",
    "predicted_class = np.argmax(outputs[0], axis=1)\n",
    "\n",
    "# Plot the image with predicted and ground truth labels as title\n",
    "plt.imshow(mnist_image.squeeze(), cmap='gray')\n",
    "plt.title(f\"GT: {mnist_label} ; Pred: {predicted_class[0]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dI4dFo8kMM04"
   },
   "source": [
    "## References\n",
    "\n",
    "\n",
    "1. <a target=\"_blank\" href=\"https://pytorch-lightning.readthedocs.io/en/stable/common/production_inference.html\">https://pytorch-lightning.readthedocs.io/en/stable/common/production_inference.html</a>\n",
    "2. <a target=\"_blank\" href=\"https://docs.microsoft.com/en-us/windows/ai/windows-ml/get-onnx-model\">https://docs.microsoft.com/en-us/windows/ai/windows-ml/get-onnx-model</a>\n",
    "3. <a target=\"_blank\" href=\"https://onnx.ai/\">https://onnx.ai/</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Inference on Production",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
