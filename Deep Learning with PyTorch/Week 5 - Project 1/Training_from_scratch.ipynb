{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Table of Content</font>\n",
    "\n",
    "- [Step 1: Understand Your Problem](#step1)\n",
    "- [Step 2: Get the Data](#step2)\n",
    "- [Step 3. Data Preparation](#step3)\n",
    "- [Step 4: Train a Simple Model](#step4)\n",
    "- [Step 5: Sample Prediction](#step5)\n",
    "- [Step 6. Display Mistakes](#step6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Project 1 - Part 2: Train an Image Classifier From Scratch</font>\n",
    "As discussed in the previous notebook, the steps for training Neural Networks are:\n",
    "\n",
    "- Step 1 - Understand your problem\n",
    "- Step 2A - Get the data\n",
    "- Step 2B - Explore and understand your data\n",
    "- Step 2C - Create a sample data from the dataset\n",
    "- Step 3 - Data preparation\n",
    "- Step 4 - Train a simple model on sample data and check the pipeline before proceeding to train the full network\n",
    "- Step 5 - Train on full data\n",
    "- Step 6 - Improve your model\n",
    "\n",
    "You have already been through Steps `1-4` in the previous notebook. Use them here as well.\n",
    "\n",
    "Here, you will implement Steps `5` & `6` from scratch. Design a model that achieves `>=85%` validation accuracy on the given dataset.\n",
    "\n",
    "There are 70 points for this notebook. The sections which carry marks are in Red.\n",
    "\n",
    "There are **70 points** for this notebook. <font style=\"color:red\">The sections which carry marks are in Red.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For GPU compute you can use Kaggle Kernels, Google Colab or any other service of your choice.\n",
    "\n",
    "After completing the assignment, upload and submit it on the portal for feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:red\">Use the full data to train the network. You need to achieve 85% accuracy for validation data to successfully complete this assignment.</font>\n",
    "\n",
    "\n",
    "<font style=\"color:red\">Just remember to build your own model, not use any pre-trained models/weights.</font>\n",
    "\n",
    "\n",
    "Upon completing the assignment, <font style=\"color:red\">upload the notebook and the models folder on the portal for  feedback.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Step 1: Understand Your problem </font><a name=\"step1\"></a>\n",
    "Already covered in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Step 2: Get the Data </font><a name=\"step2\"></a>\n",
    "\n",
    "Already covered in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget \"https://www.dropbox.com/sh/n5nya3g3airlub6/AACi7vaUjdTA0t2j_iKWgp4Ra?dl=1\" -O data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">Extract the Data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip -q data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Step 3. Data Preparation</font><a name=\"step3\"></a>\n",
    "Already covered in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">3.1. Import Libraries </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from typing import Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">3.2.1. Compulsary Preprocessing Transforms</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocess_transforms():\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    return preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">3.2.2. Common Image Transforms</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_common_transforms(mean=(0.4611, 0.4359, 0.3905), std=(0.2193, 0.2150, 0.2109)):\n",
    "    preprocess = image_preprocess_transforms()\n",
    "    \n",
    "    common_transforms = transforms.Compose([\n",
    "        preprocess,\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    \n",
    "    return common_transforms\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">3.2.3. Mean and STD</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(data_root, num_workers=4):\n",
    "    \n",
    "    transform = image_preprocess_transforms()\n",
    "    \n",
    "    loader = data_loader(data_root, transform)\n",
    "    \n",
    "    batch_mean = torch.zeros(3)\n",
    "    batch_mean_sqrd = torch.zeros(3)\n",
    "    \n",
    "    for batch_data, _ in loader:\n",
    "        batch_mean += batch_data.mean(dim=(0, 2, 3)) # E[batch_i] \n",
    "        batch_mean_sqrd += (batch_data ** 2).mean(dim=(0, 2, 3)) #  E[batch_i**2]\n",
    "    \n",
    "    # E[dataset] = E[E[batch_1], E[batch_2], ...]\n",
    "    mean = batch_mean / len(loader)\n",
    "    \n",
    "    # var[X] = E[X**2] - E[X]**2\n",
    "    \n",
    "    # E[X**2] = E[E[batch_1**2], E[batch_2**2], ...]\n",
    "    # E[X]**2 = E[E[batch_1], E[batch_2], ...] ** 2\n",
    "    \n",
    "    var = (batch_mean_sqrd / len(loader)) - (mean ** 2)\n",
    "        \n",
    "    std = var ** 0.5\n",
    "    print('mean: {}, std: {}'.format(mean, std))\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">3.3. Data Loaders </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">3.3.1. Data Loader for Full Data</font>\n",
    "Data loader for generating batches of data to be used by the training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_root, transform, batch_size=16, shuffle=False, num_workers=2):\n",
    "    dataset = datasets.ImageFolder(root=data_root, transform=transform)\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size=batch_size,\n",
    "                                         num_workers=num_workers,\n",
    "                                         shuffle=shuffle)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3.4. Prepare Data</font>\n",
    "The main function which uses all the above functions to generate the train and test dataloaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size, data_root, num_workers=4, data_augmentation=False):\n",
    "#     YOUR CODE HERE\n",
    "    \n",
    "    train_data_path = os.path.join(data_root, 'training')\n",
    "       \n",
    "    mean, std = get_mean_std(data_root=train_data_path, num_workers=num_workers)\n",
    "    \n",
    "    common_transforms = image_common_transforms(mean, std)\n",
    "        \n",
    "   \n",
    "    # if data_augmentation is true \n",
    "    # data augmentation implementation\n",
    "    if data_augmentation:    \n",
    "        train_transforms = transforms.Compose([\n",
    "            transforms.RandomChoice([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "            ]),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.Normalize(mean,std)\n",
    "        ])\n",
    "    # else do common transforms\n",
    "    else:\n",
    "        train_transforms = common_transforms\n",
    "        \n",
    "        \n",
    "    # train dataloader\n",
    "    \n",
    "    train_loader = data_loader(train_data_path, \n",
    "                               transform=train_transforms, \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=True, \n",
    "                               num_workers=num_workers)\n",
    "    \n",
    "    # test dataloader\n",
    "    \n",
    "    test_data_path = os.path.join(data_root, 'validation')\n",
    "    \n",
    "    test_loader = data_loader(test_data_path, \n",
    "                              transform=common_transforms, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=False, \n",
    "                              num_workers=num_workers)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Step 4: Train Your Model</font><a name=\"step4\"></a>\n",
    "\n",
    "Now, create the training pipeline, and train your model on the full data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:red\">4.1. Configurations [20 Points]</font>\n",
    "\n",
    "To achieve good results, change the parameters given in these configurations. Score 20 points for all right choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">4.1.1. System Configuration</font>\n",
    "\n",
    "Fix the seed (e.g., `21`) to get a reproducible result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfiguration:\n",
    "    '''\n",
    "    Describes the common system setting needed for reproducible training\n",
    "    '''\n",
    "    seed: int = 21  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">4.1.2. Training Configuration</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 32 \n",
    "    epochs_count: int = 100  \n",
    "    init_learning_rate: float = 0.000001  # initial learning rate for lr scheduler\n",
    "    log_interval: int = 100  \n",
    "    test_interval: int = 1  \n",
    "    data_root: str = \"./cat-dog-panda\" \n",
    "    num_workers: int = 20  \n",
    "    device: str = 'cuda'  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:green\">4.1.3. System Setup</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_system(system_config: SystemConfiguration) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">4.2. Training Function</font>\n",
    "\n",
    "You are already familiar with the training function. No changes needed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",
    ") -> None:\n",
    "    \n",
    "    # change model in training mood\n",
    "    model.train()\n",
    "    \n",
    "    # to get batch loss\n",
    "    batch_loss = np.array([])\n",
    "    \n",
    "    # to get batch accuracy\n",
    "    batch_acc = np.array([])\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # clone target\n",
    "        indx_target = target.clone()\n",
    "        # send data to device (its is medatory if GPU has to be used)\n",
    "        data = data.to(train_config.device)\n",
    "        # send target to device\n",
    "        target = target.to(train_config.device)\n",
    "\n",
    "        # reset parameters gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # cross entropy loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        # find gradients w.r.t training parameters\n",
    "        loss.backward()\n",
    "        # Update parameters using gardients\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = np.append(batch_loss, [loss.item()])\n",
    "        \n",
    "        # Score to probability using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "            \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]  \n",
    "                        \n",
    "        # correct prediction\n",
    "        correct = pred.cpu().eq(indx_target).sum()\n",
    "            \n",
    "        # accuracy\n",
    "        acc = float(correct) / float(len(data))\n",
    "        \n",
    "        batch_acc = np.append(batch_acc, [acc])\n",
    "            \n",
    "    epoch_loss = batch_loss.mean()\n",
    "    epoch_acc = batch_acc.mean()\n",
    "    print('Epoch: {} \\nTrain Loss: {:.6f} Acc: {:.4f}'.format(epoch_idx, epoch_loss, epoch_acc))\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">4.3. Validation Function</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    train_config: TrainingConfiguration,\n",
    "    model: nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    ") -> float:\n",
    "    # \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    count_corect_predictions = 0\n",
    "    for data, target in test_loader:\n",
    "        indx_target = target.clone()\n",
    "        data = data.to(train_config.device)\n",
    "        \n",
    "        target = target.to(train_config.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "        \n",
    "        # add loss for each mini batch\n",
    "        test_loss += F.cross_entropy(output, target).item()\n",
    "        \n",
    "        # Score to probability using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1] \n",
    "        \n",
    "        # add correct prediction count\n",
    "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "    # average over number of mini-batches\n",
    "    test_loss = test_loss / len(test_loader)  \n",
    "    \n",
    "    # average over number of dataset\n",
    "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
    "    \n",
    "    print(\n",
    "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return test_loss, accuracy/100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">4.4. Save the Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, device, model_dir='models', model_file_name='cat_dog_panda_classifier.pt'):\n",
    "    \n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    model_path = os.path.join(model_dir, model_file_name)\n",
    "\n",
    "    # make sure you transfer the model to cpu.\n",
    "    if device == 'cuda':\n",
    "        model.to('cpu')\n",
    "\n",
    "    # save the state_dict\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        model.to('cuda')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">4.5. Load the Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, model_dir='models', model_file_name='cat_dog_panda_classifier.pt'):\n",
    "    model_path = os.path.join(model_dir, model_file_name)\n",
    "\n",
    "    # loading the model and getting model parameters by using load_state_dict\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">4.6. Main Function for Training</font>\n",
    "\n",
    "Note: Before calling the train function, we calculate the validation loss, which we know should be close to $\\log(\\text{num_classes})$. Also, the accuracy should be close to $\\frac{1}{\\text{num_classes}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, optimizer, scheduler=None, system_configuration=SystemConfiguration(), \n",
    "         training_configuration=TrainingConfiguration(), data_augmentation=True):\n",
    "    \n",
    "    # system configuration\n",
    "    setup_system(system_configuration)\n",
    "\n",
    "    # batch size\n",
    "    batch_size_to_set = training_configuration.batch_size\n",
    "    # num_workers\n",
    "    num_workers_to_set = training_configuration.num_workers\n",
    "    # epochs\n",
    "    epoch_num_to_set = training_configuration.epochs_count\n",
    "\n",
    "    # if GPU is available use training config, \n",
    "    # else lowers batch_size, num_workers and epochs count\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        batch_size_to_set = 16\n",
    "        num_workers_to_set = 4\n",
    "\n",
    "    # data loader\n",
    "    train_loader, test_loader = get_data(\n",
    "        batch_size=batch_size_to_set,\n",
    "        data_root=training_configuration.data_root,\n",
    "        num_workers=num_workers_to_set,\n",
    "        data_augmentation=data_augmentation\n",
    "    )\n",
    "    \n",
    "    # Update training configuration\n",
    "    training_configuration = TrainingConfiguration(\n",
    "        device=device,\n",
    "        batch_size=batch_size_to_set,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "        \n",
    "    # send model to device (GPU/CPU)\n",
    "    model.to(training_configuration.device)\n",
    "\n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    \n",
    "    # epoch train/test loss\n",
    "    epoch_train_loss = np.array([])\n",
    "    epoch_test_loss = np.array([])\n",
    "    \n",
    "    # epch train/test accuracy\n",
    "    epoch_train_acc = np.array([])\n",
    "    epoch_test_acc = np.array([])\n",
    "    \n",
    "    # Calculate Initial Test Loss\n",
    "    init_val_loss, init_val_accuracy = validate(training_configuration, model, test_loader)\n",
    "    print(\"Initial Test Loss : {:.6f}, \\nInitial Test Accuracy : {:.3f}%\\n\".format(init_val_loss, \n",
    "                                                                                   init_val_accuracy*100))\n",
    "    \n",
    "    # trainig time measurement\n",
    "    t_begin = time.time()\n",
    "    for epoch in range(training_configuration.epochs_count):\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n",
    "        \n",
    "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
    "        \n",
    "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
    "\n",
    "        elapsed_time = time.time() - t_begin\n",
    "        speed_epoch = elapsed_time / (epoch + 1)\n",
    "        speed_batch = speed_epoch / len(train_loader)\n",
    "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
    "        \n",
    "        print(\n",
    "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
    "                elapsed_time, speed_epoch, speed_batch, eta\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Validate\n",
    "        if epoch % training_configuration.test_interval == 0:\n",
    "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
    "            \n",
    "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
    "        \n",
    "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
    "            \n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                print('Model Improved. Saving the Model...\\n')\n",
    "                save_model(model, device=training_configuration.device)\n",
    "        \n",
    "                \n",
    "    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n",
    "    \n",
    "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">4.7. Plot Loss and Accuracy</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(train_loss, val_loss, train_acc, val_acc, colors, \n",
    "                       loss_legend_loc='upper center', acc_legend_loc='upper left', \n",
    "                       fig_size=(20, 10), sub_plot1=(1, 2, 1), sub_plot2=(1, 2, 2)):\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    plt.subplot(sub_plot1[0], sub_plot1[1], sub_plot1[2])\n",
    "    \n",
    "    for i in range(len(train_loss)):\n",
    "        x_train = range(len(train_loss[i]))\n",
    "        x_val = range(len(val_loss[i]))\n",
    "        \n",
    "        min_train_loss = train_loss[i].min()\n",
    "        \n",
    "        min_val_loss = val_loss[i].min()\n",
    "        \n",
    "        plt.plot(x_train, train_loss[i], linestyle='-', color='tab:{}'.format(colors[i]), \n",
    "                 label=\"TRAIN LOSS ({0:.4})\".format(min_train_loss))\n",
    "        plt.plot(x_val, val_loss[i], linestyle='--' , color='tab:{}'.format(colors[i]), \n",
    "                 label=\"VALID LOSS ({0:.4})\".format(min_val_loss))\n",
    "        \n",
    "    plt.xlabel('epoch no.')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc=loss_legend_loc)\n",
    "    plt.title('Training and Validation Loss')\n",
    "        \n",
    "    plt.subplot(sub_plot2[0], sub_plot2[1], sub_plot2[2])\n",
    "    \n",
    "    for i in range(len(train_acc)):\n",
    "        x_train = range(len(train_acc[i]))\n",
    "        x_val = range(len(val_acc[i]))\n",
    "        \n",
    "        max_train_acc = train_acc[i].max() \n",
    "        \n",
    "        max_val_acc = val_acc[i].max() \n",
    "        \n",
    "        plt.plot(x_train, train_acc[i], linestyle='-', color='tab:{}'.format(colors[i]), \n",
    "                 label=\"TRAIN ACC ({0:.4})\".format(max_train_acc))\n",
    "        plt.plot(x_val, val_acc[i], linestyle='--' , color='tab:{}'.format(colors[i]), \n",
    "                 label=\"VALID ACC ({0:.4})\".format(max_val_acc))\n",
    "        \n",
    "    plt.xlabel('epoch no.')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc=acc_legend_loc)\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    \n",
    "    fig.savefig('sample_loss_acc_plot.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:red\">4.8. Define Model [20 Points]</font>\n",
    "\n",
    "Next, define the CNN model. Keep iterating. Do this by training various models. Just ,change the :\n",
    "    \n",
    "- number of layers\n",
    "- parameters inside the layers\n",
    "- different types of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._body = nn.Sequential(\n",
    "            # convolution layers\n",
    "            # First convolution Layer\n",
    "            # input size = (224, 224), output size = (220, 220)\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (220, 220), output size = (216, 216)\n",
    "            nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5),\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # input size = (216, 216), output size = (212, 212)\n",
    "            nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5),\n",
    "            nn.BatchNorm2d(24),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # input size = (212, 212), output size = (208, 208)\n",
    "            nn.Conv2d(in_channels=24, out_channels=48, kernel_size=5),\n",
    "            nn.BatchNorm2d(48),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # input size = (208, 208), output size = (102, 102)\n",
    "            nn.Conv2d(in_channels=48, out_channels=96, kernel_size=5),\n",
    "            nn.BatchNorm2d(96),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((102,102)),\n",
    "            \n",
    "\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weight in last conv layer = 32 * 13 * 13\n",
    "            nn.Linear(in_features=96 * 102 * 102, out_features=512), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.70),\n",
    "            \n",
    "            nn.Linear(in_features=512, out_features=128), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.70),\n",
    "            \n",
    "            # Fourth fully connected layer. It is also output layer\n",
    "            # in_features = output of last linear layer = 64\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=128, out_features=3)\n",
    "        )\n",
    "\n",
    "        ###\n",
    "        ### YOUR CODE HERE\n",
    "        ###\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ###\n",
    "        ### YOUR CODE HERE\n",
    "        ###\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weights_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:red\">4.9. Training [20 Points]</font>\n",
    "\n",
    "Experiment with:\n",
    "\n",
    "- Optimizers and learning rate schedulers \\[You can  get good results even without a learning rate scheduler\\]\n",
    "\n",
    "- Regularization techniques like Data Augmentation, Dropout, BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(12, 24, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): Conv2d(24, 48, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (10): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(48, 96, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (13): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU(inplace=True)\n",
      "    (15): AdaptiveAvgPool2d(output_size=(102, 102))\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=998784, out_features=512, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.7, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.7, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "print(model)\n",
    "\n",
    "# get optimizer\n",
    "train_config = TrainingConfiguration()\n",
    "\n",
    "### CHANGE HERE ###\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr = train_config.init_learning_rate\n",
    ")\n",
    "\n",
    "step_size = 10\n",
    "decay_rate = 0.5\n",
    "scheduler = lr_scheduler.StepLR(optimizer,step_size=step_size,gamma=decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([0.4610, 0.4347, 0.3897]), std: tensor([0.2734, 0.2641, 0.2616])\n",
      "\n",
      "Test set: Average loss: 1.0992, Accuracy: 200/600 (33%)\n",
      "\n",
      "Initial Test Loss : 1.099248, \n",
      "Initial Test Accuracy : 33.333%\n",
      "\n",
      "Epoch: 0 \n",
      "Train Loss: 1.094139 Acc: 0.4054\n",
      "Elapsed 219.21s, 219.21 s/epoch, 2.92 s/batch, ets 21701.61s\n",
      "\n",
      "Test set: Average loss: 0.9754, Accuracy: 365/600 (61%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 1 \n",
      "Train Loss: 1.024571 Acc: 0.4567\n",
      "Elapsed 446.22s, 223.11 s/epoch, 2.97 s/batch, ets 21864.65s\n",
      "\n",
      "Test set: Average loss: 0.9057, Accuracy: 352/600 (59%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 2 \n",
      "Train Loss: 0.968837 Acc: 0.5083\n",
      "Elapsed 673.16s, 224.39 s/epoch, 2.99 s/batch, ets 21765.37s\n",
      "\n",
      "Test set: Average loss: 0.8524, Accuracy: 338/600 (56%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 3 \n",
      "Train Loss: 0.930447 Acc: 0.5396\n",
      "Elapsed 900.99s, 225.25 s/epoch, 3.00 s/batch, ets 21623.77s\n",
      "\n",
      "Test set: Average loss: 0.8137, Accuracy: 361/600 (60%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 4 \n",
      "Train Loss: 0.918794 Acc: 0.5546\n",
      "Elapsed 1127.39s, 225.48 s/epoch, 3.01 s/batch, ets 21420.46s\n",
      "\n",
      "Test set: Average loss: 0.7843, Accuracy: 386/600 (64%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 5 \n",
      "Train Loss: 0.898973 Acc: 0.5467\n",
      "Elapsed 1354.58s, 225.76 s/epoch, 3.01 s/batch, ets 21221.69s\n",
      "\n",
      "Test set: Average loss: 0.7707, Accuracy: 376/600 (63%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 6 \n",
      "Train Loss: 0.912027 Acc: 0.5442\n",
      "Elapsed 1583.15s, 226.16 s/epoch, 3.02 s/batch, ets 21033.25s\n",
      "\n",
      "Test set: Average loss: 0.7717, Accuracy: 380/600 (63%)\n",
      "\n",
      "Epoch: 7 \n",
      "Train Loss: 0.891911 Acc: 0.5533\n",
      "Elapsed 1806.52s, 225.81 s/epoch, 3.01 s/batch, ets 20774.98s\n",
      "\n",
      "Test set: Average loss: 0.7550, Accuracy: 388/600 (65%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 8 \n",
      "Train Loss: 0.871898 Acc: 0.5804\n",
      "Elapsed 2034.15s, 226.02 s/epoch, 3.01 s/batch, ets 20567.56s\n",
      "\n",
      "Test set: Average loss: 0.7490, Accuracy: 378/600 (63%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 9 \n",
      "Train Loss: 0.867464 Acc: 0.5887\n",
      "Elapsed 2263.27s, 226.33 s/epoch, 3.02 s/batch, ets 20369.47s\n",
      "\n",
      "Test set: Average loss: 0.7403, Accuracy: 393/600 (66%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 10 \n",
      "Train Loss: 0.863068 Acc: 0.5858\n",
      "Elapsed 2493.09s, 226.64 s/epoch, 3.02 s/batch, ets 20171.35s\n",
      "\n",
      "Test set: Average loss: 0.7464, Accuracy: 376/600 (63%)\n",
      "\n",
      "Epoch: 11 \n",
      "Train Loss: 0.850661 Acc: 0.5904\n",
      "Elapsed 2716.09s, 226.34 s/epoch, 3.02 s/batch, ets 19918.00s\n",
      "\n",
      "Test set: Average loss: 0.7173, Accuracy: 391/600 (65%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 12 \n",
      "Train Loss: 0.857846 Acc: 0.5929\n",
      "Elapsed 2943.71s, 226.44 s/epoch, 3.02 s/batch, ets 19700.20s\n",
      "\n",
      "Test set: Average loss: 0.7351, Accuracy: 384/600 (64%)\n",
      "\n",
      "Epoch: 13 \n",
      "Train Loss: 0.853411 Acc: 0.5733\n",
      "Elapsed 3166.90s, 226.21 s/epoch, 3.02 s/batch, ets 19453.84s\n",
      "\n",
      "Test set: Average loss: 0.7133, Accuracy: 391/600 (65%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 14 \n",
      "Train Loss: 0.840552 Acc: 0.6004\n",
      "Elapsed 3393.92s, 226.26 s/epoch, 3.02 s/batch, ets 19232.19s\n",
      "\n",
      "Test set: Average loss: 0.7254, Accuracy: 400/600 (67%)\n",
      "\n",
      "Epoch: 15 \n",
      "Train Loss: 0.836016 Acc: 0.6054\n",
      "Elapsed 3616.45s, 226.03 s/epoch, 3.01 s/batch, ets 18986.36s\n",
      "\n",
      "Test set: Average loss: 0.7251, Accuracy: 402/600 (67%)\n",
      "\n",
      "Epoch: 16 \n",
      "Train Loss: 0.819034 Acc: 0.6050\n",
      "Elapsed 3838.99s, 225.82 s/epoch, 3.01 s/batch, ets 18743.28s\n",
      "\n",
      "Test set: Average loss: 0.7175, Accuracy: 411/600 (68%)\n",
      "\n",
      "Epoch: 17 \n",
      "Train Loss: 0.819179 Acc: 0.6071\n",
      "Elapsed 4061.29s, 225.63 s/epoch, 3.01 s/batch, ets 18501.44s\n",
      "\n",
      "Test set: Average loss: 0.7172, Accuracy: 402/600 (67%)\n",
      "\n",
      "Epoch: 18 \n",
      "Train Loss: 0.819551 Acc: 0.6104\n",
      "Elapsed 4284.04s, 225.48 s/epoch, 3.01 s/batch, ets 18263.54s\n",
      "\n",
      "Test set: Average loss: 0.7101, Accuracy: 403/600 (67%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 19 \n",
      "Train Loss: 0.819258 Acc: 0.6067\n",
      "Elapsed 4511.38s, 225.57 s/epoch, 3.01 s/batch, ets 18045.53s\n",
      "\n",
      "Test set: Average loss: 0.7053, Accuracy: 412/600 (69%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 20 \n",
      "Train Loss: 0.805465 Acc: 0.6162\n",
      "Elapsed 4738.76s, 225.66 s/epoch, 3.01 s/batch, ets 17826.75s\n",
      "\n",
      "Test set: Average loss: 0.7026, Accuracy: 405/600 (68%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 21 \n",
      "Train Loss: 0.808165 Acc: 0.6129\n",
      "Elapsed 4966.07s, 225.73 s/epoch, 3.01 s/batch, ets 17606.97s\n",
      "\n",
      "Test set: Average loss: 0.7060, Accuracy: 395/600 (66%)\n",
      "\n",
      "Epoch: 22 \n",
      "Train Loss: 0.766032 Acc: 0.6388\n",
      "Elapsed 5189.01s, 225.61 s/epoch, 3.01 s/batch, ets 17371.91s\n",
      "\n",
      "Test set: Average loss: 0.6898, Accuracy: 404/600 (67%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 23 \n",
      "Train Loss: 0.805773 Acc: 0.6062\n",
      "Elapsed 5416.90s, 225.70 s/epoch, 3.01 s/batch, ets 17153.53s\n",
      "\n",
      "Test set: Average loss: 0.6897, Accuracy: 402/600 (67%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 24 \n",
      "Train Loss: 0.787141 Acc: 0.6400\n",
      "Elapsed 5644.47s, 225.78 s/epoch, 3.01 s/batch, ets 16933.40s\n",
      "\n",
      "Test set: Average loss: 0.6860, Accuracy: 415/600 (69%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 25 \n",
      "Train Loss: 0.780848 Acc: 0.6400\n",
      "Elapsed 5872.84s, 225.88 s/epoch, 3.01 s/batch, ets 16715.00s\n",
      "\n",
      "Test set: Average loss: 0.6889, Accuracy: 408/600 (68%)\n",
      "\n",
      "Epoch: 26 \n",
      "Train Loss: 0.767325 Acc: 0.6446\n",
      "Elapsed 6095.29s, 225.75 s/epoch, 3.01 s/batch, ets 16479.87s\n",
      "\n",
      "Test set: Average loss: 0.6917, Accuracy: 395/600 (66%)\n",
      "\n",
      "Epoch: 27 \n",
      "Train Loss: 0.776558 Acc: 0.6300\n",
      "Elapsed 6317.67s, 225.63 s/epoch, 3.01 s/batch, ets 16245.44s\n",
      "\n",
      "Test set: Average loss: 0.6845, Accuracy: 416/600 (69%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 28 \n",
      "Train Loss: 0.744602 Acc: 0.6562\n",
      "Elapsed 6546.53s, 225.74 s/epoch, 3.01 s/batch, ets 16027.71s\n",
      "\n",
      "Test set: Average loss: 0.6702, Accuracy: 418/600 (70%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 29 \n",
      "Train Loss: 0.764491 Acc: 0.6504\n",
      "Elapsed 6774.86s, 225.83 s/epoch, 3.01 s/batch, ets 15808.00s\n",
      "\n",
      "Test set: Average loss: 0.6746, Accuracy: 420/600 (70%)\n",
      "\n",
      "Epoch: 30 \n",
      "Train Loss: 0.765411 Acc: 0.6508\n",
      "Elapsed 6998.31s, 225.75 s/epoch, 3.01 s/batch, ets 15576.87s\n",
      "\n",
      "Test set: Average loss: 0.6739, Accuracy: 422/600 (70%)\n",
      "\n",
      "Epoch: 31 \n",
      "Train Loss: 0.755629 Acc: 0.6554\n",
      "Elapsed 7222.29s, 225.70 s/epoch, 3.01 s/batch, ets 15347.38s\n",
      "\n",
      "Test set: Average loss: 0.6705, Accuracy: 418/600 (70%)\n",
      "\n",
      "Epoch: 32 \n",
      "Train Loss: 0.763930 Acc: 0.6579\n",
      "Elapsed 7446.18s, 225.64 s/epoch, 3.01 s/batch, ets 15118.00s\n",
      "\n",
      "Test set: Average loss: 0.6750, Accuracy: 424/600 (71%)\n",
      "\n",
      "Epoch: 33 \n",
      "Train Loss: 0.750146 Acc: 0.6454\n",
      "Elapsed 7670.69s, 225.61 s/epoch, 3.01 s/batch, ets 14890.17s\n",
      "\n",
      "Test set: Average loss: 0.6675, Accuracy: 417/600 (70%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 34 \n",
      "Train Loss: 0.743156 Acc: 0.6579\n",
      "Elapsed 7899.25s, 225.69 s/epoch, 3.01 s/batch, ets 14670.03s\n",
      "\n",
      "Test set: Average loss: 0.6599, Accuracy: 427/600 (71%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 35 \n",
      "Train Loss: 0.730092 Acc: 0.6633\n",
      "Elapsed 8128.14s, 225.78 s/epoch, 3.01 s/batch, ets 14450.02s\n",
      "\n",
      "Test set: Average loss: 0.6724, Accuracy: 414/600 (69%)\n",
      "\n",
      "Epoch: 36 \n",
      "Train Loss: 0.715821 Acc: 0.6654\n",
      "Elapsed 8351.53s, 225.72 s/epoch, 3.01 s/batch, ets 14220.17s\n",
      "\n",
      "Test set: Average loss: 0.6595, Accuracy: 419/600 (70%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 37 \n",
      "Train Loss: 0.727387 Acc: 0.6650\n",
      "Elapsed 8579.77s, 225.78 s/epoch, 3.01 s/batch, ets 13998.57s\n",
      "\n",
      "Test set: Average loss: 0.6650, Accuracy: 423/600 (70%)\n",
      "\n",
      "Epoch: 38 \n",
      "Train Loss: 0.725661 Acc: 0.6671\n",
      "Elapsed 8804.13s, 225.75 s/epoch, 3.01 s/batch, ets 13770.56s\n",
      "\n",
      "Test set: Average loss: 0.6609, Accuracy: 427/600 (71%)\n",
      "\n",
      "Epoch: 39 \n",
      "Train Loss: 0.718924 Acc: 0.6708\n",
      "Elapsed 9028.47s, 225.71 s/epoch, 3.01 s/batch, ets 13542.71s\n",
      "\n",
      "Test set: Average loss: 0.6594, Accuracy: 426/600 (71%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 40 \n",
      "Train Loss: 0.723228 Acc: 0.6663\n",
      "Elapsed 9257.47s, 225.79 s/epoch, 3.01 s/batch, ets 13321.73s\n",
      "\n",
      "Test set: Average loss: 0.6573, Accuracy: 435/600 (72%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 41 \n",
      "Train Loss: 0.733720 Acc: 0.6596\n",
      "Elapsed 9485.11s, 225.84 s/epoch, 3.01 s/batch, ets 13098.48s\n",
      "\n",
      "Test set: Average loss: 0.6661, Accuracy: 428/600 (71%)\n",
      "\n",
      "Epoch: 42 \n",
      "Train Loss: 0.703295 Acc: 0.6767\n",
      "Elapsed 9707.68s, 225.76 s/epoch, 3.01 s/batch, ets 12868.31s\n",
      "\n",
      "Test set: Average loss: 0.6523, Accuracy: 434/600 (72%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 43 \n",
      "Train Loss: 0.702868 Acc: 0.6796\n",
      "Elapsed 9936.64s, 225.83 s/epoch, 3.01 s/batch, ets 12646.64s\n",
      "\n",
      "Test set: Average loss: 0.6515, Accuracy: 432/600 (72%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 44 \n",
      "Train Loss: 0.702863 Acc: 0.6833\n",
      "Elapsed 10164.45s, 225.88 s/epoch, 3.01 s/batch, ets 12423.21s\n",
      "\n",
      "Test set: Average loss: 0.6503, Accuracy: 425/600 (71%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 45 \n",
      "Train Loss: 0.713901 Acc: 0.6621\n",
      "Elapsed 10392.49s, 225.92 s/epoch, 3.01 s/batch, ets 12199.88s\n",
      "\n",
      "Test set: Average loss: 0.6681, Accuracy: 425/600 (71%)\n",
      "\n",
      "Epoch: 46 \n",
      "Train Loss: 0.699510 Acc: 0.6779\n",
      "Elapsed 10617.44s, 225.90 s/epoch, 3.01 s/batch, ets 11972.86s\n",
      "\n",
      "Test set: Average loss: 0.6509, Accuracy: 430/600 (72%)\n",
      "\n",
      "Epoch: 47 \n",
      "Train Loss: 0.699292 Acc: 0.6892\n",
      "Elapsed 10841.53s, 225.87 s/epoch, 3.01 s/batch, ets 11744.99s\n",
      "\n",
      "Test set: Average loss: 0.6510, Accuracy: 428/600 (71%)\n",
      "\n",
      "Epoch: 48 \n",
      "Train Loss: 0.703747 Acc: 0.6767\n",
      "Elapsed 11063.81s, 225.79 s/epoch, 3.01 s/batch, ets 11515.40s\n",
      "\n",
      "Test set: Average loss: 0.6542, Accuracy: 421/600 (70%)\n",
      "\n",
      "Epoch: 49 \n",
      "Train Loss: 0.684420 Acc: 0.6967\n",
      "Elapsed 11286.90s, 225.74 s/epoch, 3.01 s/batch, ets 11286.90s\n",
      "\n",
      "Test set: Average loss: 0.6484, Accuracy: 430/600 (72%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n",
      "Epoch: 50 \n",
      "Train Loss: 0.701463 Acc: 0.6808\n",
      "Elapsed 11514.29s, 225.77 s/epoch, 3.01 s/batch, ets 11062.74s\n",
      "\n",
      "Test set: Average loss: 0.6459, Accuracy: 429/600 (72%)\n",
      "\n",
      "Model Improved. Saving the Model...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train and validate\n",
    "model, train_loss, train_acc, val_loss, val_acc = main(model, optimizer, scheduler=None, data_augmentation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">4.10. Loss and Accuracy Plot</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(train_loss=[train_loss], \n",
    "                   val_loss=[val_loss], \n",
    "                   train_acc=[train_acc], \n",
    "                   val_acc=[val_acc], \n",
    "                   colors=['blue'], \n",
    "                   loss_legend_loc='upper center', \n",
    "                   acc_legend_loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Step 5. Sample Prediction</font><a name=\"step5\"></a>\n",
    "\n",
    "Show some sample predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">5.1. Make Predictions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, device, batch_input):\n",
    "    \n",
    "    data = batch_input.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "\n",
    "    # Score to probability using softmax\n",
    "    prob = F.softmax(output, dim=1)\n",
    "\n",
    "    # get the max probability\n",
    "    pred_prob = prob.data.max(dim=1)[0]\n",
    "    \n",
    "    # get the index of the max probability\n",
    "    pred_index = prob.data.max(dim=1)[1]\n",
    "    \n",
    "    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">5.2. Get Predictions on a Batch</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_prediction(model, data_root, mean, std):\n",
    "    batch_size = 15\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        num_workers = 8\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        num_workers = 2\n",
    "    \n",
    "    # It is important to do model.eval() before prediction\n",
    "    model.eval()\n",
    "    \n",
    "    # Send model to cpu/cuda according to your system configuration\n",
    "    model.to(device)\n",
    "\n",
    "    # transformed data\n",
    "    test_dataset_trans = datasets.ImageFolder(root=data_root, transform=image_common_transforms(mean, std))\n",
    "    \n",
    "    # original image dataset\n",
    "    test_dataset = datasets.ImageFolder(root=data_root, transform=image_preprocess_transforms())\n",
    "    \n",
    "    data_len = test_dataset.__len__()\n",
    "    \n",
    "    interval = int(data_len/batch_size)\n",
    "    \n",
    "    imgs = []\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(batch_size):\n",
    "        index = i * interval\n",
    "        trans_input, target = test_dataset_trans.__getitem__(index)\n",
    "        img, _ = test_dataset.__getitem__(index)\n",
    "        \n",
    "        imgs.append(img)\n",
    "        inputs.append(trans_input)\n",
    "        targets.append(target)\n",
    "        \n",
    "    inputs = torch.stack(inputs)\n",
    "        \n",
    "    cls, prob = prediction(model, device, batch_input=inputs)\n",
    "    \n",
    "    plt.style.use('default')\n",
    "    plt.rcParams[\"figure.figsize\"] = (15, 9)\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    \n",
    "    for i, target in enumerate(targets):\n",
    "        plt.subplot(3, 5, i+1)\n",
    "        img = transforms.functional.to_pil_image(imgs[i])\n",
    "        plt.imshow(img)\n",
    "        plt.gca().set_title('P:{0}({1:.2}), T:{2}'.format(test_dataset.classes[cls[i]], \n",
    "                                                     prob[i], \n",
    "                                                     test_dataset.classes[targets[i]]))\n",
    "    fig.savefig('sample_prediction.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">5.3. Load Model and Run Inference</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MyModel()\n",
    "m = load_model(m)\n",
    "\n",
    "train_config = TrainingConfiguration()\n",
    "\n",
    "test_data_path = os.path.join(train_config.data_root, 'validation')\n",
    "train_data_path = os.path.join(train_config.data_root, 'training')\n",
    "\n",
    "mean, std = get_mean_std(train_data_path)\n",
    "\n",
    "get_sample_prediction(m, test_data_path, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:red\">Step 6. Display Confusion Matrix [10 Points]</font><a name=\"step6\"></a>\n",
    "\n",
    "Display the confusion matrix for the above problem(Refer to the earlier lectures on Performance Metrics).\n",
    "\n",
    "\n",
    "The output should resemble this:\n",
    "\n",
    "<img src='https://www.learnopencv.com/wp-content/uploads/2020/02/c3_w5_sample_confusion_matrix.png' width=600>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon completing the assignment,  <font style=\"color:red\">pload the 2 notebooks and the models folder on the portal for feedback.</font>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
