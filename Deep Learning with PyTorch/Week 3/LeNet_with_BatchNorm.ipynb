{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Table of contents</font>\n",
    "\n",
    "- [LeNet5 Architecture](#lenet)\n",
    "- [Display the Network](#display)\n",
    "- [Get the Fashion-MNIST Data](#get-data)\n",
    "- [System Configuration](#sys-config)\n",
    "- [Training Configuration](#train-config)\n",
    "- [System Setup](#sys-setup)\n",
    "- [Training](#training)\n",
    "- [Validation](#validation)\n",
    "- [Main function](#main)\n",
    "- [Plot Loss](#plot-loss)\n",
    "- [Miscellaneous](#misc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Convolutional Neural Network Using Batch Normalization</font>\n",
    "\n",
    "In this notebook, we  add batch norm layers to the LeNet network, and see how it affects network training and convergence.\n",
    "\n",
    "Instead of the MNIST dataset, which overfits easily, we will use the Fashion MNIST dataset.\n",
    "\n",
    "The figure below shows some samples from the Fashion MNIST dataset.\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2021/01/c3-w3-fashion-mnist-sprite.jpg\" width=\"600\">\n",
    "\n",
    "There are 10 classes. Each training and testing example is assigned to one of the following labels:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "\n",
    "\n",
    "\n",
    "We want to classify images in this dataset, using the LeNet network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # one of the best graphics library for python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from typing import Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1. LeNet Architecture with BatchNorm</font><a name=\"lenet\"></a>\n",
    "\n",
    "We have already explained the architecture for LeNet in the previous notebook.\n",
    "\n",
    "Here, we create another model called LeNetBN, adding Batch Normalization layers to the 2 convolution blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2-d\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weights in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third fully connected layer which is also output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2-d\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weight in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third fully connected layer. It is also output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weights_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2. Display the Network</font><a name=\"display\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "LeNetBN(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lenet_model = LeNet()\n",
    "print(lenet_model)\n",
    "lenetBN_model = LeNetBN()\n",
    "print(lenetBN_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## <font style=\"color:green\">3. Get Fashion-MNIST Data</font><a name=\"get-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size, data_root='data', num_workers=1):\n",
    "    \n",
    "    train_test_transforms = transforms.Compose([\n",
    "        # Resize to 32X32\n",
    "        transforms.Resize((32, 32)),\n",
    "        # this re-scales image tensor values between 0-1. image_tensor /= 255\n",
    "        transforms.ToTensor(),\n",
    "        # subtract mean (0.2860) and divide by variance (0.3530).\n",
    "        # This mean and variance is calculated on training data (verify for yourself)\n",
    "        transforms.Normalize((0.2860, ), (0.3530, ))\n",
    "    ])\n",
    "    \n",
    "    # train dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(root=data_root, train=True, download=True, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # test dataloader\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(root=data_root, train=False, download=True, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4. System Configuration</font><a name=\"sys-config\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfiguration:\n",
    "    '''\n",
    "    Describes the common system setting needed for reproducible training\n",
    "    '''\n",
    "    seed: int = 42  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5. Training Configuration</font><a name=\"train-config\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 32  # amount of data to pass through the network at each forward-backward iteration\n",
    "    epochs_count: int = 20  # number of times the whole dataset will be passed through the network\n",
    "    learning_rate: float = 0.01  # determines the speed of network's weights update\n",
    "    log_interval: int = 100  # how many batches to wait between logging training status\n",
    "    test_interval: int = 1  # how many epochs to wait before another test. Set to 1 to get val loss at each epoch\n",
    "    data_root: str = \"data\"  # folder to save MNIST data (default: data)\n",
    "    num_workers: int = 10  # number of concurrent processes used to prepare data\n",
    "    device: str = 'cuda'  # device to use for training.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">6. System Setup</font><a name=\"sys-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_system(system_config: SystemConfiguration) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">7. Training</font><a name=\"training\"></a>\n",
    "We are familiar with the training pipeline used in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",
    ") -> None:\n",
    "    \n",
    "    # change model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # to get batch loss\n",
    "    batch_loss = np.array([])\n",
    "    \n",
    "    # to get batch accuracy\n",
    "    batch_acc = np.array([])\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # clone target\n",
    "        indx_target = target.clone()\n",
    "        # send data to device (its is medatory if GPU has to be used)\n",
    "        data = data.to(train_config.device)\n",
    "        # send target to device\n",
    "        target = target.to(train_config.device)\n",
    "\n",
    "        # reset parameters gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # cross entropy loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        # find gradients w.r.t training parameters\n",
    "        loss.backward()\n",
    "        # Update parameters using gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = np.append(batch_loss, [loss.item()])\n",
    "        \n",
    "        # get probability score using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "            \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]  \n",
    "                        \n",
    "        # correct prediction\n",
    "        correct = pred.cpu().eq(indx_target).sum()\n",
    "            \n",
    "        # accuracy\n",
    "        acc = float(correct) / float(len(data))\n",
    "        \n",
    "        batch_acc = np.append(batch_acc, [acc])\n",
    "\n",
    "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:              \n",
    "            print(\n",
    "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
    "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    epoch_loss = batch_loss.mean()\n",
    "    epoch_acc = batch_acc.mean()\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">8. Validation</font><a name=\"validation\"></a>\n",
    "\n",
    "After every few epochs **`validation`** is called, with the `trained model` and `test_loader` to get validation loss and accuracy.\n",
    "\n",
    "**Note:** We use `model.eval()` to enable evaluation mode of the model. This will stop calculating the running estimate of mean and variance of data. Using instead just the mean and variance computed while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    train_config: TrainingConfiguration,\n",
    "    model: nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    ") -> float:\n",
    "    # \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    count_corect_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            indx_target = target.clone()\n",
    "            data = data.to(train_config.device)\n",
    "\n",
    "            target = target.to(train_config.device)\n",
    "\n",
    "            output = model(data)\n",
    "            # add loss for each mini batch\n",
    "            test_loss += F.cross_entropy(output, target).item()\n",
    "\n",
    "            # get probability score using softmax\n",
    "            prob = F.softmax(output, dim=1)\n",
    "\n",
    "            # get the index of the max probability\n",
    "            pred = prob.data.max(dim=1)[1] \n",
    "\n",
    "            # add correct prediction count\n",
    "            count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "        # average over number of mini-batches\n",
    "        test_loss = test_loss / len(test_loader)  \n",
    "\n",
    "        # average over number of dataset\n",
    "        accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
    "\n",
    "        print(\n",
    "            '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
    "            )\n",
    "        )\n",
    "    return test_loss, accuracy/100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">9. Main</font><a name=\"main\"></a>\n",
    "\n",
    "\n",
    "Here, we use the configuration parameters defined above and start  training. \n",
    "\n",
    "1. Set up system parameters like CPU/GPU, number of threads etc.\n",
    "1. Load the data using dataloaders.\n",
    "1. Create an instance of the LeNet model.\n",
    "1. Specify optimizer to use.\n",
    "1. Set up variables to track loss and accuracy and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, system_configuration=SystemConfiguration(), training_configuration=TrainingConfiguration()):\n",
    "    \n",
    "    # system configuration\n",
    "    setup_system(system_configuration)\n",
    "\n",
    "    # batch size\n",
    "    batch_size_to_set = training_configuration.batch_size\n",
    "    # num_workers\n",
    "    num_workers_to_set = training_configuration.num_workers\n",
    "    # epochs\n",
    "    epoch_num_to_set = training_configuration.epochs_count\n",
    "\n",
    "    # if GPU is available use training config, \n",
    "    # else lower batch_size, num_workers and epochs count\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        batch_size_to_set = 16\n",
    "        num_workers_to_set = 2\n",
    "        epoch_num_to_set = 10\n",
    "\n",
    "    # data loader\n",
    "    train_loader, test_loader = get_data(\n",
    "        batch_size=batch_size_to_set,\n",
    "        data_root=training_configuration.data_root,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "    \n",
    "    # Update training configuration\n",
    "    training_configuration = TrainingConfiguration(\n",
    "        device=device,\n",
    "        epochs_count=epoch_num_to_set,\n",
    "        batch_size=batch_size_to_set,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "        \n",
    "    # send model to device (GPU/CPU)\n",
    "    model.to(training_configuration.device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=training_configuration.learning_rate\n",
    "    )\n",
    "\n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    \n",
    "    # epoch train/test loss\n",
    "    epoch_train_loss = np.array([])\n",
    "    epoch_test_loss = np.array([])\n",
    "    \n",
    "    # epch train/test accuracy\n",
    "    epoch_train_acc = np.array([])\n",
    "    epoch_test_acc = np.array([])\n",
    "    \n",
    "    # trainig time measurement\n",
    "    t_begin = time.time()\n",
    "    for epoch in range(training_configuration.epochs_count):\n",
    "        \n",
    "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n",
    "        \n",
    "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
    "        \n",
    "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
    "\n",
    "        elapsed_time = time.time() - t_begin\n",
    "        speed_epoch = elapsed_time / (epoch + 1)\n",
    "        speed_batch = speed_epoch / len(train_loader)\n",
    "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
    "        \n",
    "        print(\n",
    "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
    "                elapsed_time, speed_epoch, speed_batch, eta\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if epoch % training_configuration.test_interval == 0:\n",
    "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
    "            \n",
    "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
    "        \n",
    "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
    "            \n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                \n",
    "    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n",
    "    \n",
    "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 26421880/26421880 [00:01<00:00, 17226178.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 29515/29515 [00:00<00:00, 9856280.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████| 4422102/4422102 [00:00<00:00, 14808648.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 5148/5148 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Train Epoch: 0 [3200/60000] Loss: 2.304528 Acc: 0.0625\n",
      "Train Epoch: 0 [6400/60000] Loss: 2.274811 Acc: 0.1562\n",
      "Train Epoch: 0 [9600/60000] Loss: 2.084673 Acc: 0.2812\n",
      "Train Epoch: 0 [12800/60000] Loss: 1.130409 Acc: 0.6250\n",
      "Train Epoch: 0 [16000/60000] Loss: 1.081287 Acc: 0.5625\n",
      "Train Epoch: 0 [19200/60000] Loss: 0.945803 Acc: 0.7500\n",
      "Train Epoch: 0 [22400/60000] Loss: 0.813738 Acc: 0.6875\n",
      "Train Epoch: 0 [25600/60000] Loss: 0.940495 Acc: 0.5625\n",
      "Train Epoch: 0 [28800/60000] Loss: 0.735702 Acc: 0.6562\n",
      "Train Epoch: 0 [32000/60000] Loss: 0.699852 Acc: 0.6875\n",
      "Train Epoch: 0 [35200/60000] Loss: 0.889655 Acc: 0.5938\n",
      "Train Epoch: 0 [38400/60000] Loss: 0.590993 Acc: 0.8125\n",
      "Train Epoch: 0 [41600/60000] Loss: 0.682077 Acc: 0.7188\n",
      "Train Epoch: 0 [44800/60000] Loss: 0.982712 Acc: 0.6562\n",
      "Train Epoch: 0 [48000/60000] Loss: 0.699022 Acc: 0.6875\n",
      "Train Epoch: 0 [51200/60000] Loss: 0.591455 Acc: 0.8125\n",
      "Train Epoch: 0 [54400/60000] Loss: 0.740161 Acc: 0.7500\n",
      "Train Epoch: 0 [57600/60000] Loss: 0.773099 Acc: 0.7500\n",
      "Elapsed 10.14s, 10.14 s/epoch, 0.01 s/batch, ets 192.72s\n",
      "\n",
      "Test set: Average loss: 0.6635, Accuracy: 7435/10000 (74%)\n",
      "\n",
      "Train Epoch: 1 [3200/60000] Loss: 0.798943 Acc: 0.6875\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.627199 Acc: 0.7500\n",
      "Train Epoch: 1 [9600/60000] Loss: 0.584241 Acc: 0.6875\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.690780 Acc: 0.7812\n",
      "Train Epoch: 1 [16000/60000] Loss: 0.549200 Acc: 0.8438\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.781625 Acc: 0.5625\n",
      "Train Epoch: 1 [22400/60000] Loss: 0.401050 Acc: 0.8750\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.546427 Acc: 0.7812\n",
      "Train Epoch: 1 [28800/60000] Loss: 0.470583 Acc: 0.8125\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.422139 Acc: 0.8438\n",
      "Train Epoch: 1 [35200/60000] Loss: 0.356016 Acc: 0.8750\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.575987 Acc: 0.7500\n",
      "Train Epoch: 1 [41600/60000] Loss: 0.498739 Acc: 0.8438\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.605207 Acc: 0.7500\n",
      "Train Epoch: 1 [48000/60000] Loss: 0.621628 Acc: 0.7188\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.415427 Acc: 0.8125\n",
      "Train Epoch: 1 [54400/60000] Loss: 0.694914 Acc: 0.7500\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.601594 Acc: 0.6875\n",
      "Elapsed 24.75s, 12.38 s/epoch, 0.01 s/batch, ets 222.77s\n",
      "\n",
      "Test set: Average loss: 0.5376, Accuracy: 8025/10000 (80%)\n",
      "\n",
      "Train Epoch: 2 [3200/60000] Loss: 0.688084 Acc: 0.8125\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.451595 Acc: 0.8438\n",
      "Train Epoch: 2 [9600/60000] Loss: 0.256494 Acc: 0.9062\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.547814 Acc: 0.8438\n",
      "Train Epoch: 2 [16000/60000] Loss: 0.304045 Acc: 0.9062\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.879389 Acc: 0.6875\n",
      "Train Epoch: 2 [22400/60000] Loss: 0.386996 Acc: 0.8750\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.451074 Acc: 0.8438\n",
      "Train Epoch: 2 [28800/60000] Loss: 0.477066 Acc: 0.8125\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.142226 Acc: 1.0000\n",
      "Train Epoch: 2 [35200/60000] Loss: 0.685894 Acc: 0.7500\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.408211 Acc: 0.8438\n",
      "Train Epoch: 2 [41600/60000] Loss: 0.501861 Acc: 0.9062\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.456819 Acc: 0.8125\n",
      "Train Epoch: 2 [48000/60000] Loss: 0.233022 Acc: 0.9062\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.291540 Acc: 0.9062\n",
      "Train Epoch: 2 [54400/60000] Loss: 0.266293 Acc: 0.9062\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.530565 Acc: 0.7812\n",
      "Elapsed 39.18s, 13.06 s/epoch, 0.01 s/batch, ets 222.00s\n",
      "\n",
      "Test set: Average loss: 0.4588, Accuracy: 8299/10000 (83%)\n",
      "\n",
      "Train Epoch: 3 [3200/60000] Loss: 0.347816 Acc: 0.8750\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.278817 Acc: 0.9375\n",
      "Train Epoch: 3 [9600/60000] Loss: 0.556452 Acc: 0.8125\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.675050 Acc: 0.8438\n",
      "Train Epoch: 3 [16000/60000] Loss: 0.332311 Acc: 0.8750\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.397074 Acc: 0.8438\n",
      "Train Epoch: 3 [22400/60000] Loss: 0.781549 Acc: 0.7188\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.296920 Acc: 0.9062\n",
      "Train Epoch: 3 [28800/60000] Loss: 0.449081 Acc: 0.8750\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.271863 Acc: 0.8438\n",
      "Train Epoch: 3 [35200/60000] Loss: 0.348064 Acc: 0.9062\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.241279 Acc: 0.9375\n",
      "Train Epoch: 3 [41600/60000] Loss: 0.369103 Acc: 0.8438\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.241103 Acc: 0.9375\n",
      "Train Epoch: 3 [48000/60000] Loss: 0.674593 Acc: 0.6250\n",
      "Train Epoch: 3 [51200/60000] Loss: 1.030354 Acc: 0.7812\n",
      "Train Epoch: 3 [54400/60000] Loss: 0.661111 Acc: 0.7188\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.258541 Acc: 0.9375\n",
      "Elapsed 53.63s, 13.41 s/epoch, 0.01 s/batch, ets 214.50s\n",
      "\n",
      "Test set: Average loss: 0.4181, Accuracy: 8459/10000 (85%)\n",
      "\n",
      "Train Epoch: 4 [3200/60000] Loss: 0.316864 Acc: 0.8750\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.341473 Acc: 0.8750\n",
      "Train Epoch: 4 [9600/60000] Loss: 0.419189 Acc: 0.8438\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.375565 Acc: 0.8125\n",
      "Train Epoch: 4 [16000/60000] Loss: 0.123402 Acc: 1.0000\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.533627 Acc: 0.8438\n",
      "Train Epoch: 4 [22400/60000] Loss: 0.307624 Acc: 0.8750\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.269561 Acc: 0.9062\n",
      "Train Epoch: 4 [28800/60000] Loss: 0.433219 Acc: 0.7812\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.331322 Acc: 0.8750\n",
      "Train Epoch: 4 [35200/60000] Loss: 0.447193 Acc: 0.7812\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.336129 Acc: 0.8750\n",
      "Train Epoch: 4 [41600/60000] Loss: 0.317870 Acc: 0.8750\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.292041 Acc: 0.8125\n",
      "Train Epoch: 4 [48000/60000] Loss: 0.316480 Acc: 0.9062\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.453499 Acc: 0.8125\n",
      "Train Epoch: 4 [54400/60000] Loss: 0.262035 Acc: 0.8750\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.559697 Acc: 0.8125\n",
      "Elapsed 68.28s, 13.66 s/epoch, 0.01 s/batch, ets 204.85s\n",
      "\n",
      "Test set: Average loss: 0.4024, Accuracy: 8542/10000 (85%)\n",
      "\n",
      "Train Epoch: 5 [3200/60000] Loss: 0.552276 Acc: 0.8438\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.284182 Acc: 0.9375\n",
      "Train Epoch: 5 [9600/60000] Loss: 0.257308 Acc: 0.9062\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.424800 Acc: 0.9062\n",
      "Train Epoch: 5 [16000/60000] Loss: 0.409331 Acc: 0.7812\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.373067 Acc: 0.8438\n",
      "Train Epoch: 5 [22400/60000] Loss: 0.452987 Acc: 0.7500\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.135412 Acc: 0.9688\n",
      "Train Epoch: 5 [28800/60000] Loss: 0.432466 Acc: 0.7812\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.266840 Acc: 0.8750\n",
      "Train Epoch: 5 [35200/60000] Loss: 0.224322 Acc: 0.9375\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.335744 Acc: 0.8750\n",
      "Train Epoch: 5 [41600/60000] Loss: 0.225510 Acc: 0.8750\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.236780 Acc: 0.8750\n",
      "Train Epoch: 5 [48000/60000] Loss: 0.571741 Acc: 0.7500\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.183637 Acc: 0.9375\n",
      "Train Epoch: 5 [54400/60000] Loss: 0.283209 Acc: 0.8750\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.257917 Acc: 0.9375\n",
      "Elapsed 82.85s, 13.81 s/epoch, 0.01 s/batch, ets 193.31s\n",
      "\n",
      "Test set: Average loss: 0.3947, Accuracy: 8606/10000 (86%)\n",
      "\n",
      "Train Epoch: 6 [3200/60000] Loss: 0.129027 Acc: 0.9375\n",
      "Train Epoch: 6 [6400/60000] Loss: 0.287004 Acc: 0.9062\n",
      "Train Epoch: 6 [9600/60000] Loss: 0.308027 Acc: 0.9062\n",
      "Train Epoch: 6 [12800/60000] Loss: 0.223224 Acc: 0.9375\n",
      "Train Epoch: 6 [16000/60000] Loss: 0.167978 Acc: 0.9688\n",
      "Train Epoch: 6 [19200/60000] Loss: 0.104180 Acc: 1.0000\n",
      "Train Epoch: 6 [22400/60000] Loss: 0.291461 Acc: 0.9375\n",
      "Train Epoch: 6 [25600/60000] Loss: 0.322482 Acc: 0.8125\n",
      "Train Epoch: 6 [28800/60000] Loss: 0.268876 Acc: 0.8750\n",
      "Train Epoch: 6 [32000/60000] Loss: 0.266008 Acc: 0.8750\n",
      "Train Epoch: 6 [35200/60000] Loss: 0.284067 Acc: 0.9062\n",
      "Train Epoch: 6 [38400/60000] Loss: 0.605617 Acc: 0.7500\n",
      "Train Epoch: 6 [41600/60000] Loss: 0.271533 Acc: 0.9062\n",
      "Train Epoch: 6 [44800/60000] Loss: 0.575063 Acc: 0.8125\n",
      "Train Epoch: 6 [48000/60000] Loss: 0.245429 Acc: 0.8750\n",
      "Train Epoch: 6 [51200/60000] Loss: 0.279549 Acc: 0.8750\n",
      "Train Epoch: 6 [54400/60000] Loss: 0.317172 Acc: 0.9062\n",
      "Train Epoch: 6 [57600/60000] Loss: 0.445919 Acc: 0.8438\n",
      "Elapsed 97.36s, 13.91 s/epoch, 0.01 s/batch, ets 180.81s\n",
      "\n",
      "Test set: Average loss: 0.3730, Accuracy: 8657/10000 (87%)\n",
      "\n",
      "Train Epoch: 7 [3200/60000] Loss: 0.265489 Acc: 0.9062\n",
      "Train Epoch: 7 [6400/60000] Loss: 0.663315 Acc: 0.7812\n",
      "Train Epoch: 7 [9600/60000] Loss: 0.363974 Acc: 0.8750\n",
      "Train Epoch: 7 [12800/60000] Loss: 0.172860 Acc: 0.9375\n",
      "Train Epoch: 7 [16000/60000] Loss: 0.321909 Acc: 0.8750\n",
      "Train Epoch: 7 [19200/60000] Loss: 0.587400 Acc: 0.8125\n",
      "Train Epoch: 7 [22400/60000] Loss: 0.289315 Acc: 0.8750\n",
      "Train Epoch: 7 [25600/60000] Loss: 0.454692 Acc: 0.8438\n",
      "Train Epoch: 7 [28800/60000] Loss: 0.337997 Acc: 0.8750\n",
      "Train Epoch: 7 [32000/60000] Loss: 0.347907 Acc: 0.9062\n",
      "Train Epoch: 7 [35200/60000] Loss: 0.411851 Acc: 0.8438\n",
      "Train Epoch: 7 [38400/60000] Loss: 0.389116 Acc: 0.8438\n",
      "Train Epoch: 7 [41600/60000] Loss: 0.315890 Acc: 0.8438\n",
      "Train Epoch: 7 [44800/60000] Loss: 0.323812 Acc: 0.8750\n",
      "Train Epoch: 7 [48000/60000] Loss: 0.361529 Acc: 0.9062\n",
      "Train Epoch: 7 [51200/60000] Loss: 0.257602 Acc: 0.8750\n",
      "Train Epoch: 7 [54400/60000] Loss: 0.218759 Acc: 0.9062\n",
      "Train Epoch: 7 [57600/60000] Loss: 0.224806 Acc: 0.8438\n",
      "Elapsed 112.05s, 14.01 s/epoch, 0.01 s/batch, ets 168.07s\n",
      "\n",
      "Test set: Average loss: 0.3404, Accuracy: 8744/10000 (87%)\n",
      "\n",
      "Train Epoch: 8 [3200/60000] Loss: 0.344532 Acc: 0.8438\n",
      "Train Epoch: 8 [6400/60000] Loss: 0.353755 Acc: 0.8438\n",
      "Train Epoch: 8 [9600/60000] Loss: 0.288168 Acc: 0.9375\n",
      "Train Epoch: 8 [12800/60000] Loss: 0.270454 Acc: 0.9062\n",
      "Train Epoch: 8 [16000/60000] Loss: 0.147704 Acc: 1.0000\n",
      "Train Epoch: 8 [19200/60000] Loss: 0.112255 Acc: 0.9688\n",
      "Train Epoch: 8 [22400/60000] Loss: 0.301580 Acc: 0.8750\n",
      "Train Epoch: 8 [25600/60000] Loss: 0.348795 Acc: 0.7812\n",
      "Train Epoch: 8 [28800/60000] Loss: 0.166590 Acc: 0.9688\n",
      "Train Epoch: 8 [32000/60000] Loss: 0.132661 Acc: 0.9688\n",
      "Train Epoch: 8 [35200/60000] Loss: 0.249197 Acc: 0.9062\n",
      "Train Epoch: 8 [38400/60000] Loss: 0.505373 Acc: 0.8125\n",
      "Train Epoch: 8 [41600/60000] Loss: 0.201195 Acc: 0.9375\n",
      "Train Epoch: 8 [44800/60000] Loss: 0.514461 Acc: 0.7812\n",
      "Train Epoch: 8 [48000/60000] Loss: 0.201619 Acc: 0.9062\n",
      "Train Epoch: 8 [51200/60000] Loss: 0.277004 Acc: 0.8438\n",
      "Train Epoch: 8 [54400/60000] Loss: 0.094361 Acc: 0.9688\n",
      "Train Epoch: 8 [57600/60000] Loss: 0.214669 Acc: 0.9688\n",
      "Elapsed 126.89s, 14.10 s/epoch, 0.01 s/batch, ets 155.09s\n",
      "\n",
      "Test set: Average loss: 0.3398, Accuracy: 8755/10000 (88%)\n",
      "\n",
      "Train Epoch: 9 [3200/60000] Loss: 0.297893 Acc: 0.9062\n",
      "Train Epoch: 9 [6400/60000] Loss: 0.159889 Acc: 0.9375\n",
      "Train Epoch: 9 [9600/60000] Loss: 0.229125 Acc: 0.9062\n",
      "Train Epoch: 9 [12800/60000] Loss: 0.200772 Acc: 0.9062\n",
      "Train Epoch: 9 [16000/60000] Loss: 0.248798 Acc: 0.9062\n",
      "Train Epoch: 9 [19200/60000] Loss: 0.289910 Acc: 0.9062\n",
      "Train Epoch: 9 [22400/60000] Loss: 0.370625 Acc: 0.8438\n",
      "Train Epoch: 9 [25600/60000] Loss: 0.385284 Acc: 0.8438\n",
      "Train Epoch: 9 [28800/60000] Loss: 0.331775 Acc: 0.8750\n",
      "Train Epoch: 9 [32000/60000] Loss: 0.369774 Acc: 0.8438\n",
      "Train Epoch: 9 [35200/60000] Loss: 0.411727 Acc: 0.8750\n",
      "Train Epoch: 9 [38400/60000] Loss: 0.271314 Acc: 0.9062\n",
      "Train Epoch: 9 [41600/60000] Loss: 0.325089 Acc: 0.9062\n",
      "Train Epoch: 9 [44800/60000] Loss: 0.563840 Acc: 0.8438\n",
      "Train Epoch: 9 [48000/60000] Loss: 0.335734 Acc: 0.8750\n",
      "Train Epoch: 9 [51200/60000] Loss: 0.416848 Acc: 0.9062\n",
      "Train Epoch: 9 [54400/60000] Loss: 0.204519 Acc: 0.9375\n",
      "Train Epoch: 9 [57600/60000] Loss: 0.194590 Acc: 0.9062\n",
      "Elapsed 141.63s, 14.16 s/epoch, 0.01 s/batch, ets 141.63s\n",
      "\n",
      "Test set: Average loss: 0.3408, Accuracy: 8741/10000 (87%)\n",
      "\n",
      "Train Epoch: 10 [3200/60000] Loss: 0.321324 Acc: 0.9375\n",
      "Train Epoch: 10 [6400/60000] Loss: 0.296945 Acc: 0.9062\n",
      "Train Epoch: 10 [9600/60000] Loss: 0.177367 Acc: 0.9375\n",
      "Train Epoch: 10 [12800/60000] Loss: 0.383026 Acc: 0.8438\n",
      "Train Epoch: 10 [16000/60000] Loss: 0.237842 Acc: 0.9062\n",
      "Train Epoch: 10 [19200/60000] Loss: 0.534930 Acc: 0.8750\n",
      "Train Epoch: 10 [22400/60000] Loss: 0.375302 Acc: 0.8750\n",
      "Train Epoch: 10 [25600/60000] Loss: 0.339697 Acc: 0.9375\n",
      "Train Epoch: 10 [28800/60000] Loss: 0.539701 Acc: 0.8125\n",
      "Train Epoch: 10 [32000/60000] Loss: 0.274692 Acc: 0.8750\n",
      "Train Epoch: 10 [35200/60000] Loss: 0.181896 Acc: 0.9062\n",
      "Train Epoch: 10 [38400/60000] Loss: 0.115324 Acc: 1.0000\n",
      "Train Epoch: 10 [41600/60000] Loss: 0.102751 Acc: 1.0000\n",
      "Train Epoch: 10 [44800/60000] Loss: 0.311616 Acc: 0.8438\n",
      "Train Epoch: 10 [48000/60000] Loss: 0.202405 Acc: 0.9375\n",
      "Train Epoch: 10 [51200/60000] Loss: 0.134713 Acc: 0.9375\n",
      "Train Epoch: 10 [54400/60000] Loss: 0.329943 Acc: 0.9062\n",
      "Train Epoch: 10 [57600/60000] Loss: 0.532233 Acc: 0.8125\n",
      "Elapsed 156.06s, 14.19 s/epoch, 0.01 s/batch, ets 127.68s\n",
      "\n",
      "Test set: Average loss: 0.3242, Accuracy: 8796/10000 (88%)\n",
      "\n",
      "Train Epoch: 11 [3200/60000] Loss: 0.109210 Acc: 0.9688\n",
      "Train Epoch: 11 [6400/60000] Loss: 0.435918 Acc: 0.7812\n",
      "Train Epoch: 11 [9600/60000] Loss: 0.225765 Acc: 0.8750\n",
      "Train Epoch: 11 [12800/60000] Loss: 0.380199 Acc: 0.8750\n",
      "Train Epoch: 11 [16000/60000] Loss: 0.265276 Acc: 0.8750\n",
      "Train Epoch: 11 [19200/60000] Loss: 0.173725 Acc: 0.9688\n",
      "Train Epoch: 11 [22400/60000] Loss: 0.160137 Acc: 0.9375\n",
      "Train Epoch: 11 [25600/60000] Loss: 0.110782 Acc: 1.0000\n",
      "Train Epoch: 11 [28800/60000] Loss: 0.598675 Acc: 0.8438\n",
      "Train Epoch: 11 [32000/60000] Loss: 0.165733 Acc: 0.9375\n",
      "Train Epoch: 11 [35200/60000] Loss: 0.204649 Acc: 0.9688\n",
      "Train Epoch: 11 [38400/60000] Loss: 0.260979 Acc: 0.9375\n",
      "Train Epoch: 11 [41600/60000] Loss: 0.186564 Acc: 0.9062\n",
      "Train Epoch: 11 [44800/60000] Loss: 0.292348 Acc: 0.8750\n",
      "Train Epoch: 11 [48000/60000] Loss: 0.330411 Acc: 0.8438\n",
      "Train Epoch: 11 [51200/60000] Loss: 0.140201 Acc: 0.9375\n",
      "Train Epoch: 11 [54400/60000] Loss: 0.090085 Acc: 1.0000\n",
      "Train Epoch: 11 [57600/60000] Loss: 0.364251 Acc: 0.7812\n",
      "Elapsed 170.44s, 14.20 s/epoch, 0.01 s/batch, ets 113.63s\n",
      "\n",
      "Test set: Average loss: 0.3270, Accuracy: 8815/10000 (88%)\n",
      "\n",
      "Train Epoch: 12 [3200/60000] Loss: 0.260714 Acc: 0.9062\n",
      "Train Epoch: 12 [6400/60000] Loss: 0.336616 Acc: 0.8750\n",
      "Train Epoch: 12 [9600/60000] Loss: 0.215671 Acc: 0.9375\n",
      "Train Epoch: 12 [12800/60000] Loss: 0.165232 Acc: 0.9375\n",
      "Train Epoch: 12 [16000/60000] Loss: 0.166075 Acc: 0.9062\n",
      "Train Epoch: 12 [19200/60000] Loss: 0.554957 Acc: 0.8125\n",
      "Train Epoch: 12 [22400/60000] Loss: 0.277170 Acc: 0.9062\n",
      "Train Epoch: 12 [25600/60000] Loss: 0.169535 Acc: 0.8750\n",
      "Train Epoch: 12 [28800/60000] Loss: 0.455595 Acc: 0.8438\n",
      "Train Epoch: 12 [32000/60000] Loss: 0.384419 Acc: 0.8750\n",
      "Train Epoch: 12 [35200/60000] Loss: 0.108158 Acc: 0.9375\n",
      "Train Epoch: 12 [38400/60000] Loss: 0.403891 Acc: 0.8750\n",
      "Train Epoch: 12 [41600/60000] Loss: 0.150171 Acc: 0.9688\n",
      "Train Epoch: 12 [44800/60000] Loss: 0.125859 Acc: 0.9688\n",
      "Train Epoch: 12 [48000/60000] Loss: 0.104954 Acc: 1.0000\n",
      "Train Epoch: 12 [51200/60000] Loss: 0.330955 Acc: 0.9062\n",
      "Train Epoch: 12 [54400/60000] Loss: 0.502482 Acc: 0.7812\n",
      "Train Epoch: 12 [57600/60000] Loss: 0.608613 Acc: 0.7500\n",
      "Elapsed 185.39s, 14.26 s/epoch, 0.01 s/batch, ets 99.83s\n",
      "\n",
      "Test set: Average loss: 0.3092, Accuracy: 8871/10000 (89%)\n",
      "\n",
      "Train Epoch: 13 [3200/60000] Loss: 0.226774 Acc: 0.9062\n",
      "Train Epoch: 13 [6400/60000] Loss: 0.285834 Acc: 0.8750\n",
      "Train Epoch: 13 [9600/60000] Loss: 0.180371 Acc: 0.9062\n",
      "Train Epoch: 13 [12800/60000] Loss: 0.258353 Acc: 0.9375\n",
      "Train Epoch: 13 [16000/60000] Loss: 0.156732 Acc: 0.9375\n",
      "Train Epoch: 13 [19200/60000] Loss: 0.291236 Acc: 0.8750\n",
      "Train Epoch: 13 [22400/60000] Loss: 0.121016 Acc: 0.9375\n",
      "Train Epoch: 13 [25600/60000] Loss: 0.434475 Acc: 0.8750\n",
      "Train Epoch: 13 [28800/60000] Loss: 0.190186 Acc: 0.9062\n",
      "Train Epoch: 13 [32000/60000] Loss: 0.266481 Acc: 0.8750\n",
      "Train Epoch: 13 [35200/60000] Loss: 0.484168 Acc: 0.8125\n",
      "Train Epoch: 13 [38400/60000] Loss: 0.206034 Acc: 0.9062\n",
      "Train Epoch: 13 [41600/60000] Loss: 0.283853 Acc: 0.8438\n",
      "Train Epoch: 13 [44800/60000] Loss: 0.044732 Acc: 1.0000\n",
      "Train Epoch: 13 [48000/60000] Loss: 0.209691 Acc: 0.9062\n",
      "Train Epoch: 13 [51200/60000] Loss: 0.446205 Acc: 0.8438\n",
      "Train Epoch: 13 [54400/60000] Loss: 0.530831 Acc: 0.7812\n",
      "Train Epoch: 13 [57600/60000] Loss: 0.173998 Acc: 0.8750\n",
      "Elapsed 199.58s, 14.26 s/epoch, 0.01 s/batch, ets 85.53s\n",
      "\n",
      "Test set: Average loss: 0.3132, Accuracy: 8867/10000 (89%)\n",
      "\n",
      "Train Epoch: 14 [3200/60000] Loss: 0.154807 Acc: 0.9375\n",
      "Train Epoch: 14 [6400/60000] Loss: 0.525559 Acc: 0.7812\n",
      "Train Epoch: 14 [9600/60000] Loss: 0.358557 Acc: 0.9062\n",
      "Train Epoch: 14 [12800/60000] Loss: 0.114559 Acc: 1.0000\n",
      "Train Epoch: 14 [16000/60000] Loss: 0.341130 Acc: 0.8750\n",
      "Train Epoch: 14 [19200/60000] Loss: 0.193039 Acc: 0.9375\n",
      "Train Epoch: 14 [22400/60000] Loss: 0.621594 Acc: 0.8125\n",
      "Train Epoch: 14 [25600/60000] Loss: 0.231834 Acc: 0.9062\n",
      "Train Epoch: 14 [28800/60000] Loss: 0.551292 Acc: 0.8125\n",
      "Train Epoch: 14 [32000/60000] Loss: 0.178696 Acc: 0.9062\n",
      "Train Epoch: 14 [35200/60000] Loss: 0.387831 Acc: 0.9375\n",
      "Train Epoch: 14 [38400/60000] Loss: 0.274848 Acc: 0.8750\n",
      "Train Epoch: 14 [41600/60000] Loss: 0.163857 Acc: 0.9375\n",
      "Train Epoch: 14 [44800/60000] Loss: 0.311070 Acc: 0.9375\n",
      "Train Epoch: 14 [48000/60000] Loss: 0.216875 Acc: 0.9375\n",
      "Train Epoch: 14 [51200/60000] Loss: 0.242557 Acc: 0.8750\n",
      "Train Epoch: 14 [54400/60000] Loss: 0.313639 Acc: 0.8438\n",
      "Train Epoch: 14 [57600/60000] Loss: 0.291084 Acc: 0.8125\n",
      "Elapsed 214.22s, 14.28 s/epoch, 0.01 s/batch, ets 71.41s\n",
      "\n",
      "Test set: Average loss: 0.3145, Accuracy: 8828/10000 (88%)\n",
      "\n",
      "Train Epoch: 15 [3200/60000] Loss: 0.407673 Acc: 0.8438\n",
      "Train Epoch: 15 [6400/60000] Loss: 0.237271 Acc: 0.9062\n",
      "Train Epoch: 15 [9600/60000] Loss: 0.093701 Acc: 0.9688\n",
      "Train Epoch: 15 [12800/60000] Loss: 0.215383 Acc: 0.8750\n",
      "Train Epoch: 15 [16000/60000] Loss: 0.220551 Acc: 0.9688\n",
      "Train Epoch: 15 [19200/60000] Loss: 0.310998 Acc: 0.8438\n",
      "Train Epoch: 15 [22400/60000] Loss: 0.353283 Acc: 0.9062\n",
      "Train Epoch: 15 [25600/60000] Loss: 0.211475 Acc: 0.9375\n",
      "Train Epoch: 15 [28800/60000] Loss: 0.407477 Acc: 0.8750\n",
      "Train Epoch: 15 [32000/60000] Loss: 0.302949 Acc: 0.8438\n",
      "Train Epoch: 15 [35200/60000] Loss: 0.269856 Acc: 0.9062\n",
      "Train Epoch: 15 [38400/60000] Loss: 0.238631 Acc: 0.9375\n",
      "Train Epoch: 15 [41600/60000] Loss: 0.411712 Acc: 0.9062\n",
      "Train Epoch: 15 [44800/60000] Loss: 0.334546 Acc: 0.8750\n",
      "Train Epoch: 15 [48000/60000] Loss: 0.356783 Acc: 0.9375\n",
      "Train Epoch: 15 [51200/60000] Loss: 0.206744 Acc: 0.9062\n",
      "Train Epoch: 15 [54400/60000] Loss: 0.219229 Acc: 0.9375\n",
      "Train Epoch: 15 [57600/60000] Loss: 0.227079 Acc: 0.9062\n",
      "Elapsed 228.57s, 14.29 s/epoch, 0.01 s/batch, ets 57.14s\n",
      "\n",
      "Test set: Average loss: 0.3055, Accuracy: 8906/10000 (89%)\n",
      "\n",
      "Train Epoch: 16 [3200/60000] Loss: 0.170447 Acc: 0.9375\n",
      "Train Epoch: 16 [6400/60000] Loss: 0.181575 Acc: 0.9375\n",
      "Train Epoch: 16 [9600/60000] Loss: 0.331324 Acc: 0.8750\n",
      "Train Epoch: 16 [12800/60000] Loss: 0.202362 Acc: 0.9375\n",
      "Train Epoch: 16 [16000/60000] Loss: 0.174901 Acc: 0.9375\n",
      "Train Epoch: 16 [19200/60000] Loss: 0.219046 Acc: 0.9375\n",
      "Train Epoch: 16 [22400/60000] Loss: 0.125166 Acc: 0.9375\n",
      "Train Epoch: 16 [25600/60000] Loss: 0.371145 Acc: 0.8125\n",
      "Train Epoch: 16 [28800/60000] Loss: 0.419632 Acc: 0.9062\n",
      "Train Epoch: 16 [32000/60000] Loss: 0.212599 Acc: 0.9062\n",
      "Train Epoch: 16 [35200/60000] Loss: 0.138941 Acc: 0.9688\n",
      "Train Epoch: 16 [38400/60000] Loss: 0.377253 Acc: 0.8750\n",
      "Train Epoch: 16 [41600/60000] Loss: 0.183782 Acc: 0.9375\n",
      "Train Epoch: 16 [44800/60000] Loss: 0.327053 Acc: 0.9062\n",
      "Train Epoch: 16 [48000/60000] Loss: 0.201777 Acc: 0.9062\n",
      "Train Epoch: 16 [51200/60000] Loss: 0.059142 Acc: 1.0000\n",
      "Train Epoch: 16 [54400/60000] Loss: 0.196593 Acc: 0.9688\n",
      "Train Epoch: 16 [57600/60000] Loss: 0.157118 Acc: 0.9062\n",
      "Elapsed 243.00s, 14.29 s/epoch, 0.01 s/batch, ets 42.88s\n",
      "\n",
      "Test set: Average loss: 0.3165, Accuracy: 8833/10000 (88%)\n",
      "\n",
      "Train Epoch: 17 [3200/60000] Loss: 0.333072 Acc: 0.8125\n",
      "Train Epoch: 17 [6400/60000] Loss: 0.136345 Acc: 0.9688\n",
      "Train Epoch: 17 [9600/60000] Loss: 0.307133 Acc: 0.8750\n",
      "Train Epoch: 17 [12800/60000] Loss: 0.193191 Acc: 0.8750\n",
      "Train Epoch: 17 [16000/60000] Loss: 0.278408 Acc: 0.9062\n",
      "Train Epoch: 17 [19200/60000] Loss: 0.119480 Acc: 0.9688\n",
      "Train Epoch: 17 [22400/60000] Loss: 0.202196 Acc: 0.9375\n",
      "Train Epoch: 17 [25600/60000] Loss: 0.195753 Acc: 0.9062\n",
      "Train Epoch: 17 [28800/60000] Loss: 0.386108 Acc: 0.8750\n",
      "Train Epoch: 17 [32000/60000] Loss: 0.116400 Acc: 0.9688\n",
      "Train Epoch: 17 [35200/60000] Loss: 0.218453 Acc: 0.8750\n",
      "Train Epoch: 17 [38400/60000] Loss: 0.193860 Acc: 0.9375\n",
      "Train Epoch: 17 [41600/60000] Loss: 0.219429 Acc: 0.9062\n",
      "Train Epoch: 17 [44800/60000] Loss: 0.403429 Acc: 0.8750\n",
      "Train Epoch: 17 [48000/60000] Loss: 0.350093 Acc: 0.8438\n",
      "Train Epoch: 17 [51200/60000] Loss: 0.259460 Acc: 0.8438\n",
      "Train Epoch: 17 [54400/60000] Loss: 0.271586 Acc: 0.8750\n",
      "Train Epoch: 17 [57600/60000] Loss: 0.315691 Acc: 0.9062\n",
      "Elapsed 257.00s, 14.28 s/epoch, 0.01 s/batch, ets 28.56s\n",
      "\n",
      "Test set: Average loss: 0.3030, Accuracy: 8892/10000 (89%)\n",
      "\n",
      "Train Epoch: 18 [3200/60000] Loss: 0.499663 Acc: 0.8125\n",
      "Train Epoch: 18 [6400/60000] Loss: 0.224072 Acc: 0.8750\n",
      "Train Epoch: 18 [9600/60000] Loss: 0.319244 Acc: 0.9062\n",
      "Train Epoch: 18 [12800/60000] Loss: 0.046819 Acc: 1.0000\n",
      "Train Epoch: 18 [16000/60000] Loss: 0.147632 Acc: 0.9375\n",
      "Train Epoch: 18 [19200/60000] Loss: 0.146876 Acc: 0.9688\n",
      "Train Epoch: 18 [22400/60000] Loss: 0.120374 Acc: 0.9688\n",
      "Train Epoch: 18 [25600/60000] Loss: 0.354596 Acc: 0.8438\n",
      "Train Epoch: 18 [28800/60000] Loss: 0.138383 Acc: 0.9688\n",
      "Train Epoch: 18 [32000/60000] Loss: 0.305656 Acc: 0.9062\n",
      "Train Epoch: 18 [35200/60000] Loss: 0.279126 Acc: 0.9062\n",
      "Train Epoch: 18 [38400/60000] Loss: 0.129566 Acc: 0.9688\n",
      "Train Epoch: 18 [41600/60000] Loss: 0.231144 Acc: 0.9375\n",
      "Train Epoch: 18 [44800/60000] Loss: 0.239436 Acc: 0.8438\n",
      "Train Epoch: 18 [48000/60000] Loss: 0.069986 Acc: 0.9688\n",
      "Train Epoch: 18 [51200/60000] Loss: 0.280577 Acc: 0.9375\n",
      "Train Epoch: 18 [54400/60000] Loss: 0.268660 Acc: 0.9062\n",
      "Train Epoch: 18 [57600/60000] Loss: 0.303974 Acc: 0.8750\n",
      "Elapsed 271.24s, 14.28 s/epoch, 0.01 s/batch, ets 14.28s\n",
      "\n",
      "Test set: Average loss: 0.2982, Accuracy: 8881/10000 (89%)\n",
      "\n",
      "Train Epoch: 19 [3200/60000] Loss: 0.296084 Acc: 0.9688\n",
      "Train Epoch: 19 [6400/60000] Loss: 0.111475 Acc: 0.9688\n",
      "Train Epoch: 19 [9600/60000] Loss: 0.202895 Acc: 0.9375\n",
      "Train Epoch: 19 [12800/60000] Loss: 0.197489 Acc: 0.9062\n",
      "Train Epoch: 19 [16000/60000] Loss: 0.225846 Acc: 0.9375\n",
      "Train Epoch: 19 [19200/60000] Loss: 0.228549 Acc: 0.9375\n",
      "Train Epoch: 19 [22400/60000] Loss: 0.063123 Acc: 1.0000\n",
      "Train Epoch: 19 [25600/60000] Loss: 0.257786 Acc: 0.8438\n",
      "Train Epoch: 19 [28800/60000] Loss: 0.123757 Acc: 0.9375\n",
      "Train Epoch: 19 [32000/60000] Loss: 0.204882 Acc: 0.9062\n",
      "Train Epoch: 19 [35200/60000] Loss: 0.196191 Acc: 0.8750\n",
      "Train Epoch: 19 [38400/60000] Loss: 0.246084 Acc: 0.8750\n",
      "Train Epoch: 19 [41600/60000] Loss: 0.273973 Acc: 0.9062\n",
      "Train Epoch: 19 [44800/60000] Loss: 0.489274 Acc: 0.9062\n",
      "Train Epoch: 19 [48000/60000] Loss: 0.108945 Acc: 0.9375\n",
      "Train Epoch: 19 [51200/60000] Loss: 0.213765 Acc: 0.9688\n",
      "Train Epoch: 19 [54400/60000] Loss: 0.413670 Acc: 0.8125\n",
      "Train Epoch: 19 [57600/60000] Loss: 0.253363 Acc: 0.8750\n",
      "Elapsed 285.45s, 14.27 s/epoch, 0.01 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.3075, Accuracy: 8852/10000 (89%)\n",
      "\n",
      "Total time: 289.97, Best Loss: 0.298\n",
      "Train Epoch: 0 [3200/60000] Loss: 1.842369 Acc: 0.3750\n",
      "Train Epoch: 0 [6400/60000] Loss: 1.193115 Acc: 0.5312\n",
      "Train Epoch: 0 [9600/60000] Loss: 1.011325 Acc: 0.6875\n",
      "Train Epoch: 0 [12800/60000] Loss: 0.806264 Acc: 0.6875\n",
      "Train Epoch: 0 [16000/60000] Loss: 0.740050 Acc: 0.7500\n",
      "Train Epoch: 0 [19200/60000] Loss: 0.640651 Acc: 0.7500\n",
      "Train Epoch: 0 [22400/60000] Loss: 0.554633 Acc: 0.8438\n",
      "Train Epoch: 0 [25600/60000] Loss: 0.662229 Acc: 0.7500\n",
      "Train Epoch: 0 [28800/60000] Loss: 0.541719 Acc: 0.8438\n",
      "Train Epoch: 0 [32000/60000] Loss: 0.419956 Acc: 0.8125\n",
      "Train Epoch: 0 [35200/60000] Loss: 0.591860 Acc: 0.8438\n",
      "Train Epoch: 0 [38400/60000] Loss: 0.352541 Acc: 0.8438\n",
      "Train Epoch: 0 [41600/60000] Loss: 0.511040 Acc: 0.8438\n",
      "Train Epoch: 0 [44800/60000] Loss: 0.550380 Acc: 0.8438\n",
      "Train Epoch: 0 [48000/60000] Loss: 0.571645 Acc: 0.8125\n",
      "Train Epoch: 0 [51200/60000] Loss: 0.325007 Acc: 0.8438\n",
      "Train Epoch: 0 [54400/60000] Loss: 0.493785 Acc: 0.8125\n",
      "Train Epoch: 0 [57600/60000] Loss: 0.498728 Acc: 0.8750\n",
      "Elapsed 10.48s, 10.48 s/epoch, 0.01 s/batch, ets 199.07s\n",
      "\n",
      "Test set: Average loss: 0.4397, Accuracy: 8418/10000 (84%)\n",
      "\n",
      "Train Epoch: 1 [3200/60000] Loss: 0.577146 Acc: 0.7812\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.642585 Acc: 0.7812\n",
      "Train Epoch: 1 [9600/60000] Loss: 0.458588 Acc: 0.7500\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.566070 Acc: 0.8438\n",
      "Train Epoch: 1 [16000/60000] Loss: 0.395907 Acc: 0.9062\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.771538 Acc: 0.6875\n",
      "Train Epoch: 1 [22400/60000] Loss: 0.231130 Acc: 0.9375\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.328154 Acc: 0.9062\n",
      "Train Epoch: 1 [28800/60000] Loss: 0.323049 Acc: 0.8750\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.290191 Acc: 0.9062\n",
      "Train Epoch: 1 [35200/60000] Loss: 0.147008 Acc: 0.9688\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.380366 Acc: 0.8125\n",
      "Train Epoch: 1 [41600/60000] Loss: 0.282255 Acc: 0.9062\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.354991 Acc: 0.8438\n",
      "Train Epoch: 1 [48000/60000] Loss: 0.442119 Acc: 0.7812\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.264229 Acc: 0.8438\n",
      "Train Epoch: 1 [54400/60000] Loss: 0.698132 Acc: 0.7812\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.537326 Acc: 0.8125\n",
      "Elapsed 25.57s, 12.79 s/epoch, 0.01 s/batch, ets 230.13s\n",
      "\n",
      "Test set: Average loss: 0.3824, Accuracy: 8621/10000 (86%)\n",
      "\n",
      "Train Epoch: 2 [3200/60000] Loss: 0.473613 Acc: 0.8438\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.448112 Acc: 0.8438\n",
      "Train Epoch: 2 [9600/60000] Loss: 0.185640 Acc: 0.9375\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.467824 Acc: 0.8750\n",
      "Train Epoch: 2 [16000/60000] Loss: 0.156190 Acc: 0.9375\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.492540 Acc: 0.8438\n",
      "Train Epoch: 2 [22400/60000] Loss: 0.410310 Acc: 0.8438\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.249025 Acc: 0.9375\n",
      "Train Epoch: 2 [28800/60000] Loss: 0.296686 Acc: 0.9062\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.164286 Acc: 0.9375\n",
      "Train Epoch: 2 [35200/60000] Loss: 0.415431 Acc: 0.8125\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.364886 Acc: 0.8125\n",
      "Train Epoch: 2 [41600/60000] Loss: 0.527164 Acc: 0.8750\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.254646 Acc: 0.9062\n",
      "Train Epoch: 2 [48000/60000] Loss: 0.144766 Acc: 0.9688\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.162930 Acc: 0.9688\n",
      "Train Epoch: 2 [54400/60000] Loss: 0.243846 Acc: 0.9062\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.197980 Acc: 0.9375\n",
      "Elapsed 40.79s, 13.60 s/epoch, 0.01 s/batch, ets 231.17s\n",
      "\n",
      "Test set: Average loss: 0.3725, Accuracy: 8650/10000 (86%)\n",
      "\n",
      "Train Epoch: 3 [3200/60000] Loss: 0.238883 Acc: 0.8750\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.215969 Acc: 0.9375\n",
      "Train Epoch: 3 [9600/60000] Loss: 0.586289 Acc: 0.7500\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.262141 Acc: 0.9062\n",
      "Train Epoch: 3 [16000/60000] Loss: 0.236175 Acc: 0.9375\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.328906 Acc: 0.8125\n",
      "Train Epoch: 3 [22400/60000] Loss: 0.470058 Acc: 0.8750\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.348489 Acc: 0.8438\n",
      "Train Epoch: 3 [28800/60000] Loss: 0.337110 Acc: 0.8750\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.190403 Acc: 0.8750\n",
      "Train Epoch: 3 [35200/60000] Loss: 0.453958 Acc: 0.8750\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.164243 Acc: 0.9062\n",
      "Train Epoch: 3 [41600/60000] Loss: 0.332946 Acc: 0.8750\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.203560 Acc: 0.9062\n",
      "Train Epoch: 3 [48000/60000] Loss: 0.775124 Acc: 0.7500\n",
      "Train Epoch: 3 [51200/60000] Loss: 0.845176 Acc: 0.8438\n",
      "Train Epoch: 3 [54400/60000] Loss: 0.398533 Acc: 0.8438\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.216101 Acc: 0.9375\n",
      "Elapsed 55.99s, 14.00 s/epoch, 0.01 s/batch, ets 223.96s\n",
      "\n",
      "Test set: Average loss: 0.3335, Accuracy: 8808/10000 (88%)\n",
      "\n",
      "Train Epoch: 4 [3200/60000] Loss: 0.227457 Acc: 0.9062\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.181314 Acc: 0.9375\n",
      "Train Epoch: 4 [9600/60000] Loss: 0.254713 Acc: 0.9375\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.192037 Acc: 0.9375\n",
      "Train Epoch: 4 [16000/60000] Loss: 0.081323 Acc: 1.0000\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.390928 Acc: 0.8438\n",
      "Train Epoch: 4 [22400/60000] Loss: 0.222240 Acc: 0.9375\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.207887 Acc: 0.9375\n",
      "Train Epoch: 4 [28800/60000] Loss: 0.374734 Acc: 0.8750\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.325747 Acc: 0.8750\n",
      "Train Epoch: 4 [35200/60000] Loss: 0.357848 Acc: 0.8750\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.242069 Acc: 0.9062\n",
      "Train Epoch: 4 [41600/60000] Loss: 0.268181 Acc: 0.9062\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.114699 Acc: 1.0000\n",
      "Train Epoch: 4 [48000/60000] Loss: 0.231364 Acc: 0.9062\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.251511 Acc: 0.9375\n",
      "Train Epoch: 4 [54400/60000] Loss: 0.210315 Acc: 0.9375\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.331677 Acc: 0.8750\n",
      "Elapsed 71.14s, 14.23 s/epoch, 0.01 s/batch, ets 213.43s\n",
      "\n",
      "Test set: Average loss: 0.3359, Accuracy: 8795/10000 (88%)\n",
      "\n",
      "Train Epoch: 5 [3200/60000] Loss: 0.325865 Acc: 0.9062\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.234359 Acc: 0.8750\n",
      "Train Epoch: 5 [9600/60000] Loss: 0.273340 Acc: 0.8750\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.345233 Acc: 0.9062\n",
      "Train Epoch: 5 [16000/60000] Loss: 0.262565 Acc: 0.8750\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.347168 Acc: 0.8125\n",
      "Train Epoch: 5 [22400/60000] Loss: 0.194177 Acc: 0.9062\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.104517 Acc: 0.9688\n",
      "Train Epoch: 5 [28800/60000] Loss: 0.247335 Acc: 0.8438\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.184619 Acc: 0.9375\n",
      "Train Epoch: 5 [35200/60000] Loss: 0.099874 Acc: 0.9688\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.360718 Acc: 0.8438\n",
      "Train Epoch: 5 [41600/60000] Loss: 0.121679 Acc: 0.9688\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.256086 Acc: 0.9062\n",
      "Train Epoch: 5 [48000/60000] Loss: 0.495024 Acc: 0.7500\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.161612 Acc: 0.9375\n",
      "Train Epoch: 5 [54400/60000] Loss: 0.221635 Acc: 0.9375\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.148099 Acc: 0.9688\n",
      "Elapsed 86.56s, 14.43 s/epoch, 0.01 s/batch, ets 201.98s\n",
      "\n",
      "Test set: Average loss: 0.3246, Accuracy: 8858/10000 (89%)\n",
      "\n",
      "Train Epoch: 6 [3200/60000] Loss: 0.120588 Acc: 0.9375\n",
      "Train Epoch: 6 [6400/60000] Loss: 0.252553 Acc: 0.9375\n",
      "Train Epoch: 6 [9600/60000] Loss: 0.196021 Acc: 0.9062\n",
      "Train Epoch: 6 [12800/60000] Loss: 0.186286 Acc: 0.8750\n",
      "Train Epoch: 6 [16000/60000] Loss: 0.224805 Acc: 0.9062\n",
      "Train Epoch: 6 [19200/60000] Loss: 0.174567 Acc: 0.9062\n",
      "Train Epoch: 6 [22400/60000] Loss: 0.265759 Acc: 0.8750\n",
      "Train Epoch: 6 [25600/60000] Loss: 0.243672 Acc: 0.9062\n",
      "Train Epoch: 6 [28800/60000] Loss: 0.296715 Acc: 0.8750\n",
      "Train Epoch: 6 [32000/60000] Loss: 0.199959 Acc: 0.8438\n",
      "Train Epoch: 6 [35200/60000] Loss: 0.355001 Acc: 0.9062\n",
      "Train Epoch: 6 [38400/60000] Loss: 0.413805 Acc: 0.8438\n",
      "Train Epoch: 6 [41600/60000] Loss: 0.129485 Acc: 0.9688\n",
      "Train Epoch: 6 [44800/60000] Loss: 0.484878 Acc: 0.8438\n",
      "Train Epoch: 6 [48000/60000] Loss: 0.271779 Acc: 0.8750\n",
      "Train Epoch: 6 [51200/60000] Loss: 0.256650 Acc: 0.9062\n",
      "Train Epoch: 6 [54400/60000] Loss: 0.297696 Acc: 0.9062\n",
      "Train Epoch: 6 [57600/60000] Loss: 0.241294 Acc: 0.9062\n",
      "Elapsed 102.08s, 14.58 s/epoch, 0.01 s/batch, ets 189.57s\n",
      "\n",
      "Test set: Average loss: 0.3393, Accuracy: 8797/10000 (88%)\n",
      "\n",
      "Train Epoch: 7 [3200/60000] Loss: 0.273570 Acc: 0.8750\n",
      "Train Epoch: 7 [6400/60000] Loss: 0.574378 Acc: 0.8438\n",
      "Train Epoch: 7 [9600/60000] Loss: 0.301415 Acc: 0.9062\n",
      "Train Epoch: 7 [12800/60000] Loss: 0.286356 Acc: 0.9062\n",
      "Train Epoch: 7 [16000/60000] Loss: 0.192655 Acc: 0.9375\n",
      "Train Epoch: 7 [19200/60000] Loss: 0.434308 Acc: 0.8438\n",
      "Train Epoch: 7 [22400/60000] Loss: 0.234815 Acc: 0.9062\n",
      "Train Epoch: 7 [25600/60000] Loss: 0.370882 Acc: 0.8750\n",
      "Train Epoch: 7 [28800/60000] Loss: 0.256945 Acc: 0.8750\n",
      "Train Epoch: 7 [32000/60000] Loss: 0.267702 Acc: 0.9062\n",
      "Train Epoch: 7 [35200/60000] Loss: 0.332279 Acc: 0.9062\n",
      "Train Epoch: 7 [38400/60000] Loss: 0.421477 Acc: 0.8438\n",
      "Train Epoch: 7 [41600/60000] Loss: 0.276718 Acc: 0.8438\n",
      "Train Epoch: 7 [44800/60000] Loss: 0.207631 Acc: 0.8750\n",
      "Train Epoch: 7 [48000/60000] Loss: 0.264603 Acc: 0.9062\n",
      "Train Epoch: 7 [51200/60000] Loss: 0.132504 Acc: 0.9688\n",
      "Train Epoch: 7 [54400/60000] Loss: 0.192758 Acc: 0.9062\n",
      "Train Epoch: 7 [57600/60000] Loss: 0.212687 Acc: 0.9688\n",
      "Elapsed 117.31s, 14.66 s/epoch, 0.01 s/batch, ets 175.96s\n",
      "\n",
      "Test set: Average loss: 0.2979, Accuracy: 8921/10000 (89%)\n",
      "\n",
      "Train Epoch: 8 [3200/60000] Loss: 0.237591 Acc: 0.9062\n",
      "Train Epoch: 8 [6400/60000] Loss: 0.227967 Acc: 0.8750\n",
      "Train Epoch: 8 [9600/60000] Loss: 0.312103 Acc: 0.9062\n",
      "Train Epoch: 8 [12800/60000] Loss: 0.211484 Acc: 0.9688\n",
      "Train Epoch: 8 [16000/60000] Loss: 0.123377 Acc: 0.9375\n",
      "Train Epoch: 8 [19200/60000] Loss: 0.117831 Acc: 0.9688\n",
      "Train Epoch: 8 [22400/60000] Loss: 0.149030 Acc: 0.9375\n",
      "Train Epoch: 8 [25600/60000] Loss: 0.172309 Acc: 0.9375\n",
      "Train Epoch: 8 [28800/60000] Loss: 0.116786 Acc: 0.9688\n",
      "Train Epoch: 8 [32000/60000] Loss: 0.089132 Acc: 0.9688\n",
      "Train Epoch: 8 [35200/60000] Loss: 0.215666 Acc: 0.9375\n",
      "Train Epoch: 8 [38400/60000] Loss: 0.326447 Acc: 0.8438\n",
      "Train Epoch: 8 [41600/60000] Loss: 0.135529 Acc: 0.9375\n",
      "Train Epoch: 8 [44800/60000] Loss: 0.394004 Acc: 0.8125\n",
      "Train Epoch: 8 [48000/60000] Loss: 0.276426 Acc: 0.9062\n",
      "Train Epoch: 8 [51200/60000] Loss: 0.259237 Acc: 0.9062\n",
      "Train Epoch: 8 [54400/60000] Loss: 0.069749 Acc: 0.9688\n",
      "Train Epoch: 8 [57600/60000] Loss: 0.155914 Acc: 0.9375\n",
      "Elapsed 132.67s, 14.74 s/epoch, 0.01 s/batch, ets 162.15s\n",
      "\n",
      "Test set: Average loss: 0.2955, Accuracy: 8951/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [3200/60000] Loss: 0.380534 Acc: 0.8750\n",
      "Train Epoch: 9 [6400/60000] Loss: 0.074847 Acc: 0.9688\n",
      "Train Epoch: 9 [9600/60000] Loss: 0.187042 Acc: 0.9375\n",
      "Train Epoch: 9 [12800/60000] Loss: 0.086593 Acc: 0.9375\n",
      "Train Epoch: 9 [16000/60000] Loss: 0.173773 Acc: 0.9062\n",
      "Train Epoch: 9 [19200/60000] Loss: 0.196328 Acc: 0.9375\n",
      "Train Epoch: 9 [22400/60000] Loss: 0.259919 Acc: 0.9375\n",
      "Train Epoch: 9 [25600/60000] Loss: 0.223872 Acc: 0.8750\n",
      "Train Epoch: 9 [28800/60000] Loss: 0.219956 Acc: 0.9375\n",
      "Train Epoch: 9 [32000/60000] Loss: 0.263347 Acc: 0.9375\n",
      "Train Epoch: 9 [35200/60000] Loss: 0.351104 Acc: 0.9375\n",
      "Train Epoch: 9 [38400/60000] Loss: 0.233675 Acc: 0.9062\n",
      "Train Epoch: 9 [41600/60000] Loss: 0.255040 Acc: 0.9062\n",
      "Train Epoch: 9 [44800/60000] Loss: 0.377367 Acc: 0.8438\n",
      "Train Epoch: 9 [48000/60000] Loss: 0.255342 Acc: 0.9375\n",
      "Train Epoch: 9 [51200/60000] Loss: 0.448420 Acc: 0.8750\n",
      "Train Epoch: 9 [54400/60000] Loss: 0.169547 Acc: 0.9375\n",
      "Train Epoch: 9 [57600/60000] Loss: 0.207825 Acc: 0.9375\n",
      "Elapsed 148.13s, 14.81 s/epoch, 0.01 s/batch, ets 148.13s\n",
      "\n",
      "Test set: Average loss: 0.3127, Accuracy: 8902/10000 (89%)\n",
      "\n",
      "Train Epoch: 10 [3200/60000] Loss: 0.304962 Acc: 0.9062\n",
      "Train Epoch: 10 [6400/60000] Loss: 0.249673 Acc: 0.9062\n",
      "Train Epoch: 10 [9600/60000] Loss: 0.119761 Acc: 0.9375\n",
      "Train Epoch: 10 [12800/60000] Loss: 0.293862 Acc: 0.9062\n",
      "Train Epoch: 10 [16000/60000] Loss: 0.193120 Acc: 0.9062\n",
      "Train Epoch: 10 [19200/60000] Loss: 0.598623 Acc: 0.8750\n",
      "Train Epoch: 10 [22400/60000] Loss: 0.263947 Acc: 0.9375\n",
      "Train Epoch: 10 [25600/60000] Loss: 0.360630 Acc: 0.8750\n",
      "Train Epoch: 10 [28800/60000] Loss: 0.273597 Acc: 0.9062\n",
      "Train Epoch: 10 [32000/60000] Loss: 0.226482 Acc: 0.8438\n",
      "Train Epoch: 10 [35200/60000] Loss: 0.170688 Acc: 0.9062\n",
      "Train Epoch: 10 [38400/60000] Loss: 0.071228 Acc: 1.0000\n",
      "Train Epoch: 10 [41600/60000] Loss: 0.096182 Acc: 1.0000\n",
      "Train Epoch: 10 [44800/60000] Loss: 0.284343 Acc: 0.8750\n",
      "Train Epoch: 10 [48000/60000] Loss: 0.129040 Acc: 0.9688\n",
      "Train Epoch: 10 [51200/60000] Loss: 0.167462 Acc: 0.9062\n",
      "Train Epoch: 10 [54400/60000] Loss: 0.176764 Acc: 0.9375\n",
      "Train Epoch: 10 [57600/60000] Loss: 0.444229 Acc: 0.8750\n",
      "Elapsed 163.57s, 14.87 s/epoch, 0.01 s/batch, ets 133.83s\n",
      "\n",
      "Test set: Average loss: 0.2873, Accuracy: 8999/10000 (90%)\n",
      "\n",
      "Train Epoch: 11 [3200/60000] Loss: 0.094290 Acc: 1.0000\n",
      "Train Epoch: 11 [6400/60000] Loss: 0.344276 Acc: 0.8750\n",
      "Train Epoch: 11 [9600/60000] Loss: 0.125277 Acc: 0.9375\n",
      "Train Epoch: 11 [12800/60000] Loss: 0.350183 Acc: 0.8438\n",
      "Train Epoch: 11 [16000/60000] Loss: 0.166031 Acc: 0.9688\n",
      "Train Epoch: 11 [19200/60000] Loss: 0.210268 Acc: 0.9688\n",
      "Train Epoch: 11 [22400/60000] Loss: 0.132913 Acc: 0.9375\n",
      "Train Epoch: 11 [25600/60000] Loss: 0.058814 Acc: 1.0000\n",
      "Train Epoch: 11 [28800/60000] Loss: 0.512413 Acc: 0.8438\n",
      "Train Epoch: 11 [32000/60000] Loss: 0.089846 Acc: 0.9688\n",
      "Train Epoch: 11 [35200/60000] Loss: 0.166531 Acc: 0.8750\n",
      "Train Epoch: 11 [38400/60000] Loss: 0.189380 Acc: 0.9375\n",
      "Train Epoch: 11 [41600/60000] Loss: 0.134124 Acc: 0.9375\n",
      "Train Epoch: 11 [44800/60000] Loss: 0.192394 Acc: 0.8750\n",
      "Train Epoch: 11 [48000/60000] Loss: 0.198961 Acc: 0.9062\n",
      "Train Epoch: 11 [51200/60000] Loss: 0.098669 Acc: 0.9688\n",
      "Train Epoch: 11 [54400/60000] Loss: 0.071071 Acc: 1.0000\n",
      "Train Epoch: 11 [57600/60000] Loss: 0.329826 Acc: 0.8438\n",
      "Elapsed 179.26s, 14.94 s/epoch, 0.01 s/batch, ets 119.51s\n",
      "\n",
      "Test set: Average loss: 0.3000, Accuracy: 8910/10000 (89%)\n",
      "\n",
      "Train Epoch: 12 [3200/60000] Loss: 0.273556 Acc: 0.9375\n",
      "Train Epoch: 12 [6400/60000] Loss: 0.248865 Acc: 0.8750\n",
      "Train Epoch: 12 [9600/60000] Loss: 0.195391 Acc: 0.9062\n",
      "Train Epoch: 12 [12800/60000] Loss: 0.136780 Acc: 0.9688\n",
      "Train Epoch: 12 [16000/60000] Loss: 0.136513 Acc: 0.9375\n",
      "Train Epoch: 12 [19200/60000] Loss: 0.367558 Acc: 0.8750\n",
      "Train Epoch: 12 [22400/60000] Loss: 0.244632 Acc: 0.9062\n",
      "Train Epoch: 12 [25600/60000] Loss: 0.195190 Acc: 0.9375\n",
      "Train Epoch: 12 [28800/60000] Loss: 0.431656 Acc: 0.8125\n",
      "Train Epoch: 12 [32000/60000] Loss: 0.195235 Acc: 0.9062\n",
      "Train Epoch: 12 [35200/60000] Loss: 0.073230 Acc: 0.9688\n",
      "Train Epoch: 12 [38400/60000] Loss: 0.292419 Acc: 0.9062\n",
      "Train Epoch: 12 [41600/60000] Loss: 0.101681 Acc: 1.0000\n",
      "Train Epoch: 12 [44800/60000] Loss: 0.110962 Acc: 0.9375\n",
      "Train Epoch: 12 [48000/60000] Loss: 0.048870 Acc: 1.0000\n",
      "Train Epoch: 12 [51200/60000] Loss: 0.214941 Acc: 0.9062\n",
      "Train Epoch: 12 [54400/60000] Loss: 0.403566 Acc: 0.8125\n",
      "Train Epoch: 12 [57600/60000] Loss: 0.459307 Acc: 0.8750\n",
      "Elapsed 194.68s, 14.98 s/epoch, 0.01 s/batch, ets 104.83s\n",
      "\n",
      "Test set: Average loss: 0.2775, Accuracy: 9010/10000 (90%)\n",
      "\n",
      "Train Epoch: 13 [3200/60000] Loss: 0.201418 Acc: 0.9062\n",
      "Train Epoch: 13 [6400/60000] Loss: 0.301477 Acc: 0.9062\n",
      "Train Epoch: 13 [9600/60000] Loss: 0.133997 Acc: 0.9688\n",
      "Train Epoch: 13 [12800/60000] Loss: 0.353902 Acc: 0.9062\n",
      "Train Epoch: 13 [16000/60000] Loss: 0.095246 Acc: 1.0000\n",
      "Train Epoch: 13 [19200/60000] Loss: 0.249487 Acc: 0.8750\n",
      "Train Epoch: 13 [22400/60000] Loss: 0.156525 Acc: 0.9062\n",
      "Train Epoch: 13 [25600/60000] Loss: 0.280616 Acc: 0.8750\n",
      "Train Epoch: 13 [28800/60000] Loss: 0.129525 Acc: 0.9375\n",
      "Train Epoch: 13 [32000/60000] Loss: 0.163488 Acc: 0.9375\n",
      "Train Epoch: 13 [35200/60000] Loss: 0.474844 Acc: 0.8125\n",
      "Train Epoch: 13 [38400/60000] Loss: 0.125891 Acc: 0.9688\n",
      "Train Epoch: 13 [41600/60000] Loss: 0.364524 Acc: 0.8750\n",
      "Train Epoch: 13 [44800/60000] Loss: 0.029614 Acc: 1.0000\n",
      "Train Epoch: 13 [48000/60000] Loss: 0.177287 Acc: 0.9062\n",
      "Train Epoch: 13 [51200/60000] Loss: 0.331255 Acc: 0.9062\n",
      "Train Epoch: 13 [54400/60000] Loss: 0.376872 Acc: 0.8750\n",
      "Train Epoch: 13 [57600/60000] Loss: 0.199448 Acc: 0.9062\n",
      "Elapsed 210.17s, 15.01 s/epoch, 0.01 s/batch, ets 90.07s\n",
      "\n",
      "Test set: Average loss: 0.2893, Accuracy: 8990/10000 (90%)\n",
      "\n",
      "Train Epoch: 14 [3200/60000] Loss: 0.178479 Acc: 0.9375\n",
      "Train Epoch: 14 [6400/60000] Loss: 0.435963 Acc: 0.8750\n",
      "Train Epoch: 14 [9600/60000] Loss: 0.430858 Acc: 0.9062\n",
      "Train Epoch: 14 [12800/60000] Loss: 0.243949 Acc: 0.9375\n",
      "Train Epoch: 14 [16000/60000] Loss: 0.294112 Acc: 0.9062\n",
      "Train Epoch: 14 [19200/60000] Loss: 0.131918 Acc: 0.9688\n",
      "Train Epoch: 14 [22400/60000] Loss: 0.476337 Acc: 0.8438\n",
      "Train Epoch: 14 [25600/60000] Loss: 0.104445 Acc: 0.9375\n",
      "Train Epoch: 14 [28800/60000] Loss: 0.563830 Acc: 0.8438\n",
      "Train Epoch: 14 [32000/60000] Loss: 0.084115 Acc: 0.9688\n",
      "Train Epoch: 14 [35200/60000] Loss: 0.323049 Acc: 0.8750\n",
      "Train Epoch: 14 [38400/60000] Loss: 0.236556 Acc: 0.9375\n",
      "Train Epoch: 14 [41600/60000] Loss: 0.129515 Acc: 0.9688\n",
      "Train Epoch: 14 [44800/60000] Loss: 0.382478 Acc: 0.8750\n",
      "Train Epoch: 14 [48000/60000] Loss: 0.097441 Acc: 1.0000\n",
      "Train Epoch: 14 [51200/60000] Loss: 0.156857 Acc: 0.9688\n",
      "Train Epoch: 14 [54400/60000] Loss: 0.201794 Acc: 0.9375\n",
      "Train Epoch: 14 [57600/60000] Loss: 0.214284 Acc: 0.9375\n",
      "Elapsed 225.52s, 15.03 s/epoch, 0.01 s/batch, ets 75.17s\n",
      "\n",
      "Test set: Average loss: 0.2930, Accuracy: 8970/10000 (90%)\n",
      "\n",
      "Train Epoch: 15 [3200/60000] Loss: 0.408621 Acc: 0.8750\n",
      "Train Epoch: 15 [6400/60000] Loss: 0.176588 Acc: 0.9062\n",
      "Train Epoch: 15 [9600/60000] Loss: 0.151157 Acc: 0.9375\n",
      "Train Epoch: 15 [12800/60000] Loss: 0.221281 Acc: 0.8750\n",
      "Train Epoch: 15 [16000/60000] Loss: 0.140220 Acc: 0.9688\n",
      "Train Epoch: 15 [19200/60000] Loss: 0.159451 Acc: 0.9062\n",
      "Train Epoch: 15 [22400/60000] Loss: 0.292436 Acc: 0.9375\n",
      "Train Epoch: 15 [25600/60000] Loss: 0.178829 Acc: 0.8750\n",
      "Train Epoch: 15 [28800/60000] Loss: 0.321239 Acc: 0.9062\n",
      "Train Epoch: 15 [32000/60000] Loss: 0.258875 Acc: 0.8750\n",
      "Train Epoch: 15 [35200/60000] Loss: 0.203794 Acc: 0.9062\n",
      "Train Epoch: 15 [38400/60000] Loss: 0.142400 Acc: 1.0000\n",
      "Train Epoch: 15 [41600/60000] Loss: 0.364915 Acc: 0.9062\n",
      "Train Epoch: 15 [44800/60000] Loss: 0.423384 Acc: 0.8750\n",
      "Train Epoch: 15 [48000/60000] Loss: 0.284308 Acc: 0.9375\n",
      "Train Epoch: 15 [51200/60000] Loss: 0.227530 Acc: 0.8750\n",
      "Train Epoch: 15 [54400/60000] Loss: 0.133982 Acc: 0.9375\n",
      "Train Epoch: 15 [57600/60000] Loss: 0.153165 Acc: 0.9375\n",
      "Elapsed 240.74s, 15.05 s/epoch, 0.01 s/batch, ets 60.19s\n",
      "\n",
      "Test set: Average loss: 0.2838, Accuracy: 8987/10000 (90%)\n",
      "\n",
      "Train Epoch: 16 [3200/60000] Loss: 0.123219 Acc: 0.9688\n",
      "Train Epoch: 16 [6400/60000] Loss: 0.133044 Acc: 0.9375\n",
      "Train Epoch: 16 [9600/60000] Loss: 0.275735 Acc: 0.9688\n",
      "Train Epoch: 16 [12800/60000] Loss: 0.165286 Acc: 0.9062\n",
      "Train Epoch: 16 [16000/60000] Loss: 0.230184 Acc: 0.9375\n",
      "Train Epoch: 16 [19200/60000] Loss: 0.247982 Acc: 0.9062\n",
      "Train Epoch: 16 [22400/60000] Loss: 0.107618 Acc: 0.9375\n",
      "Train Epoch: 16 [25600/60000] Loss: 0.419980 Acc: 0.8125\n",
      "Train Epoch: 16 [28800/60000] Loss: 0.494096 Acc: 0.8438\n",
      "Train Epoch: 16 [32000/60000] Loss: 0.221247 Acc: 0.9062\n",
      "Train Epoch: 16 [35200/60000] Loss: 0.122547 Acc: 0.9688\n",
      "Train Epoch: 16 [38400/60000] Loss: 0.382829 Acc: 0.8438\n",
      "Train Epoch: 16 [41600/60000] Loss: 0.195896 Acc: 0.9375\n",
      "Train Epoch: 16 [44800/60000] Loss: 0.243531 Acc: 0.8750\n",
      "Train Epoch: 16 [48000/60000] Loss: 0.202979 Acc: 0.9375\n",
      "Train Epoch: 16 [51200/60000] Loss: 0.055786 Acc: 0.9688\n",
      "Train Epoch: 16 [54400/60000] Loss: 0.163047 Acc: 0.9688\n",
      "Train Epoch: 16 [57600/60000] Loss: 0.126053 Acc: 0.9062\n",
      "Elapsed 256.01s, 15.06 s/epoch, 0.01 s/batch, ets 45.18s\n",
      "\n",
      "Test set: Average loss: 0.2939, Accuracy: 8974/10000 (90%)\n",
      "\n",
      "Train Epoch: 17 [3200/60000] Loss: 0.245959 Acc: 0.8750\n",
      "Train Epoch: 17 [6400/60000] Loss: 0.085541 Acc: 1.0000\n",
      "Train Epoch: 17 [9600/60000] Loss: 0.270928 Acc: 0.8750\n",
      "Train Epoch: 17 [12800/60000] Loss: 0.209668 Acc: 0.9688\n",
      "Train Epoch: 17 [16000/60000] Loss: 0.089439 Acc: 0.9688\n",
      "Train Epoch: 17 [19200/60000] Loss: 0.085606 Acc: 0.9688\n",
      "Train Epoch: 17 [22400/60000] Loss: 0.196456 Acc: 0.9375\n",
      "Train Epoch: 17 [25600/60000] Loss: 0.188249 Acc: 0.9375\n",
      "Train Epoch: 17 [28800/60000] Loss: 0.238334 Acc: 0.8750\n",
      "Train Epoch: 17 [32000/60000] Loss: 0.084781 Acc: 1.0000\n",
      "Train Epoch: 17 [35200/60000] Loss: 0.152349 Acc: 0.9062\n",
      "Train Epoch: 17 [38400/60000] Loss: 0.110276 Acc: 0.9688\n",
      "Train Epoch: 17 [41600/60000] Loss: 0.081366 Acc: 0.9688\n",
      "Train Epoch: 17 [44800/60000] Loss: 0.309383 Acc: 0.9062\n",
      "Train Epoch: 17 [48000/60000] Loss: 0.236365 Acc: 0.8750\n",
      "Train Epoch: 17 [51200/60000] Loss: 0.232848 Acc: 0.8750\n",
      "Train Epoch: 17 [54400/60000] Loss: 0.183647 Acc: 0.9062\n",
      "Train Epoch: 17 [57600/60000] Loss: 0.322492 Acc: 0.9375\n",
      "Elapsed 271.36s, 15.08 s/epoch, 0.01 s/batch, ets 30.15s\n",
      "\n",
      "Test set: Average loss: 0.2856, Accuracy: 8999/10000 (90%)\n",
      "\n",
      "Train Epoch: 18 [3200/60000] Loss: 0.150890 Acc: 0.9688\n",
      "Train Epoch: 18 [6400/60000] Loss: 0.083958 Acc: 0.9688\n",
      "Train Epoch: 18 [9600/60000] Loss: 0.141738 Acc: 0.9375\n",
      "Train Epoch: 18 [12800/60000] Loss: 0.073600 Acc: 1.0000\n",
      "Train Epoch: 18 [16000/60000] Loss: 0.098649 Acc: 0.9688\n",
      "Train Epoch: 18 [19200/60000] Loss: 0.157235 Acc: 0.9375\n",
      "Train Epoch: 18 [22400/60000] Loss: 0.124532 Acc: 0.9375\n",
      "Train Epoch: 18 [25600/60000] Loss: 0.339921 Acc: 0.8750\n",
      "Train Epoch: 18 [28800/60000] Loss: 0.068545 Acc: 1.0000\n",
      "Train Epoch: 18 [32000/60000] Loss: 0.273673 Acc: 0.9375\n",
      "Train Epoch: 18 [35200/60000] Loss: 0.210584 Acc: 0.9375\n",
      "Train Epoch: 18 [38400/60000] Loss: 0.142905 Acc: 0.9688\n",
      "Train Epoch: 18 [41600/60000] Loss: 0.188558 Acc: 0.9062\n",
      "Train Epoch: 18 [44800/60000] Loss: 0.210200 Acc: 0.8438\n",
      "Train Epoch: 18 [48000/60000] Loss: 0.085032 Acc: 0.9688\n",
      "Train Epoch: 18 [51200/60000] Loss: 0.172091 Acc: 0.9688\n",
      "Train Epoch: 18 [54400/60000] Loss: 0.277003 Acc: 0.9062\n",
      "Train Epoch: 18 [57600/60000] Loss: 0.287207 Acc: 0.8750\n",
      "Elapsed 286.49s, 15.08 s/epoch, 0.01 s/batch, ets 15.08s\n",
      "\n",
      "Test set: Average loss: 0.2763, Accuracy: 9034/10000 (90%)\n",
      "\n",
      "Train Epoch: 19 [3200/60000] Loss: 0.269106 Acc: 0.9375\n",
      "Train Epoch: 19 [6400/60000] Loss: 0.085196 Acc: 0.9688\n",
      "Train Epoch: 19 [9600/60000] Loss: 0.150247 Acc: 0.9688\n",
      "Train Epoch: 19 [12800/60000] Loss: 0.074448 Acc: 1.0000\n",
      "Train Epoch: 19 [16000/60000] Loss: 0.201594 Acc: 0.9375\n",
      "Train Epoch: 19 [19200/60000] Loss: 0.175719 Acc: 0.9375\n",
      "Train Epoch: 19 [22400/60000] Loss: 0.086572 Acc: 0.9688\n",
      "Train Epoch: 19 [25600/60000] Loss: 0.178326 Acc: 0.9688\n",
      "Train Epoch: 19 [28800/60000] Loss: 0.131461 Acc: 0.9375\n",
      "Train Epoch: 19 [32000/60000] Loss: 0.176204 Acc: 0.9062\n",
      "Train Epoch: 19 [35200/60000] Loss: 0.173366 Acc: 0.9062\n",
      "Train Epoch: 19 [38400/60000] Loss: 0.128199 Acc: 0.9688\n",
      "Train Epoch: 19 [41600/60000] Loss: 0.321834 Acc: 0.8750\n",
      "Train Epoch: 19 [44800/60000] Loss: 0.435877 Acc: 0.8750\n",
      "Train Epoch: 19 [48000/60000] Loss: 0.100772 Acc: 0.9688\n",
      "Train Epoch: 19 [51200/60000] Loss: 0.093098 Acc: 0.9688\n",
      "Train Epoch: 19 [54400/60000] Loss: 0.276643 Acc: 0.8438\n",
      "Train Epoch: 19 [57600/60000] Loss: 0.170575 Acc: 0.9688\n",
      "Elapsed 301.88s, 15.09 s/epoch, 0.01 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.2953, Accuracy: 8984/10000 (90%)\n",
      "\n",
      "Total time: 306.59, Best Loss: 0.276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [35200/60000] Loss: 0.242817 Acc: 0.9062\n",
      "Train Epoch: 14 [38400/60000] Loss: 0.300826 Acc: 0.8438\n",
      "Train Epoch: 14 [41600/60000] Loss: 0.155230 Acc: 0.9375\n",
      "Train Epoch: 14 [44800/60000] Loss: 0.154523 Acc: 0.9375\n",
      "Train Epoch: 14 [48000/60000] Loss: 0.119851 Acc: 0.9688\n",
      "Train Epoch: 14 [51200/60000] Loss: 0.466593 Acc: 0.8438\n",
      "Train Epoch: 14 [54400/60000] Loss: 0.208276 Acc: 0.9062\n",
      "Train Epoch: 14 [57600/60000] Loss: 0.106473 Acc: 1.0000\n",
      "Elapsed 92.12s, 6.14 s/epoch, 0.00 s/batch, ets 30.71s\n",
      "\n",
      "Test set: Average loss: 0.3120, Accuracy: 8851/10000 (89%)\n",
      "\n",
      "Train Epoch: 15 [3200/60000] Loss: 0.278953 Acc: 0.8750\n",
      "Train Epoch: 15 [6400/60000] Loss: 0.156669 Acc: 0.9375\n",
      "Train Epoch: 15 [9600/60000] Loss: 0.127001 Acc: 0.9375\n",
      "Train Epoch: 15 [12800/60000] Loss: 0.136738 Acc: 1.0000\n",
      "Train Epoch: 15 [16000/60000] Loss: 0.237523 Acc: 0.8750\n",
      "Train Epoch: 15 [19200/60000] Loss: 0.271087 Acc: 0.8438\n",
      "Train Epoch: 15 [22400/60000] Loss: 0.165440 Acc: 0.9062\n",
      "Train Epoch: 15 [25600/60000] Loss: 0.249023 Acc: 0.8438\n",
      "Train Epoch: 15 [28800/60000] Loss: 0.408027 Acc: 0.7188\n",
      "Train Epoch: 15 [32000/60000] Loss: 0.186742 Acc: 0.9062\n",
      "Train Epoch: 15 [35200/60000] Loss: 0.382967 Acc: 0.8750\n",
      "Train Epoch: 15 [38400/60000] Loss: 0.306791 Acc: 0.9062\n",
      "Train Epoch: 15 [41600/60000] Loss: 0.410660 Acc: 0.9062\n",
      "Train Epoch: 15 [44800/60000] Loss: 0.140944 Acc: 0.9688\n",
      "Train Epoch: 15 [48000/60000] Loss: 0.226126 Acc: 0.9375\n",
      "Train Epoch: 15 [51200/60000] Loss: 0.274341 Acc: 0.9688\n",
      "Train Epoch: 15 [54400/60000] Loss: 0.187435 Acc: 0.9062\n",
      "Train Epoch: 15 [57600/60000] Loss: 0.293416 Acc: 0.9062\n",
      "Elapsed 98.26s, 6.14 s/epoch, 0.00 s/batch, ets 24.57s\n",
      "\n",
      "Test set: Average loss: 0.2930, Accuracy: 8937/10000 (89%)\n",
      "\n",
      "Train Epoch: 16 [3200/60000] Loss: 0.064175 Acc: 1.0000\n",
      "Train Epoch: 16 [6400/60000] Loss: 0.086041 Acc: 0.9375\n",
      "Train Epoch: 16 [9600/60000] Loss: 0.121120 Acc: 0.9688\n",
      "Train Epoch: 16 [12800/60000] Loss: 0.302166 Acc: 0.9062\n",
      "Train Epoch: 16 [16000/60000] Loss: 0.163053 Acc: 0.9688\n",
      "Train Epoch: 16 [19200/60000] Loss: 0.234567 Acc: 0.8750\n",
      "Train Epoch: 16 [22400/60000] Loss: 0.354824 Acc: 0.8438\n",
      "Train Epoch: 16 [25600/60000] Loss: 0.194620 Acc: 0.9062\n",
      "Train Epoch: 16 [28800/60000] Loss: 0.110342 Acc: 0.9688\n",
      "Train Epoch: 16 [32000/60000] Loss: 0.225842 Acc: 0.9688\n",
      "Train Epoch: 16 [35200/60000] Loss: 0.162732 Acc: 0.9375\n",
      "Train Epoch: 16 [38400/60000] Loss: 0.113323 Acc: 1.0000\n",
      "Train Epoch: 16 [41600/60000] Loss: 0.145671 Acc: 0.9375\n",
      "Train Epoch: 16 [44800/60000] Loss: 0.268134 Acc: 0.8750\n",
      "Train Epoch: 16 [48000/60000] Loss: 0.151414 Acc: 0.9062\n",
      "Train Epoch: 16 [51200/60000] Loss: 0.126569 Acc: 0.9688\n",
      "Train Epoch: 16 [54400/60000] Loss: 0.141279 Acc: 0.9688\n",
      "Train Epoch: 16 [57600/60000] Loss: 0.231005 Acc: 0.8750\n",
      "Elapsed 104.44s, 6.14 s/epoch, 0.00 s/batch, ets 18.43s\n",
      "\n",
      "Test set: Average loss: 0.3006, Accuracy: 8895/10000 (89%)\n",
      "\n",
      "Train Epoch: 17 [3200/60000] Loss: 0.449140 Acc: 0.8750\n",
      "Train Epoch: 17 [6400/60000] Loss: 0.229727 Acc: 0.9375\n",
      "Train Epoch: 17 [9600/60000] Loss: 0.181282 Acc: 0.9688\n",
      "Train Epoch: 17 [12800/60000] Loss: 0.386352 Acc: 0.8750\n",
      "Train Epoch: 17 [16000/60000] Loss: 0.153220 Acc: 0.9688\n",
      "Train Epoch: 17 [19200/60000] Loss: 0.213026 Acc: 0.9062\n",
      "Train Epoch: 17 [22400/60000] Loss: 0.546978 Acc: 0.8438\n",
      "Train Epoch: 17 [25600/60000] Loss: 0.203140 Acc: 0.9688\n",
      "Train Epoch: 17 [28800/60000] Loss: 0.293355 Acc: 0.9062\n",
      "Train Epoch: 17 [32000/60000] Loss: 0.127861 Acc: 0.9375\n",
      "Train Epoch: 17 [35200/60000] Loss: 0.131085 Acc: 0.9375\n",
      "Train Epoch: 17 [38400/60000] Loss: 0.140873 Acc: 0.9688\n",
      "Train Epoch: 17 [41600/60000] Loss: 0.145352 Acc: 0.9375\n",
      "Train Epoch: 17 [44800/60000] Loss: 0.140877 Acc: 0.9688\n",
      "Train Epoch: 17 [48000/60000] Loss: 0.226598 Acc: 0.8750\n",
      "Train Epoch: 17 [51200/60000] Loss: 0.199978 Acc: 0.9375\n",
      "Train Epoch: 17 [54400/60000] Loss: 0.258557 Acc: 0.9062\n",
      "Train Epoch: 17 [57600/60000] Loss: 0.125101 Acc: 0.9062\n",
      "Elapsed 110.42s, 6.13 s/epoch, 0.00 s/batch, ets 12.27s\n",
      "\n",
      "Test set: Average loss: 0.3032, Accuracy: 8913/10000 (89%)\n",
      "\n",
      "Train Epoch: 18 [3200/60000] Loss: 0.227705 Acc: 0.9375\n",
      "Train Epoch: 18 [6400/60000] Loss: 0.141556 Acc: 0.9375\n",
      "Train Epoch: 18 [9600/60000] Loss: 0.220209 Acc: 0.8750\n",
      "Train Epoch: 18 [12800/60000] Loss: 0.185627 Acc: 0.9688\n",
      "Train Epoch: 18 [16000/60000] Loss: 0.263256 Acc: 0.9062\n",
      "Train Epoch: 18 [19200/60000] Loss: 0.123247 Acc: 0.9688\n",
      "Train Epoch: 18 [22400/60000] Loss: 0.221987 Acc: 0.8750\n",
      "Train Epoch: 18 [25600/60000] Loss: 0.233655 Acc: 0.9062\n",
      "Train Epoch: 18 [28800/60000] Loss: 0.278151 Acc: 0.8750\n",
      "Train Epoch: 18 [32000/60000] Loss: 0.315081 Acc: 0.8750\n",
      "Train Epoch: 18 [35200/60000] Loss: 0.198923 Acc: 0.9688\n",
      "Train Epoch: 18 [38400/60000] Loss: 0.312364 Acc: 0.9375\n",
      "Train Epoch: 18 [41600/60000] Loss: 0.171805 Acc: 0.9688\n",
      "Train Epoch: 18 [44800/60000] Loss: 0.183044 Acc: 0.8750\n",
      "Train Epoch: 18 [48000/60000] Loss: 0.404249 Acc: 0.8125\n",
      "Train Epoch: 18 [51200/60000] Loss: 0.080402 Acc: 0.9688\n",
      "Train Epoch: 18 [54400/60000] Loss: 0.152811 Acc: 0.9688\n",
      "Train Epoch: 18 [57600/60000] Loss: 0.201690 Acc: 0.9062\n",
      "Elapsed 116.73s, 6.14 s/epoch, 0.00 s/batch, ets 6.14s\n",
      "\n",
      "Test set: Average loss: 0.2937, Accuracy: 8958/10000 (90%)\n",
      "\n",
      "Train Epoch: 19 [3200/60000] Loss: 0.320440 Acc: 0.8438\n",
      "Train Epoch: 19 [6400/60000] Loss: 0.172557 Acc: 0.9062\n",
      "Train Epoch: 19 [9600/60000] Loss: 0.586339 Acc: 0.9062\n",
      "Train Epoch: 19 [12800/60000] Loss: 0.106348 Acc: 0.9688\n",
      "Train Epoch: 19 [16000/60000] Loss: 0.074152 Acc: 0.9688\n",
      "Train Epoch: 19 [19200/60000] Loss: 0.308980 Acc: 0.8750\n",
      "Train Epoch: 19 [22400/60000] Loss: 0.065371 Acc: 1.0000\n",
      "Train Epoch: 19 [25600/60000] Loss: 0.141370 Acc: 0.9375\n",
      "Train Epoch: 19 [28800/60000] Loss: 0.271981 Acc: 0.8438\n",
      "Train Epoch: 19 [32000/60000] Loss: 0.255091 Acc: 0.9062\n",
      "Train Epoch: 19 [35200/60000] Loss: 0.232604 Acc: 0.9062\n",
      "Train Epoch: 19 [38400/60000] Loss: 0.101710 Acc: 1.0000\n",
      "Train Epoch: 19 [41600/60000] Loss: 0.165721 Acc: 0.9688\n",
      "Train Epoch: 19 [44800/60000] Loss: 0.254255 Acc: 0.8750\n",
      "Train Epoch: 19 [48000/60000] Loss: 0.273236 Acc: 0.9062\n",
      "Train Epoch: 19 [51200/60000] Loss: 0.371949 Acc: 0.8438\n",
      "Train Epoch: 19 [54400/60000] Loss: 0.375186 Acc: 0.9062\n",
      "Train Epoch: 19 [57600/60000] Loss: 0.280283 Acc: 0.9375\n",
      "Elapsed 122.97s, 6.15 s/epoch, 0.00 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.3187, Accuracy: 8851/10000 (89%)\n",
      "\n",
      "Total time: 123.70, Best Loss: 0.293\n",
      "Train Epoch: 0 [3200/60000] Loss: 1.913762 Acc: 0.4688\n",
      "Train Epoch: 0 [6400/60000] Loss: 1.366199 Acc: 0.5625\n",
      "Train Epoch: 0 [9600/60000] Loss: 0.993837 Acc: 0.7188\n",
      "Train Epoch: 0 [12800/60000] Loss: 1.000988 Acc: 0.5625\n",
      "Train Epoch: 0 [16000/60000] Loss: 0.585033 Acc: 0.8438\n",
      "Train Epoch: 0 [19200/60000] Loss: 0.536666 Acc: 0.8438\n",
      "Train Epoch: 0 [22400/60000] Loss: 0.960056 Acc: 0.6875\n",
      "Train Epoch: 0 [25600/60000] Loss: 0.768767 Acc: 0.6875\n",
      "Train Epoch: 0 [28800/60000] Loss: 0.468666 Acc: 0.8125\n",
      "Train Epoch: 0 [32000/60000] Loss: 0.676432 Acc: 0.6875\n",
      "Train Epoch: 0 [35200/60000] Loss: 0.501992 Acc: 0.8438\n",
      "Train Epoch: 0 [38400/60000] Loss: 0.584635 Acc: 0.6875\n",
      "Train Epoch: 0 [41600/60000] Loss: 0.323162 Acc: 0.9062\n",
      "Train Epoch: 0 [44800/60000] Loss: 0.667134 Acc: 0.7812\n",
      "Train Epoch: 0 [48000/60000] Loss: 0.679572 Acc: 0.7812\n",
      "Train Epoch: 0 [51200/60000] Loss: 0.477021 Acc: 0.8125\n",
      "Train Epoch: 0 [54400/60000] Loss: 0.454536 Acc: 0.8438\n",
      "Train Epoch: 0 [57600/60000] Loss: 0.552928 Acc: 0.7500\n",
      "Elapsed 5.85s, 5.85 s/epoch, 0.00 s/batch, ets 111.19s\n",
      "\n",
      "Test set: Average loss: 0.4989, Accuracy: 8143/10000 (81%)\n",
      "\n",
      "Train Epoch: 1 [3200/60000] Loss: 0.444912 Acc: 0.7812\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.799942 Acc: 0.7812\n",
      "Train Epoch: 1 [9600/60000] Loss: 0.457949 Acc: 0.7812\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.350030 Acc: 0.8438\n",
      "Train Epoch: 1 [16000/60000] Loss: 0.211350 Acc: 0.9375\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.214548 Acc: 0.9375\n",
      "Train Epoch: 1 [22400/60000] Loss: 0.442878 Acc: 0.8438\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.380842 Acc: 0.9062\n",
      "Train Epoch: 1 [28800/60000] Loss: 0.415249 Acc: 0.8125\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.685785 Acc: 0.7812\n",
      "Train Epoch: 1 [35200/60000] Loss: 0.990957 Acc: 0.6562\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.375771 Acc: 0.8750\n",
      "Train Epoch: 1 [41600/60000] Loss: 0.229077 Acc: 0.9062\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.420569 Acc: 0.7812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [48000/60000] Loss: 0.315432 Acc: 0.9062\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.236998 Acc: 0.8750\n",
      "Train Epoch: 1 [54400/60000] Loss: 0.230768 Acc: 0.9062\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.290227 Acc: 0.8750\n",
      "Elapsed 12.70s, 6.35 s/epoch, 0.00 s/batch, ets 114.29s\n",
      "\n",
      "Test set: Average loss: 0.3979, Accuracy: 8558/10000 (86%)\n",
      "\n",
      "Train Epoch: 2 [3200/60000] Loss: 0.436743 Acc: 0.7812\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.306836 Acc: 0.9062\n",
      "Train Epoch: 2 [9600/60000] Loss: 0.265878 Acc: 0.8750\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.375737 Acc: 0.8438\n",
      "Train Epoch: 2 [16000/60000] Loss: 0.562706 Acc: 0.7812\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.219371 Acc: 0.9375\n",
      "Train Epoch: 2 [22400/60000] Loss: 0.332222 Acc: 0.9375\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.237044 Acc: 0.9062\n",
      "Train Epoch: 2 [28800/60000] Loss: 0.389849 Acc: 0.8750\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.365898 Acc: 0.7812\n",
      "Train Epoch: 2 [35200/60000] Loss: 0.212068 Acc: 0.9375\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.467402 Acc: 0.9062\n",
      "Train Epoch: 2 [41600/60000] Loss: 0.337512 Acc: 0.9375\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.523303 Acc: 0.7812\n",
      "Train Epoch: 2 [48000/60000] Loss: 0.387071 Acc: 0.7812\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.366569 Acc: 0.8125\n",
      "Train Epoch: 2 [54400/60000] Loss: 0.425895 Acc: 0.8438\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.248803 Acc: 0.9375\n",
      "Elapsed 19.46s, 6.49 s/epoch, 0.00 s/batch, ets 110.25s\n",
      "\n",
      "Test set: Average loss: 0.3635, Accuracy: 8711/10000 (87%)\n",
      "\n",
      "Train Epoch: 3 [3200/60000] Loss: 0.318891 Acc: 0.8125\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.280946 Acc: 0.8750\n",
      "Train Epoch: 3 [9600/60000] Loss: 0.398927 Acc: 0.8750\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.322511 Acc: 0.9375\n",
      "Train Epoch: 3 [16000/60000] Loss: 0.163940 Acc: 0.9375\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.120980 Acc: 1.0000\n",
      "Train Epoch: 3 [22400/60000] Loss: 0.375481 Acc: 0.8438\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.302495 Acc: 0.9062\n",
      "Train Epoch: 3 [28800/60000] Loss: 0.472078 Acc: 0.8438\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.294011 Acc: 0.8750\n",
      "Train Epoch: 3 [35200/60000] Loss: 0.442789 Acc: 0.8125\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.634242 Acc: 0.8125\n",
      "Train Epoch: 3 [41600/60000] Loss: 0.271653 Acc: 0.9062\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.260425 Acc: 0.9062\n",
      "Train Epoch: 3 [48000/60000] Loss: 0.388122 Acc: 0.8750\n",
      "Train Epoch: 3 [51200/60000] Loss: 0.169958 Acc: 0.9375\n",
      "Train Epoch: 3 [54400/60000] Loss: 0.476631 Acc: 0.7500\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.363796 Acc: 0.8750\n",
      "Elapsed 26.32s, 6.58 s/epoch, 0.00 s/batch, ets 105.29s\n",
      "\n",
      "Test set: Average loss: 0.3367, Accuracy: 8812/10000 (88%)\n",
      "\n",
      "Train Epoch: 4 [3200/60000] Loss: 0.238414 Acc: 0.9375\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.150382 Acc: 0.9375\n",
      "Train Epoch: 4 [9600/60000] Loss: 0.515032 Acc: 0.8438\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.285570 Acc: 0.8750\n",
      "Train Epoch: 4 [16000/60000] Loss: 0.188504 Acc: 0.9375\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.155213 Acc: 0.9688\n",
      "Train Epoch: 4 [22400/60000] Loss: 0.392789 Acc: 0.8438\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.249845 Acc: 0.9375\n",
      "Train Epoch: 4 [28800/60000] Loss: 0.281963 Acc: 0.8438\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.191575 Acc: 0.9375\n",
      "Train Epoch: 4 [35200/60000] Loss: 0.149668 Acc: 1.0000\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.308004 Acc: 0.8750\n",
      "Train Epoch: 4 [41600/60000] Loss: 0.214901 Acc: 0.8750\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.277303 Acc: 0.8438\n",
      "Train Epoch: 4 [48000/60000] Loss: 0.407011 Acc: 0.8125\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.387951 Acc: 0.8438\n",
      "Train Epoch: 4 [54400/60000] Loss: 0.338002 Acc: 0.8750\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.220580 Acc: 0.8750\n",
      "Elapsed 32.83s, 6.57 s/epoch, 0.00 s/batch, ets 98.48s\n",
      "\n",
      "Test set: Average loss: 0.3388, Accuracy: 8771/10000 (88%)\n",
      "\n",
      "Train Epoch: 5 [3200/60000] Loss: 0.181129 Acc: 0.9375\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.274682 Acc: 0.8750\n",
      "Train Epoch: 5 [9600/60000] Loss: 0.214960 Acc: 0.9375\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.371052 Acc: 0.9062\n",
      "Train Epoch: 5 [16000/60000] Loss: 0.164170 Acc: 0.9375\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.169027 Acc: 0.9688\n",
      "Train Epoch: 5 [22400/60000] Loss: 0.248558 Acc: 0.9062\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.331176 Acc: 0.8750\n",
      "Train Epoch: 5 [28800/60000] Loss: 0.206622 Acc: 0.9062\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.240593 Acc: 0.8750\n",
      "Train Epoch: 5 [35200/60000] Loss: 0.364583 Acc: 0.9062\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.231615 Acc: 0.9375\n",
      "Train Epoch: 5 [41600/60000] Loss: 0.317340 Acc: 0.8750\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.290205 Acc: 0.9062\n",
      "Train Epoch: 5 [48000/60000] Loss: 0.355866 Acc: 0.7812\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.339312 Acc: 0.8750\n",
      "Train Epoch: 5 [54400/60000] Loss: 0.168126 Acc: 0.9688\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.340069 Acc: 0.9062\n",
      "Elapsed 39.42s, 6.57 s/epoch, 0.00 s/batch, ets 91.98s\n",
      "\n",
      "Test set: Average loss: 0.3357, Accuracy: 8811/10000 (88%)\n",
      "\n",
      "Train Epoch: 6 [3200/60000] Loss: 0.102145 Acc: 0.9688\n",
      "Train Epoch: 6 [6400/60000] Loss: 0.093840 Acc: 0.9688\n",
      "Train Epoch: 6 [9600/60000] Loss: 0.246671 Acc: 0.8750\n",
      "Train Epoch: 6 [12800/60000] Loss: 0.225185 Acc: 0.9062\n",
      "Train Epoch: 6 [16000/60000] Loss: 0.282549 Acc: 0.8438\n",
      "Train Epoch: 6 [19200/60000] Loss: 0.114617 Acc: 0.9688\n",
      "Train Epoch: 6 [22400/60000] Loss: 0.423194 Acc: 0.8750\n",
      "Train Epoch: 6 [25600/60000] Loss: 0.233614 Acc: 0.9375\n",
      "Train Epoch: 6 [28800/60000] Loss: 0.111850 Acc: 0.9688\n",
      "Train Epoch: 6 [32000/60000] Loss: 0.344990 Acc: 0.8125\n",
      "Train Epoch: 6 [35200/60000] Loss: 0.199788 Acc: 0.9688\n",
      "Train Epoch: 6 [38400/60000] Loss: 0.313531 Acc: 0.9062\n",
      "Train Epoch: 6 [41600/60000] Loss: 0.286497 Acc: 0.9375\n",
      "Train Epoch: 6 [44800/60000] Loss: 0.300240 Acc: 0.8438\n",
      "Train Epoch: 6 [48000/60000] Loss: 0.110741 Acc: 1.0000\n",
      "Train Epoch: 6 [51200/60000] Loss: 0.279260 Acc: 0.8438\n",
      "Train Epoch: 6 [54400/60000] Loss: 0.311294 Acc: 0.9375\n",
      "Train Epoch: 6 [57600/60000] Loss: 0.323959 Acc: 0.8750\n",
      "Elapsed 46.04s, 6.58 s/epoch, 0.00 s/batch, ets 85.50s\n",
      "\n",
      "Test set: Average loss: 0.3149, Accuracy: 8891/10000 (89%)\n",
      "\n",
      "Train Epoch: 7 [3200/60000] Loss: 0.231192 Acc: 0.8750\n",
      "Train Epoch: 7 [6400/60000] Loss: 0.429808 Acc: 0.8438\n",
      "Train Epoch: 7 [9600/60000] Loss: 0.263131 Acc: 0.9062\n",
      "Train Epoch: 7 [12800/60000] Loss: 0.099467 Acc: 0.9688\n",
      "Train Epoch: 7 [16000/60000] Loss: 0.283472 Acc: 0.8438\n",
      "Train Epoch: 7 [19200/60000] Loss: 0.209067 Acc: 0.9375\n",
      "Train Epoch: 7 [22400/60000] Loss: 0.211048 Acc: 0.8750\n",
      "Train Epoch: 7 [25600/60000] Loss: 0.451800 Acc: 0.8125\n",
      "Train Epoch: 7 [28800/60000] Loss: 0.116837 Acc: 1.0000\n",
      "Train Epoch: 7 [32000/60000] Loss: 0.466892 Acc: 0.8438\n",
      "Train Epoch: 7 [35200/60000] Loss: 0.438771 Acc: 0.8438\n",
      "Train Epoch: 7 [38400/60000] Loss: 0.242079 Acc: 0.9375\n",
      "Train Epoch: 7 [41600/60000] Loss: 0.315426 Acc: 0.9062\n",
      "Train Epoch: 7 [44800/60000] Loss: 0.142164 Acc: 0.9375\n",
      "Train Epoch: 7 [48000/60000] Loss: 0.322213 Acc: 0.8438\n",
      "Train Epoch: 7 [51200/60000] Loss: 0.132368 Acc: 0.9062\n",
      "Train Epoch: 7 [54400/60000] Loss: 0.193045 Acc: 0.9062\n",
      "Train Epoch: 7 [57600/60000] Loss: 0.432870 Acc: 0.8438\n",
      "Elapsed 52.60s, 6.57 s/epoch, 0.00 s/batch, ets 78.90s\n",
      "\n",
      "Test set: Average loss: 0.3033, Accuracy: 8898/10000 (89%)\n",
      "\n",
      "Train Epoch: 8 [3200/60000] Loss: 0.384248 Acc: 0.9062\n",
      "Train Epoch: 8 [6400/60000] Loss: 0.087726 Acc: 0.9688\n",
      "Train Epoch: 8 [9600/60000] Loss: 0.265532 Acc: 0.8750\n",
      "Train Epoch: 8 [12800/60000] Loss: 0.088088 Acc: 0.9688\n",
      "Train Epoch: 8 [16000/60000] Loss: 0.254724 Acc: 0.9062\n",
      "Train Epoch: 8 [19200/60000] Loss: 0.211043 Acc: 0.9062\n",
      "Train Epoch: 8 [22400/60000] Loss: 0.205172 Acc: 0.9375\n",
      "Train Epoch: 8 [25600/60000] Loss: 0.347026 Acc: 0.9375\n",
      "Train Epoch: 8 [28800/60000] Loss: 0.400810 Acc: 0.8750\n",
      "Train Epoch: 8 [32000/60000] Loss: 0.204347 Acc: 0.9062\n",
      "Train Epoch: 8 [35200/60000] Loss: 0.075556 Acc: 1.0000\n",
      "Train Epoch: 8 [38400/60000] Loss: 0.131341 Acc: 0.9375\n",
      "Train Epoch: 8 [41600/60000] Loss: 0.060804 Acc: 0.9688\n",
      "Train Epoch: 8 [44800/60000] Loss: 0.235493 Acc: 0.8438\n",
      "Train Epoch: 8 [48000/60000] Loss: 0.265837 Acc: 0.8750\n",
      "Train Epoch: 8 [51200/60000] Loss: 0.141011 Acc: 0.9375\n",
      "Train Epoch: 8 [54400/60000] Loss: 0.260084 Acc: 0.9062\n",
      "Train Epoch: 8 [57600/60000] Loss: 0.249365 Acc: 0.9062\n",
      "Elapsed 59.16s, 6.57 s/epoch, 0.00 s/batch, ets 72.31s\n",
      "\n",
      "Test set: Average loss: 0.3414, Accuracy: 8766/10000 (88%)\n",
      "\n",
      "Train Epoch: 9 [3200/60000] Loss: 0.252476 Acc: 0.9062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [6400/60000] Loss: 0.425166 Acc: 0.8438\n",
      "Train Epoch: 9 [9600/60000] Loss: 0.122013 Acc: 0.9688\n",
      "Train Epoch: 9 [12800/60000] Loss: 0.418962 Acc: 0.9375\n",
      "Train Epoch: 9 [16000/60000] Loss: 0.122426 Acc: 0.9375\n",
      "Train Epoch: 9 [19200/60000] Loss: 0.151508 Acc: 0.9062\n",
      "Train Epoch: 9 [22400/60000] Loss: 0.311149 Acc: 0.8438\n",
      "Train Epoch: 9 [25600/60000] Loss: 0.170491 Acc: 0.9062\n",
      "Train Epoch: 9 [28800/60000] Loss: 0.512506 Acc: 0.8125\n",
      "Train Epoch: 9 [32000/60000] Loss: 0.182610 Acc: 0.9688\n",
      "Train Epoch: 9 [35200/60000] Loss: 0.144600 Acc: 1.0000\n",
      "Train Epoch: 9 [38400/60000] Loss: 0.105158 Acc: 1.0000\n",
      "Train Epoch: 9 [41600/60000] Loss: 0.112190 Acc: 1.0000\n",
      "Train Epoch: 9 [44800/60000] Loss: 0.143535 Acc: 0.9688\n",
      "Train Epoch: 9 [48000/60000] Loss: 0.059460 Acc: 1.0000\n",
      "Train Epoch: 9 [51200/60000] Loss: 0.376045 Acc: 0.8750\n",
      "Train Epoch: 9 [54400/60000] Loss: 0.534732 Acc: 0.8125\n",
      "Train Epoch: 9 [57600/60000] Loss: 0.239681 Acc: 0.9375\n",
      "Elapsed 66.02s, 6.60 s/epoch, 0.00 s/batch, ets 66.02s\n",
      "\n",
      "Test set: Average loss: 0.3000, Accuracy: 8919/10000 (89%)\n",
      "\n",
      "Train Epoch: 10 [3200/60000] Loss: 0.280259 Acc: 0.8750\n",
      "Train Epoch: 10 [6400/60000] Loss: 0.109035 Acc: 0.9688\n",
      "Train Epoch: 10 [9600/60000] Loss: 0.251394 Acc: 0.9062\n",
      "Train Epoch: 10 [12800/60000] Loss: 0.393977 Acc: 0.8750\n",
      "Train Epoch: 10 [16000/60000] Loss: 0.309339 Acc: 0.8438\n",
      "Train Epoch: 10 [19200/60000] Loss: 0.355785 Acc: 0.9062\n",
      "Train Epoch: 10 [22400/60000] Loss: 0.217772 Acc: 0.9062\n",
      "Train Epoch: 10 [25600/60000] Loss: 0.163385 Acc: 0.9688\n",
      "Train Epoch: 10 [28800/60000] Loss: 0.255248 Acc: 0.9375\n",
      "Train Epoch: 10 [32000/60000] Loss: 0.260465 Acc: 0.9375\n",
      "Train Epoch: 10 [35200/60000] Loss: 0.094442 Acc: 0.9688\n",
      "Train Epoch: 10 [38400/60000] Loss: 0.206552 Acc: 0.9375\n",
      "Train Epoch: 10 [41600/60000] Loss: 0.354853 Acc: 0.8438\n",
      "Train Epoch: 10 [44800/60000] Loss: 0.155540 Acc: 0.9375\n",
      "Train Epoch: 10 [48000/60000] Loss: 0.291811 Acc: 0.8750\n",
      "Train Epoch: 10 [51200/60000] Loss: 0.322007 Acc: 0.8438\n",
      "Train Epoch: 10 [54400/60000] Loss: 0.233570 Acc: 0.9062\n",
      "Train Epoch: 10 [57600/60000] Loss: 0.495338 Acc: 0.8125\n",
      "Elapsed 72.75s, 6.61 s/epoch, 0.00 s/batch, ets 59.52s\n",
      "\n",
      "Test set: Average loss: 0.2935, Accuracy: 8938/10000 (89%)\n",
      "\n",
      "Train Epoch: 11 [3200/60000] Loss: 0.323821 Acc: 0.9062\n",
      "Train Epoch: 11 [6400/60000] Loss: 0.193061 Acc: 0.9375\n",
      "Train Epoch: 11 [9600/60000] Loss: 0.137606 Acc: 0.9688\n",
      "Train Epoch: 11 [12800/60000] Loss: 0.189039 Acc: 0.9375\n",
      "Train Epoch: 11 [16000/60000] Loss: 0.143793 Acc: 0.8750\n",
      "Train Epoch: 11 [19200/60000] Loss: 0.248259 Acc: 0.9375\n",
      "Train Epoch: 11 [22400/60000] Loss: 0.151908 Acc: 0.9062\n",
      "Train Epoch: 11 [25600/60000] Loss: 0.260076 Acc: 0.9375\n",
      "Train Epoch: 11 [28800/60000] Loss: 0.102952 Acc: 0.9688\n",
      "Train Epoch: 11 [32000/60000] Loss: 0.321571 Acc: 0.9062\n",
      "Train Epoch: 11 [35200/60000] Loss: 0.065904 Acc: 1.0000\n",
      "Train Epoch: 11 [38400/60000] Loss: 0.310662 Acc: 0.8750\n",
      "Train Epoch: 11 [41600/60000] Loss: 0.143900 Acc: 0.9688\n",
      "Train Epoch: 11 [44800/60000] Loss: 0.222389 Acc: 0.9062\n",
      "Train Epoch: 11 [48000/60000] Loss: 0.122047 Acc: 0.9688\n",
      "Train Epoch: 11 [51200/60000] Loss: 0.264619 Acc: 0.9375\n",
      "Train Epoch: 11 [54400/60000] Loss: 0.446887 Acc: 0.8750\n",
      "Train Epoch: 11 [57600/60000] Loss: 0.138684 Acc: 0.9375\n",
      "Elapsed 79.76s, 6.65 s/epoch, 0.00 s/batch, ets 53.17s\n",
      "\n",
      "Test set: Average loss: 0.2894, Accuracy: 8960/10000 (90%)\n",
      "\n",
      "Train Epoch: 12 [3200/60000] Loss: 0.407344 Acc: 0.9375\n",
      "Train Epoch: 12 [6400/60000] Loss: 0.132422 Acc: 0.9688\n",
      "Train Epoch: 12 [9600/60000] Loss: 0.270598 Acc: 0.8438\n",
      "Train Epoch: 12 [12800/60000] Loss: 0.085074 Acc: 0.9688\n",
      "Train Epoch: 12 [16000/60000] Loss: 0.338674 Acc: 0.8438\n",
      "Train Epoch: 12 [19200/60000] Loss: 0.284592 Acc: 0.8438\n",
      "Train Epoch: 12 [22400/60000] Loss: 0.218124 Acc: 0.8750\n",
      "Train Epoch: 12 [25600/60000] Loss: 0.136713 Acc: 0.9375\n",
      "Train Epoch: 12 [28800/60000] Loss: 0.309454 Acc: 0.9062\n",
      "Train Epoch: 12 [32000/60000] Loss: 0.321859 Acc: 0.8750\n",
      "Train Epoch: 12 [35200/60000] Loss: 0.155531 Acc: 0.9375\n",
      "Train Epoch: 12 [38400/60000] Loss: 0.381749 Acc: 0.8438\n",
      "Train Epoch: 12 [41600/60000] Loss: 0.200498 Acc: 0.9375\n",
      "Train Epoch: 12 [44800/60000] Loss: 0.089929 Acc: 0.9688\n",
      "Train Epoch: 12 [48000/60000] Loss: 0.169475 Acc: 0.9375\n",
      "Train Epoch: 12 [51200/60000] Loss: 0.384965 Acc: 0.8750\n",
      "Train Epoch: 12 [54400/60000] Loss: 0.221142 Acc: 0.9375\n",
      "Train Epoch: 12 [57600/60000] Loss: 0.340476 Acc: 0.8438\n",
      "Elapsed 86.67s, 6.67 s/epoch, 0.00 s/batch, ets 46.67s\n",
      "\n",
      "Test set: Average loss: 0.2975, Accuracy: 8924/10000 (89%)\n",
      "\n",
      "Train Epoch: 13 [3200/60000] Loss: 0.293343 Acc: 0.8438\n",
      "Train Epoch: 13 [6400/60000] Loss: 0.152418 Acc: 0.9375\n",
      "Train Epoch: 13 [9600/60000] Loss: 0.436257 Acc: 0.8750\n",
      "Train Epoch: 13 [12800/60000] Loss: 0.239025 Acc: 0.9062\n",
      "Train Epoch: 13 [16000/60000] Loss: 0.238002 Acc: 0.9375\n",
      "Train Epoch: 13 [19200/60000] Loss: 0.278704 Acc: 0.8750\n",
      "Train Epoch: 13 [22400/60000] Loss: 0.169894 Acc: 0.9688\n",
      "Train Epoch: 13 [25600/60000] Loss: 0.153386 Acc: 0.9375\n",
      "Train Epoch: 13 [28800/60000] Loss: 0.109542 Acc: 0.9688\n",
      "Train Epoch: 13 [32000/60000] Loss: 0.088915 Acc: 0.9688\n",
      "Train Epoch: 13 [35200/60000] Loss: 0.189008 Acc: 0.9062\n",
      "Train Epoch: 13 [38400/60000] Loss: 0.293869 Acc: 0.9062\n",
      "Train Epoch: 13 [41600/60000] Loss: 0.194299 Acc: 0.9688\n",
      "Train Epoch: 13 [44800/60000] Loss: 0.305924 Acc: 0.8750\n",
      "Train Epoch: 13 [48000/60000] Loss: 0.117977 Acc: 0.9688\n",
      "Train Epoch: 13 [51200/60000] Loss: 0.035519 Acc: 1.0000\n",
      "Train Epoch: 13 [54400/60000] Loss: 0.247959 Acc: 0.9062\n",
      "Train Epoch: 13 [57600/60000] Loss: 0.301004 Acc: 0.9062\n",
      "Elapsed 93.65s, 6.69 s/epoch, 0.00 s/batch, ets 40.13s\n",
      "\n",
      "Test set: Average loss: 0.2949, Accuracy: 8946/10000 (89%)\n",
      "\n",
      "Train Epoch: 14 [3200/60000] Loss: 0.229767 Acc: 0.9062\n",
      "Train Epoch: 14 [6400/60000] Loss: 0.271069 Acc: 0.9062\n",
      "Train Epoch: 14 [9600/60000] Loss: 0.162814 Acc: 0.9062\n",
      "Train Epoch: 14 [12800/60000] Loss: 0.370729 Acc: 0.8750\n",
      "Train Epoch: 14 [16000/60000] Loss: 0.225270 Acc: 0.9062\n",
      "Train Epoch: 14 [19200/60000] Loss: 0.126029 Acc: 0.9688\n",
      "Train Epoch: 14 [22400/60000] Loss: 0.117928 Acc: 0.9688\n",
      "Train Epoch: 14 [25600/60000] Loss: 0.434438 Acc: 0.8438\n",
      "Train Epoch: 14 [28800/60000] Loss: 0.251593 Acc: 0.9375\n",
      "Train Epoch: 14 [32000/60000] Loss: 0.234331 Acc: 0.8750\n",
      "Train Epoch: 14 [35200/60000] Loss: 0.162455 Acc: 0.8750\n",
      "Train Epoch: 14 [38400/60000] Loss: 0.358767 Acc: 0.8750\n",
      "Train Epoch: 14 [41600/60000] Loss: 0.098375 Acc: 1.0000\n",
      "Train Epoch: 14 [44800/60000] Loss: 0.252813 Acc: 0.8438\n",
      "Train Epoch: 14 [48000/60000] Loss: 0.105699 Acc: 0.9688\n",
      "Train Epoch: 14 [51200/60000] Loss: 0.391207 Acc: 0.8750\n",
      "Train Epoch: 14 [54400/60000] Loss: 0.187191 Acc: 0.9062\n",
      "Train Epoch: 14 [57600/60000] Loss: 0.105252 Acc: 0.9062\n",
      "Elapsed 100.37s, 6.69 s/epoch, 0.00 s/batch, ets 33.46s\n",
      "\n",
      "Test set: Average loss: 0.2937, Accuracy: 8937/10000 (89%)\n",
      "\n",
      "Train Epoch: 15 [3200/60000] Loss: 0.152860 Acc: 0.9688\n",
      "Train Epoch: 15 [6400/60000] Loss: 0.196764 Acc: 0.9062\n",
      "Train Epoch: 15 [9600/60000] Loss: 0.065858 Acc: 1.0000\n",
      "Train Epoch: 15 [12800/60000] Loss: 0.143458 Acc: 0.9375\n",
      "Train Epoch: 15 [16000/60000] Loss: 0.220000 Acc: 0.9375\n",
      "Train Epoch: 15 [19200/60000] Loss: 0.200191 Acc: 0.8750\n",
      "Train Epoch: 15 [22400/60000] Loss: 0.173793 Acc: 0.9062\n",
      "Train Epoch: 15 [25600/60000] Loss: 0.177986 Acc: 0.8750\n",
      "Train Epoch: 15 [28800/60000] Loss: 0.503234 Acc: 0.8125\n",
      "Train Epoch: 15 [32000/60000] Loss: 0.171619 Acc: 0.9375\n",
      "Train Epoch: 15 [35200/60000] Loss: 0.284721 Acc: 0.9062\n",
      "Train Epoch: 15 [38400/60000] Loss: 0.315225 Acc: 0.8750\n",
      "Train Epoch: 15 [41600/60000] Loss: 0.415655 Acc: 0.8750\n",
      "Train Epoch: 15 [44800/60000] Loss: 0.068128 Acc: 1.0000\n",
      "Train Epoch: 15 [48000/60000] Loss: 0.138095 Acc: 0.9688\n",
      "Train Epoch: 15 [51200/60000] Loss: 0.210818 Acc: 0.9062\n",
      "Train Epoch: 15 [54400/60000] Loss: 0.160816 Acc: 0.9375\n",
      "Train Epoch: 15 [57600/60000] Loss: 0.196604 Acc: 0.9375\n",
      "Elapsed 107.14s, 6.70 s/epoch, 0.00 s/batch, ets 26.78s\n",
      "\n",
      "Test set: Average loss: 0.2828, Accuracy: 8990/10000 (90%)\n",
      "\n",
      "Train Epoch: 16 [3200/60000] Loss: 0.057620 Acc: 0.9688\n",
      "Train Epoch: 16 [6400/60000] Loss: 0.121888 Acc: 0.9375\n",
      "Train Epoch: 16 [9600/60000] Loss: 0.094509 Acc: 0.9375\n",
      "Train Epoch: 16 [12800/60000] Loss: 0.278067 Acc: 0.9062\n",
      "Train Epoch: 16 [16000/60000] Loss: 0.070983 Acc: 1.0000\n",
      "Train Epoch: 16 [19200/60000] Loss: 0.201291 Acc: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16 [22400/60000] Loss: 0.347742 Acc: 0.8438\n",
      "Train Epoch: 16 [25600/60000] Loss: 0.180350 Acc: 0.9375\n",
      "Train Epoch: 16 [28800/60000] Loss: 0.058046 Acc: 1.0000\n",
      "Train Epoch: 16 [32000/60000] Loss: 0.266591 Acc: 0.9375\n",
      "Train Epoch: 16 [35200/60000] Loss: 0.218201 Acc: 0.8750\n",
      "Train Epoch: 16 [38400/60000] Loss: 0.070371 Acc: 1.0000\n",
      "Train Epoch: 16 [41600/60000] Loss: 0.115711 Acc: 0.9375\n",
      "Train Epoch: 16 [44800/60000] Loss: 0.256617 Acc: 0.8750\n",
      "Train Epoch: 16 [48000/60000] Loss: 0.150983 Acc: 0.9375\n",
      "Train Epoch: 16 [51200/60000] Loss: 0.107541 Acc: 0.9375\n",
      "Train Epoch: 16 [54400/60000] Loss: 0.057391 Acc: 0.9688\n",
      "Train Epoch: 16 [57600/60000] Loss: 0.258996 Acc: 0.8750\n",
      "Elapsed 113.72s, 6.69 s/epoch, 0.00 s/batch, ets 20.07s\n",
      "\n",
      "Test set: Average loss: 0.2854, Accuracy: 8972/10000 (90%)\n",
      "\n",
      "Train Epoch: 17 [3200/60000] Loss: 0.350819 Acc: 0.8750\n",
      "Train Epoch: 17 [6400/60000] Loss: 0.221051 Acc: 0.9375\n",
      "Train Epoch: 17 [9600/60000] Loss: 0.160791 Acc: 0.9688\n",
      "Train Epoch: 17 [12800/60000] Loss: 0.308498 Acc: 0.8125\n",
      "Train Epoch: 17 [16000/60000] Loss: 0.183566 Acc: 0.9062\n",
      "Train Epoch: 17 [19200/60000] Loss: 0.249773 Acc: 0.9062\n",
      "Train Epoch: 17 [22400/60000] Loss: 0.453275 Acc: 0.8125\n",
      "Train Epoch: 17 [25600/60000] Loss: 0.329399 Acc: 0.7812\n",
      "Train Epoch: 17 [28800/60000] Loss: 0.248101 Acc: 0.9062\n",
      "Train Epoch: 17 [32000/60000] Loss: 0.081818 Acc: 0.9375\n",
      "Train Epoch: 17 [35200/60000] Loss: 0.168356 Acc: 0.9375\n",
      "Train Epoch: 17 [38400/60000] Loss: 0.125672 Acc: 0.9688\n",
      "Train Epoch: 17 [41600/60000] Loss: 0.097566 Acc: 0.9688\n",
      "Train Epoch: 17 [44800/60000] Loss: 0.151840 Acc: 0.9688\n",
      "Train Epoch: 17 [48000/60000] Loss: 0.190647 Acc: 0.9375\n",
      "Train Epoch: 17 [51200/60000] Loss: 0.142548 Acc: 0.9375\n",
      "Train Epoch: 17 [54400/60000] Loss: 0.216620 Acc: 0.9062\n",
      "Train Epoch: 17 [57600/60000] Loss: 0.087549 Acc: 0.9688\n",
      "Elapsed 120.42s, 6.69 s/epoch, 0.00 s/batch, ets 13.38s\n",
      "\n",
      "Test set: Average loss: 0.3014, Accuracy: 8940/10000 (89%)\n",
      "\n",
      "Train Epoch: 18 [3200/60000] Loss: 0.116883 Acc: 0.9688\n",
      "Train Epoch: 18 [6400/60000] Loss: 0.037588 Acc: 0.9688\n",
      "Train Epoch: 18 [9600/60000] Loss: 0.160617 Acc: 0.9375\n",
      "Train Epoch: 18 [12800/60000] Loss: 0.290501 Acc: 0.8750\n",
      "Train Epoch: 18 [16000/60000] Loss: 0.151753 Acc: 0.9062\n",
      "Train Epoch: 18 [19200/60000] Loss: 0.165154 Acc: 0.9375\n",
      "Train Epoch: 18 [22400/60000] Loss: 0.233402 Acc: 0.8750\n",
      "Train Epoch: 18 [25600/60000] Loss: 0.168258 Acc: 0.9375\n",
      "Train Epoch: 18 [28800/60000] Loss: 0.268832 Acc: 0.8438\n",
      "Train Epoch: 18 [32000/60000] Loss: 0.451161 Acc: 0.8438\n",
      "Train Epoch: 18 [35200/60000] Loss: 0.091762 Acc: 0.9688\n",
      "Train Epoch: 18 [38400/60000] Loss: 0.258829 Acc: 0.9375\n",
      "Train Epoch: 18 [41600/60000] Loss: 0.302335 Acc: 0.9375\n",
      "Train Epoch: 18 [44800/60000] Loss: 0.136723 Acc: 0.9062\n",
      "Train Epoch: 18 [48000/60000] Loss: 0.335525 Acc: 0.8125\n",
      "Train Epoch: 18 [51200/60000] Loss: 0.108735 Acc: 0.9375\n",
      "Train Epoch: 18 [54400/60000] Loss: 0.110170 Acc: 1.0000\n",
      "Train Epoch: 18 [57600/60000] Loss: 0.208705 Acc: 0.9062\n",
      "Elapsed 127.13s, 6.69 s/epoch, 0.00 s/batch, ets 6.69s\n",
      "\n",
      "Test set: Average loss: 0.2760, Accuracy: 9029/10000 (90%)\n",
      "\n",
      "Train Epoch: 19 [3200/60000] Loss: 0.338423 Acc: 0.8438\n",
      "Train Epoch: 19 [6400/60000] Loss: 0.087157 Acc: 0.9688\n",
      "Train Epoch: 19 [9600/60000] Loss: 0.525601 Acc: 0.9062\n",
      "Train Epoch: 19 [12800/60000] Loss: 0.094365 Acc: 0.9688\n",
      "Train Epoch: 19 [16000/60000] Loss: 0.068628 Acc: 1.0000\n",
      "Train Epoch: 19 [19200/60000] Loss: 0.440677 Acc: 0.8125\n",
      "Train Epoch: 19 [22400/60000] Loss: 0.060394 Acc: 0.9688\n",
      "Train Epoch: 19 [25600/60000] Loss: 0.118227 Acc: 0.9688\n",
      "Train Epoch: 19 [28800/60000] Loss: 0.249056 Acc: 0.8750\n",
      "Train Epoch: 19 [32000/60000] Loss: 0.133308 Acc: 1.0000\n",
      "Train Epoch: 19 [35200/60000] Loss: 0.227074 Acc: 0.8750\n",
      "Train Epoch: 19 [38400/60000] Loss: 0.110251 Acc: 0.9688\n",
      "Train Epoch: 19 [41600/60000] Loss: 0.110069 Acc: 1.0000\n",
      "Train Epoch: 19 [44800/60000] Loss: 0.161664 Acc: 0.9062\n",
      "Train Epoch: 19 [48000/60000] Loss: 0.242238 Acc: 0.9062\n",
      "Train Epoch: 19 [51200/60000] Loss: 0.425480 Acc: 0.8438\n",
      "Train Epoch: 19 [54400/60000] Loss: 0.349881 Acc: 0.8750\n",
      "Train Epoch: 19 [57600/60000] Loss: 0.193975 Acc: 0.9375\n",
      "Elapsed 133.77s, 6.69 s/epoch, 0.00 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.2908, Accuracy: 9008/10000 (90%)\n",
      "\n",
      "Total time: 134.47, Best Loss: 0.276\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "modelBN = LeNetBN() \n",
    "\n",
    "model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc = main(model)\n",
    "\n",
    "modelBN, epoch_train_loss_bn, epoch_train_acc_bn, epoch_test_loss_bn, epoch_test_acc_bn = main(modelBN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">10. Plot Loss</font> <a name=\"plot-loss\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "x = range(len(epoch_train_loss))\n",
    "\n",
    "\n",
    "plt.figure\n",
    "plt.plot(x, epoch_train_loss, 'r', label=\"train loss\")\n",
    "plt.plot(x, epoch_test_loss, 'b', label=\"validation loss\")\n",
    "\n",
    "plt.plot(x, epoch_train_loss_bn, 'r--', label=\"train loss with BN\")\n",
    "plt.plot(x, epoch_test_loss_bn, 'b--',label=\"validation loss with BN\")\n",
    "\n",
    "plt.xlabel('epoch no.')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above curves show that when we use Batch Normalization, the training converges faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue\">Miscellaneous: Calculate Mean and Standard Deviation of Fashion MNIST </font> <a name=\"misc\"></a>\n",
    "\n",
    "Ideally, we should not use the same mean and standard deviation for Fashion MNIST and MNIST. Refrain, even when you find many continuing to do this, simply because it does not have a profound effect on the results.\n",
    "\n",
    "Let us find  the mean and standard deviation for Fashion MNIST and use it instead of MNIST.\n",
    "\n",
    "We need to simply find the mean and standard deviation of the whole dataset. So, we load the dataset and then use the functions given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2860)\n",
      "tensor(0.3530)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "train_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=train_transform)\n",
    "\n",
    "print(train_set.data.float().mean()/255)\n",
    "print(train_set.data.float().std()/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
