{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">How to stabilize landmark points in a video</font>\n",
    "\n",
    "When you use Dlib’s Facial Landmark Detection on a video, you will notice they jiggle a bit. When the video is obtained under good and consistent lighting conditions, the landmarks tend to be more stable than when the lighting or imaging conditions are bad. \n",
    "\n",
    "The most important reason for this instability is that the landmarks are detected in every frame independently. There is nothing that ties the information in one frame to the information in the next. \n",
    "\n",
    "In this section, we will present a few strategies for stabilizing facial landmark points in videos. Depending on the algorithm we use for stabilizing the points, we will need up to four different pieces of information for stabilization\n",
    "\n",
    "1. The location of a landmark in the current frame. \n",
    "\n",
    "2. The location of the same landmark in the previous frame(s). \n",
    "\n",
    "3. Pixels intensities in a small window in the current frame around the location of the landmark. \n",
    "\n",
    "4. Pixels intensities in a small window in the previous frame around the location of the landmark. \n",
    "\n",
    "**Note:** The methods we use are very general and can be applied to tracking points in general and not just facial landmarks. \n",
    "\n",
    "## <font style = \"color:rgb(50,120,229)\">Moving average : A simple solution</font>\n",
    "\n",
    "The easiest solution to stabilize the points is to average the point locations over a small time window. This is called a moving average. \n",
    "\n",
    "For example, you can replace the current location of a landmark by the average location of the landmark over the last 5 frames. You could also use a weighted average, where the landmark location in the frame closest to the current frame is weighted more than the frames further away. \n",
    "\n",
    "The basic assumptions behind using moving average to stabilize landmark points are the following \n",
    "\n",
    "1. The motion of the points are slow compared to the frame rate of the video. \n",
    "\n",
    "2. The noise in the estimated landmark locations is zero mean. In other words, the process of averaging points over multiple frames will cancel out the noise. \n",
    "\n",
    "When the first assumption is violated, moving average produces points that lag the actual location of the landmarks. \n",
    "\n",
    "The moving averages algorithm is very easy to implement and therefore quite frequently used. \n",
    "\n",
    "## <font style = \"color:rgb(50,120,229)\">Kalman Filtering</font>\n",
    "\n",
    "Computer Vision is not rocket science! But in Computer Vision, we do use technology that was initially used for tracking missiles and spacecrafts. One such technique is called Kalman Filtering invented by Rudolf E. Kálmán during the height of the cold war. \n",
    "\n",
    "In this approach, a prediction about the location of the landmark points ( or missiles for that matter ) is made based on the current state ( location + velocity ) of the landmark point. Kalman Filtering takes into account the measurement noise and the state is constantly updated based on quality of prediction on the current frame. This produces better results compared to moving averages, but requires more skill and understanding. However, like the moving average method, the pixel values are not used to estimate the location of landmark points. \n",
    "\n",
    "In this course, we will not cover Kalman Filtering because the method we describe next is more appropriate for tracking points on images. \n",
    "\n",
    "## <font style = \"color:rgb(50,120,229)\">Optical Flow</font>\n",
    "\n",
    "In the methods described so far, only the location and velocity of the landmarks were used for tracking. However, because landmarks are points in an image, it makes sense to use a small patch around a landmarks in one frame to locate it in the next frame. \n",
    "\n",
    "The technique we describe next is called Optical Flow and it uses pixel information for making a prediction. \n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">What is Optical Flow?</font>\n",
    "\n",
    "Let us say, we are tracking a landmark point located at point $(x,y)$ in the image at frame $t$. In the next frame $(t+dt)$, the landmark moves to the point $(x+dx$ , $y+dy)$ where $dx$ , $dy$ are small displacements in the x and y directions. \n",
    "\n",
    "The optical flow at a point is simply the velocity vector of the point. It is given by \n",
    "\n",
    "$$\n",
    "(u, v) = \\left ( \\frac{dx}{dt}, \\frac{dy}{dt} \\right )\n",
    "$$  \n",
    "\n",
    "When the optical flow is calculated over the entire image, it is called **dense optical flow** and when it is calculated over just a few points ( e.g. landmark points or feature points ), it is called **sparse optical flow.**\n",
    "\n",
    "We want to calculate the optical flow at landmark locations, because once we have calculated the motion vector $(u,v)$, we are able to predict the location of landmark in the next frame based on the current frame by simply adding the the motion vector to the landmark location. \n",
    "\n",
    "We have additional material on Optical flow in the next section. You can go through them for more information on Optical Flow.\n",
    "\n",
    "One of the methods of calculating Optical Flow is Lucas Kanade Algorithm. Again, we have additional material on Lucas Kanade algorithm in the next section. Here, we will just give a brief overview. You can go over Lucas Kanade algorithm in detail in the next section. Think of it as if we are using it to find the motion of particular landmark points in the next frame so that we can stabilize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Lucas-Kanade Optical Flow</font>\n",
    "\n",
    "This algorithm is used for finding optical flow at discrete points. In our case, we will use this algorithm to find the flow vectors at landmark points. \n",
    "\n",
    "Like any other optical flow algorithm, Lucas and Kanade had to make an assumption to solve the under-constrained optical flow equation. They made an assumption that in a small window centered at the feature point, the optical flow is the same for all pixels. So if we choose a 3x3 window around the feature point, there are 9 equations and 2 unknowns. And yes, this system of equations is easily solvable using simple linear algebra. \n",
    "\n",
    "Without going into the derivation of Lucas-Kanade optical flow, we are simply writing the final results in matrix form. \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} u \\\\ v  \\end{bmatrix}  = \\begin{bmatrix} \\sum I^2_x & \\sum I_x I_y \\\\   \\sum I_y I_x  & \\sum I^2_y \\end{bmatrix}^{−1} \\begin{bmatrix} \\sum −I_x I_t \\\\ \\sum −I_y I_t  \\end{bmatrix}  \n",
    "$$\n",
    "\n",
    "Where the summation is carried over the small window and \n",
    "\n",
    "$$\n",
    "I_x = \\frac{\\partial I}{\\partial x}, I_y = \\frac{\\partial I}{\\partial y} \\text{ and } I_t = \\frac{\\partial I}{\\partial t}\n",
    "$$\n",
    "\n",
    "## <font style = \"color:rgb(50,120,229)\">Handling Large Motion</font> \n",
    "\n",
    "The optical flow equation derived so far only holds for differential motion. Which means it will work when the motion is less than a pixel. Fortunately, there is a way to handle it using Image Pyramids. \n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">What are Image Pyramids?</font>\n",
    "\n",
    "An Image Pyramid is a multiscale representation of an image. The most common kind of Image Pyramid is called the **Gaussian Pyramid**.\n",
    "\n",
    "<center> <a href=\"https://www.learnopencv.com/wp-content/uploads/2017/12/opcv4face-w3-m3-ImagePyramid.png\"><img src = \"https://www.learnopencv.com/wp-content/uploads/2017/12/opcv4face-w3-m3-ImagePyramid.png\"/></a></center>\n",
    "\n",
    "&nbsp;\n",
    "The Gaussian Pyramid is produced by blurring the image using a Gaussian Kernel followed by downsizing the image by a factor of 2. Blurring is done to remove high frequency signal that can cause artifacts in a lower resolution image. \n",
    "\n",
    "For optical flow calculation, the motion vectors are calculated at the lowest level where the motion is indeed subpixel. The flow vectors are propagated up to the next higher level of the pyramid. A more accurate optical flow is then recalculated for this new level using the optical flow from the previous level as an initial guess. The resulting flow is propagated one level up. This process is repeated until the flow is calculated at the highest level.\n",
    "\n",
    "**<font style=\"color:rgb(255,0,0)\">Note:</font>** If the motion is extremely large, even the pyramids will not help. In such cases, we say that we have lost track of the point and it needs to be detected again. \n",
    "\n",
    "## <font style = \"color:rgb(50,120,229)\">Combining Detection and Tracking</font>\n",
    "\n",
    "We have learned that we can predict the location of landmark points in the current frame given its location in the previous frame using optical flow. \n",
    "\n",
    "We also know the location of landmarks on the current frame calculated by the Facial Landmark Detector. \n",
    "\n",
    "Which of the two is more accurate?\n",
    "\n",
    "Typically, when the motion is small and the appearance of the tracked landmark does not change between frames, tracking does a very good job. However, it is not uncommon for a tracker to lose track of the point completely because of large motion. \n",
    "\n",
    "On the other hand, the landmark detector usually provides a good rough location of the point. \n",
    "\n",
    "Therefore, when the motion of the face is small, we can choose the location predicted by optical flow. On the other hand, if the landmark locations predicted by the detector and the optical flow tracker are not close, we can assume the tracker has lost track and trust the landmark location predicted by the detector. \n",
    "\n",
    "However, if we make this binary decision to choose one prediction over the other, we will see the points jump around in a non-smooth way. So, we need to combine the two predictions in a more intelligent way. \n",
    "\n",
    "Here is the trick to combine the two estimates. \n",
    "\n",
    "Let, \n",
    "\n",
    "$p(t)$ = Position of a landmark in the current frame.\n",
    "\n",
    "$p(t-1)$ = Position of the landmark in the previous frame.\n",
    "\n",
    "$p_0(t)$ =  Position of the landmark predicted by optical flow in the current frame. \n",
    "\n",
    "Now, we need to combine $p(t)$ and $p_0(t)$ in some ratio. Let us call this ratio $\\alpha$ , where $$ 0 \\leq \\alpha \\leq 1 $$ We can find the stabilized point $p_s(t)$ using the following formula\n",
    "\n",
    "$$  \n",
    "p_s(t) = ( 1 − \\alpha ) p(t) + \\alpha p_o(t) \n",
    "$$\n",
    "\n",
    "Stare at that equation for a bit and you will notice, when $\\alpha=0.5$, the two predictions contribute equally to the final prediction. When $\\alpha=0$, we fully trust the position given by the landmark detector - $p(t)$ and when $\\alpha=1$ , fully trust the prediction given by optical flow - $p_0(t)$. \n",
    "\n",
    "As mentioned above, we also want this $\\alpha$ to depend on how fast the point is moving. In other words, we want $\\alpha$ to depend on the distance between the location of the point in the current frame ( i.e. $p(t)$) and the location of the point in the previous frame ( $p(t-1)$).\n",
    "\n",
    "Let, \n",
    "\n",
    "$d$ = Distance between $p(t)$ and $p(t-1)$ = $||p(t)-p(t-1)||$ \n",
    "\n",
    "The distance $d$ can take any value between 0 and infinity ( well, theoretically speaking ). We need to map it between 0 and 1. To do this we define $\\alpha$ to be the following \n",
    "\n",
    "$$\n",
    "\\alpha = e^{\\frac{−d^2}{\\sigma^2}}\n",
    "$$\n",
    "\n",
    "Looks exotic, but it is a simple way to map $d$ to a number between 0 and 1. What about $\\sigma^2$? It is used to normalize $d$ because the distance $d$ is in pixels. For a high resolution image the $d$ will be higher compared to a lower resolution image for the same level of motion. So those numbers need to be scaled. In our implementation, $d$  is normalized by $\\sigma$ which is proportional to the distance ( in pixels ) between the corners of the eyes. This is a better measure of scale than image resolution because we can have a small face in a high resolution image and vice versa. \n",
    "\n",
    "## <font style = \"color:rgb(50,120,229)\">Stabilizing landmarks using OpenCV</font>\n",
    "\n",
    "As we know from previous sections, one way to implement stabilization requires optical flow calculation. Fortunately, OpenCV has a good implementation of Lukas Kanade optical flow described in this section. It can be invoked using **[`calcOpticalFlowPyrLK`](https://docs.opencv.org/4.1.0/dc/d6b/group__video__track.html#ga473e4b886d0bcc6b65831eb88ed93323)**.\n",
    "\n",
    "```python\n",
    "\n",
    "nextPts, status, err\t=\tcv.calcOpticalFlowPyrLK(\tprevImg, nextImg, prevPts, nextPts[, status[, err[, winSize[, maxLevel[, criteria[, flags[, minEigThreshold]]]]]]]\t)\n",
    "\n",
    "```\n",
    "\n",
    "Where,\n",
    "\n",
    "- **`prevImg`** - first 8-bit input image or pyramid constructed by buildOpticalFlowPyramid.\n",
    "- **`nextImg`** - second input image or pyramid of the same size and the same type as prevImg.\n",
    "- **`prevPts`** - vector of 2D points for which the flow needs to be found; point coordinates must be single-precision floating-point numbers.\n",
    "- **`nextPts`** - output vector of 2D points (with single-precision floating-point coordinates) containing the calculated new positions of input features in the second image; when OPTFLOW_USE_INITIAL_FLOW flag is passed, the vector must have the same size as in the input.\n",
    "- **`status`** - output status vector (of unsigned chars); each element of the vector is set to 1 if the flow for the corresponding features has been found, otherwise, it is set to 0.\n",
    "- **`err`** - output vector of errors; each element of the vector is set to an error for the corresponding feature, type of the error measure can be set in flags parameter; if the flow wasn't found then the error is not defined (use the status parameter to find such cases).\n",
    "- **`winSize`** - size of the search window at each pyramid level.\n",
    "maxLevel\t0-based maximal pyramid level number; if set to 0, pyramids are not used (single level), if set to 1, two levels are used, and so on; if pyramids are passed to input then algorithm will use as many levels as pyramids have but no more than maxLevel.\n",
    "- **`criteria`** - parameter, specifying the termination criteria of the iterative search algorithm (after the specified maximum number of iterations criteria.maxCount or when the search window moves by less than criteria.epsilon.\n",
    "- **`flags`** - operation flags:\n",
    "  - **`OPTFLOW_USE_INITIAL_FLOW`** - uses initial estimations, stored in nextPts; if the flag is not set, then prevPts is copied to nextPts and is considered the initial estimate.\n",
    "  - **`OPTFLOW_LK_GET_MIN_EIGENVALS`** - use minimum eigen values as an error measure (see minEigThreshold description); if the flag is not set, then L1 distance between patches around the original and a moved point, divided by number of pixels in a window, is used as a error measure.\n",
    "- **`minEigThreshold`** - the algorithm calculates the minimum eigen value of a 2x2 normal matrix of optical flow equations (this matrix is called a spatial gradient matrix), divided by number of pixels in a window; if this value is less than minEigThreshold, then a corresponding feature is filtered out and its flow is not processed, so it allows to remove bad points and get a performance boost.\n",
    "\n",
    "As mentioned earlier, optical flow computation requires building Image pyramids for the current frame and the previous frame. **calcOpticalFlowPyrLK**does this calculation internally when you pass the previous frame and the current frame. When using optical flow in a video, the image pyramid for the same frame is built twice -- once while doing optical flow calculation for the current frame and the other for the next frame. This double calculation can be avoided by building and storing the image pyramid for every frame and passing it to **calcOpticalFlowPyrLK**.\n",
    "\n",
    "The most common usage of **[`buildOpticalFlowPyramid`](https://docs.opencv.org/4.1.0/dc/d6b/group__video__track.html#ga86640c1c470f87b2660c096d2b22b2ce)** is shown below.\n",
    "\n",
    "```python\n",
    "\n",
    "retval, pyramid\t=\tcv.buildOpticalFlowPyramid(\timg, winSize, maxLevel[, pyramid[, withDerivatives[, pyrBorder[, derivBorder[, tryReuseInputImage]]]]]\t)\n",
    "\n",
    "```\n",
    "\n",
    "Where,\n",
    "\n",
    "\n",
    "- **`img`** - 8-bit input image.\n",
    "- **`pyramid`** - output pyramid.\n",
    "- **`winSize`** - window size of optical flow algorithm. Must be not less than winSize argument of `calcOpticalFlowPyrLK`. It is needed to calculate required padding for pyramid levels.\n",
    "- **`maxLevel`** - 0-based maximal pyramid level number.\n",
    "- **`withDerivatives`** - set to precompute gradients for the every pyramid level. If pyramid is constructed without the gradients then calcOpticalFlowPyrLK will calculate them internally.\n",
    "- **`pyrBorder`** - the border mode for pyramid layers.\n",
    "- **`derivBorder`** - the border mode for gradients.\n",
    "- **`tryReuseInputImage`** - put ROI of input image into the pyramid if possible. You can pass false to force data copying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, dlib\n",
    "import numpy as np\n",
    "import math, sys\n",
    "from dataPath import DATA_PATH\n",
    "from dataPath import MODEL_PATH\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (6.0,6.0)\n",
    "matplotlib.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTOR_PATH = MODEL_PATH + \"shape_predictor_68_face_landmarks.dat\"\n",
    "RESIZE_HEIGHT = 480\n",
    "NUM_FRAMES_FOR_FPS = 100\n",
    "SKIP_FRAMES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the intereye distance.\n",
    "def interEyeDistance(predict):\n",
    "  leftEyeLeftCorner = (predict[36].x, predict[36].y)\n",
    "  rightEyeRightCorner = (predict[45].x, predict[45].y)\n",
    "  distance = cv2.norm(np.array(rightEyeRightCorner) - np.array(leftEyeLeftCorner))\n",
    "  distance = int(distance)\n",
    "  return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "winName = \"Stabilized facial landmark detector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoFileName = \"\"\n",
    "\n",
    "# Initializing video capture object.\n",
    "cap = cv2.VideoCapture(videoFileName)\n",
    "\n",
    "if(cap.isOpened()==False):\n",
    "  print(\"Unable to load video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(8,133,37)\">Specifying the parameters of Lucas Kanade method </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winSize = 101\n",
    "maxLevel = 10\n",
    "fps = 30.0\n",
    "# Grab a frame\n",
    "ret,imPrev = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to grayscale.\n",
    "imGrayPrev = cv2.cvtColor(imPrev, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the size of the image.\n",
    "size = imPrev.shape[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "landmarkDetector = dlib.shape_predictor(PREDICTOR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(8,133,37)\">Initializing the points </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the parameters\n",
    "points=[]\n",
    "pointsPrev=[]\n",
    "pointsDetectedCur=[]\n",
    "pointsDetectedPrev=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eyeDistanceNotCalculated = True\n",
    "eyeDistance = 0\n",
    "isFirstFrame = True\n",
    "# Initial value, actual value calculated after 100 frames\n",
    "fps = 10\n",
    "showStabilized = False\n",
    "count =0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:rgb(8,133,37)\">Use Detection + Tracking for stabilization </font>\n",
    "\n",
    "As discussed above, we detect the landmarks in each frame and also use the Lucas Kanade method to track the points in the current frame w.r.t previous frame. Then we take a weighted average of the two measurements and that is the stabilized landmark point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "  if (count==0):\n",
    "    t = cv2.getTickCount()\n",
    "\n",
    "  # Grab a frame\n",
    "  ret,im = cap.read()\n",
    "  imDlib = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "  # COnverting to grayscale\n",
    "  imGray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "  height = im.shape[0]\n",
    "  IMAGE_RESIZE = float(height)/RESIZE_HEIGHT\n",
    "  # Resize image for faster face detection\n",
    "  imSmall = cv2.resize(im, None, fx=1.0/IMAGE_RESIZE, fy=1.0/IMAGE_RESIZE,interpolation = cv2.INTER_LINEAR)\n",
    "  imSmallDlib = cv2.cvtColor(imSmall, cv2.COLOR_BGR2RGB)\n",
    "  # Skipping the frames for faster processing\n",
    "  if (count % SKIP_FRAMES == 0):\n",
    "    faces = detector(imSmallDlib,0)\n",
    "\n",
    "  # If no face was detected\n",
    "  if len(faces)==0:\n",
    "    print(\"No face detected\")\n",
    "\n",
    "  # If faces are detected, iterate through each image and detect landmark points\n",
    "  else:\n",
    "    for i in range(0,len(faces)):\n",
    "      print(\"face detected\")\n",
    "      # Face detector was found over a smaller image.\n",
    "      # So, we scale face rectangle to correct size.\n",
    "      newRect = dlib.rectangle(int(faces[i].left() * IMAGE_RESIZE),\n",
    "        int(faces[i].top() * IMAGE_RESIZE),\n",
    "        int(faces[i].right() * IMAGE_RESIZE),\n",
    "        int(faces[i].bottom() * IMAGE_RESIZE))\n",
    "      \n",
    "      # Detect landmarks in current frame\n",
    "      landmarks = landmarkDetector(imDlib, newRect).parts()\n",
    "      \n",
    "      # Handling the first frame of video differently,for the first frame copy the current frame points\n",
    "      \n",
    "      if (isFirstFrame==True):\n",
    "        pointsPrev=[]\n",
    "        pointsDetectedPrev = []\n",
    "        [pointsPrev.append((p.x, p.y)) for p in landmarks]\n",
    "        [pointsDetectedPrev.append((p.x, p.y)) for p in landmarks]\n",
    "\n",
    "      # If not the first frame, copy points from previous frame.\n",
    "      else:\n",
    "        pointsPrev=[]\n",
    "        pointsDetectedPrev = []\n",
    "        pointsPrev = points\n",
    "        pointsDetectedPrev = pointsDetectedCur\n",
    "\n",
    "      # pointsDetectedCur stores results returned by the facial landmark detector\n",
    "      # points stores the stabilized landmark points\n",
    "      points = []\n",
    "      pointsDetectedCur = []\n",
    "      [points.append((p.x, p.y)) for p in landmarks]\n",
    "      [pointsDetectedCur.append((p.x, p.y)) for p in landmarks]\n",
    "\n",
    "      # Convert to numpy float array\n",
    "      pointsArr = np.array(points,np.float32)\n",
    "      pointsPrevArr = np.array(pointsPrev,np.float32)\n",
    "\n",
    "      # If eye distance is not calculated before\n",
    "      if eyeDistanceNotCalculated:\n",
    "        eyeDistance = interEyeDistance(landmarks)\n",
    "        print(eyeDistance)\n",
    "        eyeDistanceNotCalculated = False\n",
    "\n",
    "      if eyeDistance > 100:\n",
    "          dotRadius = 3\n",
    "      else:\n",
    "        dotRadius = 2\n",
    "\n",
    "      print(eyeDistance)\n",
    "      sigma = eyeDistance * eyeDistance / 400\n",
    "      s = 2*int(eyeDistance/4)+1\n",
    "\n",
    "      #  Set up optical flow params\n",
    "      lk_params = dict(winSize  = (s, s), maxLevel = 5, criteria = (cv2.TERM_CRITERIA_COUNT | cv2.TERM_CRITERIA_EPS, 20, 0.03))\n",
    "      # Python Bug. Calculating pyramids and then calculating optical flow results in an error. So directly images are used.\n",
    "      # ret, imGrayPyr= cv2.buildOpticalFlowPyramid(imGray, (winSize,winSize), maxLevel)\n",
    "\n",
    "      pointsArr,status, err = cv2.calcOpticalFlowPyrLK(imGrayPrev,imGray,pointsPrevArr,pointsArr,**lk_params)\n",
    "      \n",
    "\n",
    "      # Converting to float\n",
    "      pointsArrFloat = np.array(pointsArr,np.float32)\n",
    "\n",
    "      # Converting back to list\n",
    "      points = pointsArrFloat.tolist()\n",
    "\n",
    "      # Final landmark points are a weighted average of\n",
    "      # detected landmarks and tracked landmarks\n",
    "      for k in range(0,len(landmarks)):\n",
    "        d = cv2.norm(np.array(pointsDetectedPrev[k]) - np.array(pointsDetectedCur[k]))\n",
    "        alpha = math.exp(-d*d/sigma)\n",
    "        points[k] = (1 - alpha) * np.array(pointsDetectedCur[k]) + alpha * np.array(points[k])\n",
    "\n",
    "      # Drawing over the stabilized landmark points\n",
    "      if showStabilized is True:\n",
    "        for p in points:\n",
    "          cv2.circle(im,(int(p[0]),int(p[1])),dotRadius, (255,0,0),-1)\n",
    "      else:\n",
    "        for p in pointsDetectedCur:\n",
    "          cv2.circle(im,(int(p[0]),int(p[1])),dotRadius, (0,0,255),-1)\n",
    "\n",
    "      isFirstFrame = False\n",
    "      count = count+1\n",
    "\n",
    "      # Calculating the fps value\n",
    "      if ( count == NUM_FRAMES_FOR_FPS):\n",
    "        t = (cv2.getTickCount()-t)/cv2.getTickFrequency()\n",
    "        fps = NUM_FRAMES_FOR_FPS/t\n",
    "        count = 0\n",
    "        isFirstFrame = True\n",
    "\n",
    "      # Display the landmarks points\n",
    "      cv2.putText(im, \"{:.1f}-fps\".format(fps), (50, size[0]-50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0, 0, 255), 3,cv2.LINE_AA)\n",
    "      cv2.imshow(winName, im)\n",
    "      key = cv2.waitKey(25) & 0xFF\n",
    "\n",
    "      # Use spacebar to toggle between Stabilized and Unstabilized version.\n",
    "      if key==32:\n",
    "        showStabilized = not showStabilized\n",
    "\n",
    "      # Stop the program.\n",
    "      if key==27:\n",
    "        sys.exit()\n",
    "      # Getting ready for next frame\n",
    "      imPrev = im\n",
    "      imGrayPrev = imGray\n",
    "\n",
    "cv2.destroyAllwindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font style=\"color:rgb(255,0,0)\">NOTE:</font>** Be careful with the code in Python. The **cv2.calcOpticalFlowPyrLK** returns a numpy array but for appending the points we are using a list, hence once we get the array we should convert it into a list. \n",
    "\n",
    "# <font style = \"color:rgb(50,120,229)\">References and Further readings</font>\n",
    "\n",
    "1. [https://en.wikipedia.org/wiki/Moving_average](https://en.wikipedia.org/wiki/Moving_average)\n",
    "\n",
    "2. [http://www.analog.com/media/en/technical-documentation/dsp-book/dsp_book_Ch15.pdf](http://www.analog.com/media/en/technical-documentation/dsp-book/dsp_book_Ch15.pdf)\n",
    "\n",
    "3. [https://en.wikipedia.org/wiki/Lucas%E2%80%93Kanade_method](https://en.wikipedia.org/wiki/Lucas%E2%80%93Kanade_method)\n",
    "\n",
    "4. [https://en.wikipedia.org/wiki/Kanade%E2%80%93Lucas%E2%80%93Tomasi_feature_tracker](https://en.wikipedia.org/wiki/Kanade%E2%80%93Lucas%E2%80%93Tomasi_feature_tracker)\n",
    "\n",
    "5. [https://en.wikipedia.org/wiki/Kalman_filter](https://en.wikipedia.org/wiki/Kalman_filter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
