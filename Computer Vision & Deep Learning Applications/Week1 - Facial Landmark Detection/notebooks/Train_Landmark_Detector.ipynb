{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">How to Train a Custom Facial Landmark Detector</font>\n",
    "\n",
    "# <font style = \"color:rgb(50,120,229)\">Introduction</font>\n",
    "\n",
    "To train a facial landmark detector, we need three pieces of information\n",
    "\n",
    "1. A few thousand images of containing faces.\n",
    "\n",
    "2. Bounding boxes corresponding to faces in those images.\n",
    "\n",
    "3. Accurately placed landmarks for each face in the image. \n",
    "\n",
    "Most facial landmark datasets are annotated for a relatively smaller number of points. \n",
    "\n",
    "Fortunately, Imperial College of London’s research group, iBUG, annotated many famous datasets (300-W, XM2VTS, FRGC, LFPW, HELEN, AFW and IBUG) for the same set of 68 facial landmarks.\n",
    "\n",
    "Dlib’s facial landmark model is trained on a subset of this data which consists AFW, HELEN, iBUG and LFPW datasets.\n",
    "\n",
    "# <font style = \"color:rgb(50,120,229)\">Train 70-points Facial Landmarks</font>\n",
    "\n",
    "The 68 points do not include the centers of the iris. So, our data collection team spent hours to annotate these two additional eye points. We are making this 70-points dataset available to everybody in this course.\n",
    "\n",
    "<center> <a href=\"https://www.learnopencv.com/wp-content/uploads/2017/12/opcv4face-w3-m6-68points.jpg\"><img src = \"https://www.learnopencv.com/wp-content/uploads/2017/12/opcv4face-w3-m6-68points.jpg\" width=500/></a></center>\n",
    "\n",
    "## <font style = \"color:rgb(50,120,229)\">Data</font>\n",
    "You can download the data from the download link given at the top of the module. You can also download it from Dropbox **[link](https://www.dropbox.com/s/q5w1wolzhtnuy5q/facial_landmark_data.zip?dl=1)**. You can get a quick overview on how data is organized in facial_landmark_data directory by looking at the following snapshot.\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\"><a href=\"https://www.dropbox.com/s/q5w1wolzhtnuy5q/facial_landmark_data.zip?dl=1\">Dropbox Link for Data</a></font>\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Dataset Structure</font>\n",
    "\n",
    "`facial_landmark_data` directory has:\n",
    "\n",
    "3 folders\n",
    "\n",
    "+ datasets - contains images and annotation files for face rectangles and facial landmarks (for both 70-points and 33-points)\n",
    "+ 33_points - contains XML files for training & testing and 33-points shape predictor model\n",
    "+ 70_points - contains XML files for training & testing and 70-points shape predictor model\n",
    "\n",
    "and\n",
    "\n",
    "6 files\n",
    "\n",
    "+ image_names.txt - image paths to images in datasets directory, relative to this folder\n",
    "+ 33_point_landmarks.jpg - 33 facial landmarks marked on face\n",
    "+ 70_point_landmarks.jpg - 70 facial landmarks marked on face\n",
    "+ shape_predictor_70_face_landmarks.dat - Dlib shape predictor model file for 70 facial landmarks\n",
    "+ training_with_face_landmarks.xml - input file to training code, created from annotation files for 70-points to train model\n",
    "+ testing_with_face_landmarks.xml - input file to training code, created from annotation files for 70-points to test model\n",
    "\n",
    "```\n",
    "facial_landmark_data\n",
    "├── image_names.txt\n",
    "│\n",
    "├── 33_point_landmarks.jpg\n",
    "├── 70_point_landmarks.jpg\n",
    "│\n",
    "├── 33_points\n",
    "│   ├── shape_predictor_33_face_landmarks.dat\n",
    "│   ├── testing_with_face_landmarks.xml\n",
    "│   └── training_with_face_landmarks.xml\n",
    "│\n",
    "├── 70_points\n",
    "│   ├── shape_predictor_70_face_landmarks.dat\n",
    "│   ├── testing_with_face_landmarks.xml\n",
    "│   └── training_with_face_landmarks.xml\n",
    "│\n",
    "```\n",
    "\n",
    "`datasets` directory has 4 sub directories:\n",
    "\n",
    "+ afw\n",
    "+ helen\n",
    "+ ibug\n",
    "+ lfpw\n",
    "\n",
    "Within each of these 4 sub directories, we have:\n",
    "\n",
    "+ an image file e.g. afw/111076519_1.jpg\n",
    "+ annotation file containing rectangle corresponding to face in image e.g. afw/111076519_1_rect.txt\n",
    "+ 2 annotation files (33-points & 70-points) containing facial landmarks corresponding to same face of image which is specified in _rect.txt e.g. afw/111076519_1_bv33.txt and afw/111076519_1_bv70.txt\n",
    "+ mirrored image of original image e.g. afw/111076519_1_mirror.jpg\n",
    "+ annotation file containing rectangle corresponding to face in mirrored image e.g. afw/111076519_1_mirror_rect.txt\n",
    "+ 2 annotations files for facial landmarks corresponding to same face of mirrored image which is specified in _mirror_rect.txt e.g. 111076519_2_mirror_bv33.txt & 111076519_2_mirror_bv70.txt\n",
    "\n",
    "An important point to note here is that each of _rect.txt and _bv33.txt or _bv70.txt contains annotation for only 1 face. If an image has more than 1 faces, we create separate annotation and image files and suffix it with a number. e.g. 111076519 has 2 faces, so we have 111076519_1.jpg and 111076519_2.jpg and other files (annotation files, mirrored image & annotation files corresponding to mirrored image). If you look at 111076519_1.jpg and 111076519_2.jpg, both images are same.\n",
    "\n",
    "```\n",
    "│\n",
    "├── datasets\n",
    "│   ├── afw\n",
    "│   │   ├── 111076519_1.jpg\n",
    "│   │   ├── 111076519_1_rect.txt\n",
    "│   │   ├── 111076519_1_bv33.txt\n",
    "│   │   ├── 111076519_1_bv70.txt\n",
    "│   │   ├── 111076519_1_mirror.jpg\n",
    "│   │   ├── 111076519_1_mirror_rect.txt\n",
    "│   │   ├── 111076519_1_mirror_bv33.txt\n",
    "│   │   ├── 111076519_1_mirror_bv70.txt\n",
    "│   │   ├── 111076519_2.jpg\n",
    "│   │   ├── 111076519_2_rect.txt\n",
    "│   │   ├── 111076519_2_bv33.txt\n",
    "│   │   ├── 111076519_2_bv70.txt\n",
    "│   │   ├── 111076519_2_mirror.jpg\n",
    "│   │   ├── 111076519_2_mirror_rect.txt\n",
    "│   │   ├── 111076519_2_mirror_bv33.txt\n",
    "│   │   ├── 111076519_2_mirror_bv70.txt\n",
    "│   ├── helen\n",
    "│   │   ├── testset\n",
    "│   │   │   ├── 2978322154_1.jpg\n",
    "│   │   │   ├── 2978322154_1_rect.txt\n",
    "│   │   │   ├── 2978322154_1_bv33.txt\n",
    "│   │   │   ├── 2978322154_1_bv70.txt\n",
    "│   │   │   ├── 2978322154_1_mirror.jpg\n",
    "│   │   │   ├── 2978322154_1_mirror_rect.txt\n",
    "│   │   │   ├── 2978322154_1_mirror_bv33.txt\n",
    "│   │   │   ├── 2978322154_1_mirror_bv70.txt\n",
    "│   │   └── trainset\n",
    "│   │       ├── 100040721_1.jpg\n",
    "│   │       ├── 100040721_1_rect.txt\n",
    "│   │       ├── 100040721_1_bv33.txt\n",
    "│   │       ├── 100040721_1_bv70.txt\n",
    "│   │       ├── 100040721_1_mirror.jpg\n",
    "│   │       ├── 100040721_1_mirror_rect.txt\n",
    "│   │       ├── 100040721_1_mirror_bv33.txt\n",
    "│   │       ├── 100040721_1_mirror_bv70.txt\n",
    "│   ├── ibug\n",
    "│   │   ├── image_014_01.jpg\n",
    "│   │   ├── image_014_01_rect.txt\n",
    "│   │   ├── image_014_01_bv33.txt\n",
    "│   │   ├── image_014_01_bv70.txt\n",
    "│   │   ├── image_014_01_mirror.jpg\n",
    "│   │   ├── image_014_01_mirror_rect.txt\n",
    "│   │   ├── image_014_01_mirror_bv33.txt\n",
    "│   │   ├── image_014_01_mirror_bv70.txt\n",
    "│   └── lfpw\n",
    "│       ├── testset\n",
    "│       │   ├── image_0001.png\n",
    "│       │   ├── image_0001_rect.txt\n",
    "│       │   ├── image_0001_bv33.txt\n",
    "│       │   ├── image_0001_bv70.txt\n",
    "│       │   ├── image_0001_mirror.jpg\n",
    "│       │   ├── image_0001_mirror_rect.txt\n",
    "│       │   ├── image_0001_mirror_bv33.txt\n",
    "│       │   ├── image_0001_mirror_bv70.txt\n",
    "│       └── trainset\n",
    "│           ├── image_0001.png\n",
    "│           ├── image_0001_rect.txt\n",
    "│           ├── image_0001_bv33.txt\n",
    "│           ├── image_0001_bv70.txt\n",
    "│           ├── image_0001_mirror.jpg\n",
    "│           ├── image_0001_mirror_rect.txt\n",
    "│           ├── image_0001_mirror_bv33.txt\n",
    "│           ├── image_0001_mirror_bv70.txt\n",
    "\n",
    "11 directories, 30708 files\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Visualize Annotations</font>\n",
    "\n",
    "There are a few occasions when you would want to visualize annotations as a sanity check before starting training. For example, if you add new data to this dataset, you should sure the ordering of points in the new dataset is the same as the old dataset. You should also visualize the annotations if you decide to build a smaller model with fewer points for a mobile application. \n",
    "\n",
    "To visualize annotations, we will run script **drawRectLandmarks.py**\n",
    "\n",
    "This script randomly picks 50 files from datasets directory, draws rectangle around face using **file _rect.txt** and prints facial landmark number using **file _bv70.txt**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style = \"color:rgb(8,133,37)\">Python [Visual Annotations] [drawRectLandmarks.py]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this script by providing 2 arguments,\n",
    "path to facial_landmark_data directory\n",
    "number of facial landmark points (70 or 33)\n",
    "\n",
    "`python drawRectLandmarks.py path_to_facial_landmark_data 70`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to create directory, draw rectangle for face bounding box and circle + part number for facial landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directory if it doesn't exist\n",
    "def create_dir(folder):\n",
    "  try:\n",
    "    os.makedirs(folder)\n",
    "  except:\n",
    "    print('{} already exists.'.format(folder))\n",
    "\n",
    "# draw rectangle on image\n",
    "def drawRectangle(im, bbox):\n",
    "  x1, y1, x2, y2 = bbox\n",
    "  cv2.rectangle(im, (x1, y1), (x2, y2), (0, 255, 255), \n",
    "                  thickness=5, lineType=cv2.LINE_8)\n",
    "\n",
    "# draw landmarks on image\n",
    "def drawLandmarks(im, parts):\n",
    "  for i, part in enumerate(parts):\n",
    "    # print shape.num_parts()\n",
    "    px, py = part\n",
    "    # draw circle at each landmark\n",
    "    cv2.circle(im, (px, py), 1, (0, 0, 255), thickness=2, \n",
    "                lineType=cv2.LINE_AA)\n",
    "    # write landmark number at each landmark\n",
    "    cv2.putText(im, str(i+1), (px, py), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    1, (0, 200, 100), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since OpenCV font scale is pretty big, we will scale-up our image so that numbers for facial landmarks are not overlapped over each other. Since it will consume a lot of time to draw face rectangle and landmarks over full data, we will visualize a randomly selected sample of full training data.\n",
    "\n",
    "Read path to facial_landmark_data directory and number of facial landmark points from arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scale so that points can be printed well\n",
    "scale = 4\n",
    "# we will draw facial rectangles and landmarks on\n",
    "# a randomly sampled small subset of all images\n",
    "numSamples = 50\n",
    "\n",
    "# facial landmark data directory\n",
    "fldDir = sys.argv[1]\n",
    "# number of facial landmarks; pass 70 or 33\n",
    "numPoints = sys.argv[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare directories to store output images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output dirs\n",
    "# as we know we have a mirrored image corresponding\n",
    "# to each image. We will results for mirrored and original\n",
    "# images in separate directory. Although this is not\n",
    "# important for you because we already have annotation\n",
    "# files for mirrored images.\n",
    "#\n",
    "# This step was crucial, when we got eye-centers annotated\n",
    "# by data team. Data team only annotated eye-centers for\n",
    "# original images. Then we generated annotation files\n",
    "# for mirrored images from annotation files of original images\n",
    "outputDir = os.path.join(fldDir, 'output')\n",
    "outputMirrorDir = os.path.join(outputDir, 'mirror')\n",
    "outputOriginalDir = os.path.join(outputDir, 'original')\n",
    "create_dir(outputMirrorDir)\n",
    "create_dir(outputOriginalDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read image names from image_names.txt file and sample numSamples items from full image names list. We will visualize annotations on this sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to image_names file\n",
    "imageNamesFilepath = os.path.join(fldDir, 'image_names.txt')\n",
    "\n",
    "# Check whether path to image_names file exists\n",
    "# within facial_landmark_data directory\n",
    "if os.path.exists(imageNamesFilepath):\n",
    "  # If image_names.txt exists, read it\n",
    "  with open(imageNamesFilepath) as d:\n",
    "    imageNames = [x.strip() for x in d.readlines()]\n",
    "else:\n",
    "  print('Pass path to facial_landmark_data as argument to this script')\n",
    "\n",
    "\n",
    "\n",
    "# Randomly shuffle list cntaining image names\n",
    "random.shuffle(imageNames)\n",
    "# select numSamples image names from list\n",
    "imageNamesSampled = imageNames[:numSamples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all images. Read and upscale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over image names\n",
    "for k, imageName in enumerate(imageNamesSampled):\n",
    "  print(\"Processing file: {}\".format(imageName))\n",
    "\n",
    "  # create image path\n",
    "  imagePath = os.path.join(fldDir, imageName)\n",
    "  # read image\n",
    "  im = cv2.imread(imagePath, cv2.IMREAD_COLOR)\n",
    "  # scale up image\n",
    "  im = cv2.resize(im, (0, 0), fx=scale, fy=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read rectangle file corresponding to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # create path to face rectangle file\n",
    "  rectPath = os.path.splitext(imagePath)[0] + '_rect.txt'\n",
    "\n",
    "  # open rectangle file and read\n",
    "  with open(rectPath) as f:\n",
    "    line = f.readline()\n",
    "    # read annotations\n",
    "    left, top, width, height = [float(n) for n in line.strip().split()]\n",
    "    # calculate coordinates of bottom right corner of rectangle\n",
    "    right = left + width\n",
    "    bottom = top + height\n",
    "    # scale up face reactangle coordinates\n",
    "    x1, y1, x2, y2 = int(scale*left), int(scale*top), \n",
    "                        int(scale*right), int(scale*bottom)\n",
    "    # save coordinates to a list. this is also called bounding box\n",
    "    # it is a term to denote coordinates of an object\n",
    "    bbox = [x1, y1, x2, y2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read facial landmark points file corresponding to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # open facial landmarks file and read coordinates\n",
    "  pointsPath = os.path.splitext(imagePath)[0] + '_bv' + \n",
    "                  str(numPoints) + '.txt'\n",
    "  parts = []\n",
    "  # open points file and read\n",
    "  with open(pointsPath) as g:\n",
    "    # read lines. each line has coordinates of a landmark point\n",
    "    lines = [x.strip() for x in g.readlines()]\n",
    "    # iterate over all lines\n",
    "    for line in lines:\n",
    "      # each line has two numbers (x, y of each landmark)\n",
    "      left, right = [float(n) for n in line.split()]\n",
    "      # scale up landmark coordinates\n",
    "      px, py = int(scale*left), int(scale*right)\n",
    "      parts.append([px, py])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally draw rectangle and facial landmarks points on image and write it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # draw face rectangle and landmarks\n",
    "  drawRectangle(im, bbox)\n",
    "  drawLandmarks(im, parts)\n",
    "\n",
    "  # basename is filename in a filepath\n",
    "  imageBasename = os.path.basename(imagePath)\n",
    "  # if basename has mirror, output image will be stored\n",
    "  # in mirror output directory\n",
    "  # else in original outout directory\n",
    "  if 'mirror' in imageBasename:\n",
    "    outputImagePath = os.path.join(outputMirrorDir, imageBasename)\n",
    "  else:\n",
    "    outputImagePath = os.path.join(outputOriginalDir, imageBasename)\n",
    "\n",
    "  # save output image\n",
    "  cv2.imwrite(outputImagePath, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Train-Test Data Preparation</font>\n",
    "\n",
    "Dlib’s shape predictor takes XML files in a specific format as input to train and test the landmark detector. To generate these train and test xml files from our data, we will run script **createTrainTestXml.py**\n",
    "\n",
    "This script will generate two xml files named \"<font style=\"color:rgb(8,133,37)\">training_with_face_landmarks.xml</font>\" and “<font style=\"color:rgb(8,133,37)\">testing_with_face_landmarks.xml</font>” in facial_landmark_data folder.\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Python [Create train and test XML files] [createTrainTestXml.py]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this script by providing 2 arguments,\n",
    "\n",
    "+ path to facial_landmark_data directory\n",
    "+ number of facial landmark points (70 or 33)\n",
    "\n",
    "python createTrainTestXml.py path_to_facial_landmark_data 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "try:\n",
    "  from lxml import etree as ET\n",
    "except ImportError:\n",
    "  print('install lxml using pip')\n",
    "  print('pip install lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create an XML document and store annotations for face rectangle and facial landmarks in this document. Structure of this XML is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<dataset>\n",
    "  <name>Training faces</name>\n",
    "  <images>\n",
    "    <image file=\"datasets/helen/testset/3048914345_1.jpg\">\n",
    "      <box height=\"772\" left=\"567\" top=\"481\" width=\"771\">\n",
    "        <part name=\"00\" x=\"744\" y=\"809\"/>\n",
    "        <part name=\"01\" x=\"739\" y=\"870\"/>\n",
    "        <part name=\"02\" x=\"735\" y=\"941\"/>\n",
    "        <part name=\"03\" x=\"746\" y=\"1018\"/>\n",
    "        <part name=\"04\" x=\"763\" y=\"1092\"/>\n",
    "        ...\n",
    "        Similarly for all 70 landmark points\n",
    "      </box>\n",
    "    </image>\n",
    "    <image file=\"datasets/afw/281972218_1.jpg\">\n",
    "      <box height=\"259\" left=\"878\" top=\"247\" width=\"259\">\n",
    "    ...\n",
    "    Similarly for rest of the images\n",
    "  </images>\n",
    "</dataset>\n",
    "```\n",
    "Create root node \"dataset\" and child nodes \"name\" & \"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create XML from annotations\n",
    "def createXml(imageNames, xmlName, numPoints):\n",
    "  # create a root node names dataset\n",
    "  dataset = ET.Element('dataset')\n",
    "  # create a child node \"name\" within root node \"dataset\"\n",
    "  ET.SubElement(dataset, \"name\").text = \"Training Faces\"\n",
    "  # create another child node \"images\" within root node \"dataset\"\n",
    "  images = ET.SubElement(dataset, \"images\")\n",
    "\n",
    "  # print information about xml filename and total files\n",
    "  numFiles = len(imageNames)\n",
    "  print('{0} : {1} files'.format(xmlName, numFiles))\n",
    "  # iterate over all files\n",
    "  for k, imageName in enumerate(imageNames):\n",
    "    # print progress about files being read\n",
    "    print('{}:{} - {}'.format(k+1, numFiles, imageName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read rectangle coordinates and create \"image\" and \"box\" nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # read rectangle file corresponding to image\n",
    "    rect_name = os.path.splitext(imageName)[0] + '_rect.txt'\n",
    "    with open(os.path.join(fldDatadir, rect_name), 'r') as file:\n",
    "      rect = file.readline()\n",
    "    rect = rect.split()\n",
    "    left, top, width, height = rect[0:4]\n",
    "\n",
    "    # create a child node \"image\" within node \"images\"\n",
    "    # this node will have annotation data for an image\n",
    "    image = ET.SubElement(images, \"image\", file=imageName)\n",
    "    # create a child node \"box\" within node \"image\"\n",
    "    # this node has values for bounding box or rectangle of face\n",
    "    box = ET.SubElement(image, 'box', top=top, left=left, \n",
    "                        width=width, height=height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read facial landmark points, create \"parts\" node and store point landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # read points file corresponding to image\n",
    "    points_name = os.path.splitext(imageName)[0] + '_bv' + \n",
    "                                    numPoints + '.txt'\n",
    "    with open(os.path.join(fldDatadir, points_name), 'r') as file:\n",
    "      for i, point in enumerate(file):\n",
    "        x, y = point.split()\n",
    "        # points annotation file has coordinates in float\n",
    "        # but we want them to be in int format\n",
    "        x = str(int(float(x)))\n",
    "        y = str(int(float(y)))\n",
    "        # name is the facial landmark or point number, starting from 0\n",
    "        name = str(i).zfill(2)\n",
    "        # create a child node \"parts\" within node \"box\"\n",
    "        # this node has values for facial landmarks\n",
    "        ET.SubElement(box, 'part', name=name, x=x, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create XML tree and write it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # finally create an XML tree\n",
    "  tree = ET.ElementTree(dataset)\n",
    "\n",
    "  print('writing on disk: {}'.format(xmlName))\n",
    "  # write XML file to disk. pretty_print=True indents the XML \n",
    "  # to enhance readability\n",
    "  tree.write(xmlName, pretty_print=True, xml_declaration=True, \n",
    "              encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read image paths (which are relative to facial landmark data directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "  # read value to facial_landmark_data directory\n",
    "  # and number of facial landmarks\n",
    "  fldDatadir = sys.argv[1]\n",
    "  numPoints = sys.argv[2]\n",
    "\n",
    "  # Read names of all images\n",
    "  with open(os.path.join(fldDatadir, 'image_names.txt')) as d:\n",
    "    imageNames = [x.strip() for x in d.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is relevant if you don’t have sufficient RAM on your machine to train model on whole training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  ################# trick to use less data #################\n",
    "  # If you are unable to train all images on your machine,\n",
    "  # you can reduce training data by randomly sampling n \n",
    "  # images from the total list.\n",
    "  # Keep decreasing the value of n from len(imageNames) to\n",
    "  # a value which works on your machine.\n",
    "  # Uncomment the next two lines to decrease training data\n",
    "  # n = 1000\n",
    "  # imageNames = random.sample(imageNames, n)\n",
    "  ##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split train and test data in a 95:5 ratio. Generate and save training and test XML files. These XML files will be stored in facial_landmark_data folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  totalNumFiles = len(imageNames)\n",
    "  # We will split data into 95:5 for train and test\n",
    "  numTestFiles = int(0.05 * totalNumFiles)\n",
    "\n",
    "  # randomly sample 5% items from list of image names\n",
    "  testFiles = random.sample(imageNames, numTestFiles)\n",
    "  # assign rest of image names as train\n",
    "  trainFiles = list(set(imageNames) - set(testFiles))\n",
    "\n",
    "  # generate XML files for train and test data\n",
    "  createXml(trainFiles, os.path.join(fldDatadir,\n",
    "              'training_with_face_landmarks.xml'), numPoints)\n",
    "  createXml(testFiles, os.path.join(fldDatadir, \n",
    "              'testing_with_face_landmarks.xml'), numPoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Training</font>\n",
    "\n",
    "We will train shape predictor on landmark data either using <font style=\"color:rgb(8,133,37)\">C++ code **trainFLD.cpp**</font> or <font style=\"color:rgb(8,133,37)\">Python code **trainFLD.py.**</font>\n",
    "\n",
    "Both of these code look for <font style=\"color:rgb(8,133,37)\">training_with_face_landmarks.xml</font> and <font style=\"color:rgb(8,133,37)\">testing_with_face_landmarks.xml</font> files in facial_landmark_data directory. Once you run the training code, it will tell you the approximate time required to train the landmark detector. \n",
    "\n",
    "After the training is complete, the facial landmark model is saved to disk and the model is tested on the training and the test data to calculate training and test errors.\n",
    "\n",
    "We discussed in the theory section that this landmark detection algorithm is based on V. Kazemi’s paper \"One Millisecond Face Alignment with an Ensemble of Regression Trees\". This algorithm has few parameters which affect model size and accuracy. \n",
    "\n",
    "Let’s go through those parameters. Values specified in brackets is the default value used in Dlib’s shape_predctor_trainer class. Some of the text below is copied from Dlib’s documentation.\n",
    "\n",
    ">**cascade_depth** (10): &nbsp; Number of cascades created when you train a model. This parameter corresponds to the parameter T in the Kazemi paper.\n",
    ">\n",
    "> **num_trees_per_cascade_level** (500): &nbsp; Number of trees created for each cascade. This means that the total number of trees in the learned model is equal to cascade_depth x num_trees_per_cascade_level. This parameter corresponds to the parameter K in the Kazemi paper. \n",
    ">\n",
    "> **tree_depth** (4): &nbsp; Depth of the trees used in the cascade. In particular, there are pow(2,get_tree_depth()) leaves in each tree.This parameter corresponds to the parameter F in the Kazemi paper. \n",
    "> \n",
    ">**nu** (0.1): &nbsp;nu is the regularization parameter. Larger values of this parameter will cause the algorithm to fit the training data better but may also cause overfitting. This parameter corresponds to the parameter  in the Kazemi paper. \n",
    ">\n",
    "> **oversampling_amount** (20): &nbsp; We can reduce the capacity of the model by explicitly increasing the regularization (making nu smaller) and by using trees with smaller depths. We can effectively increase the amount of training data by adding in each training example multiple times but with a randomly selected deformation applied to it. That is what this parameter controls. i.e. if you supply n training samples to train then the algorithm runs internally with n x oversampling_amount training samples. The bigger this parameter the better (excepting that larger values make training take longer). In terms of the Kazemi paper, this parameter is the number of randomly selected initial starting points sampled for each training example. This parameter corresponds to the parameter R in the Kazemi paper. \n",
    ">\n",
    "> **feature_pool_size** (400): &nbsp; At each level of the cascade we randomly sample feature_pool_size pixels from the image.  These pixels are used to generate features for the random trees.  So in general larger settings of this parameter give better accuracy but make the algorithm run slower. This parameter corresponds to the parameter P in the Kazemi paper. \n",
    ">\n",
    "> **feature_pool_region_padding** (0): &nbsp;When we randomly sample the pixels for the feature pool we do so in a box fit around the provided training landmarks. By default, feature_pool_region_padding=0 and the box is the tightest box that contains the landmarks. However, we can expand or shrink the size of the pixel sampling region by setting a different value of feature_pool_region_padding. To explain this precisely, for a padding of 0 we say that the pixels are sampled from a box of size 1x1.  The padding value is added to each side of the box.  So a padding of 0.5 would cause the algorithm to sample pixels from a box that was 2x2, effectively multiplying the area pixels are sampled from by 4.  Similarly, setting the padding to -0.2 would cause it to sample from a box 0.6x0.6 in size.\n",
    "> \n",
    ">\n",
    "> **lambda_param** (0.1): &nbsp;To decide how to split nodes in the regression trees the algorithm looks at the intensity difference between pairs of pixels in the image. These pixel pairs are randomly sampled but with a preference for selecting pixels that are closer to each other. lambda controls this \"nearness\" preference.  In particular, smaller values of lambda_param will make the algorithm prefer pixels close together and larger values of lambda will make it care less about picking nearby pixel pairs. lambda_param should be in a number between 0 and 1. \n",
    "> \n",
    ">\n",
    "> **num_test_splits** (20): &nbsp;When generating the random trees we randomly sample num_test_splits possible split features at each node and pick the one that gives the best split. Larger values of this parameter will usually give more accurate outputs but take longer to train.This parameter corresponds to the parameter S in the Kazemi paper. \n",
    ">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style = \"color:rgb(8,133,37)\">Python [Train Facial Landmark Detector] [trainFLD.py]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start training using following command:\n",
    "\n",
    "`python trainFLD.py path_to_facial_landmark_data 70`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read facial_landmark_data and number of facial landmark points from command arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fldDatadir = sys.argv[1]\n",
    "numPoints = sys.argv[2]\n",
    "modelName = 'shape_predictor_' + numPoints + '_face_landmarks.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters of shape_predictor_trainer\n",
    "options = dlib.shape_predictor_training_options()\n",
    "options.cascade_depth = 10\n",
    "options.num_trees_per_cascade_level = 500\n",
    "options.tree_depth = 4\n",
    "options.nu = 0.1\n",
    "options.oversampling_amount = 20\n",
    "options.feature_pool_size = 400\n",
    "options.feature_pool_region_padding = 0\n",
    "options.lambda_param = 0.1\n",
    "options.num_test_splits = 20\n",
    "\n",
    "# Tell the trainer to print status messages to the console so we can\n",
    "# see training options and how long the training will take.\n",
    "options.be_verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if train and test XML files are present in facial_landmark_data folder\n",
    "trainingXmlPath = os.path.join(fldDatadir, \n",
    "                                \"training_with_face_landmarks.xml\")\n",
    "testingXmlPath = os.path.join(fldDatadir, \n",
    "                                \"testing_with_face_landmarks.xml\")\n",
    "outputModelPath = os.path.join(fldDatadir, modelName)\n",
    "\n",
    "# check whether path to XML files is correct\n",
    "if os.path.exists(trainingXmlPath) and os.path.exists(testingXmlPath):\n",
    "  # Train and test the model\n",
    "  # dlib.train_shape_predictor() does the actual training. \n",
    "  # It will save the final predictor to predictor.dat.\n",
    "  # The input is an XML file that lists the images in \n",
    "  # the training dataset and \n",
    "  # also contains the positions of the face parts.\n",
    "  dlib.train_shape_predictor(trainingXmlPath, outputModelPath, options)\n",
    "\n",
    "  # Now that we have a model we can test it.  \n",
    "  # dlib.test_shape_predictor() measures the average distance \n",
    "  # between a face landmark output by the shape_predictor and \n",
    "  # ground truth data.\n",
    "\n",
    "  print(\"\\nTraining error: {}\".format(\n",
    "    dlib.test_shape_predictor(trainingXmlPath, outputModelPath)))\n",
    "\n",
    "  # The real test is to see how well it does \n",
    "  # on data it wasn't trained on.\n",
    "  print(\"Testing error: {}\".format(\n",
    "    dlib.test_shape_predictor(testingXmlPath, outputModelPath)))\n",
    "# Print an error message if XML files are not \n",
    "# present in facial_landmark_data folder\n",
    "else:\n",
    "  print('training and test XML files not found.')\n",
    "  print('Please check paths:')\n",
    "  print('train: {}'.format(trainingXmlPath))\n",
    "  print('test: {}'.format(testingXmlPath))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">Train 33-points Facial Landmarks</font>\n",
    "\n",
    "The 70 points facial landmark is huge in size and not appropriate for use in mobile applications. So we will create a smaller model. There is a tradeoff between model size and accuracy. It is important to pick right subset of points out of 70. After multiple experiments, we figured out a subset of 33 points which doesn’t compromise as much accuracy and is smaller in size.\n",
    "\n",
    "<center> <a href=\"https://www.learnopencv.com/wp-content/uploads/2017/12/opcv4face-w3-m6-33points.jpg\"><img src = \"https://www.learnopencv.com/wp-content/uploads/2017/12/opcv4face-w3-m6-33points.jpg\" width=500/></a></center>\n",
    "\n",
    "## <font style = \"color:rgb(50,120,229)\">Data</font>\n",
    "\n",
    "We will read _bv70.txt files as input, pick these 33 landmarks and save them as _bv33.txt for each image. Then use createTrainTestXml.py script to generate training_with_face_landmarks.xml and testing_with_face_landmarks.xml files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style = \"color:rgb(8,133,37)\">Python [Generate Landmark Annotation files for 33 points] [generate33Points.py]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script should be run as:\n",
    "\n",
    "`python generate33Points.py path_to_facial_landmark_data`\n",
    "\n",
    "facial_landmark_data folder should have files image_names.txt and a folder named \"datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read file image_names.txt. This file has path to all image files in \"datasets\" directory. We will use this to find/read/write annotation files corresponding to each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facial landmark data directory\n",
    "fldDir = sys.argv[1]\n",
    "\n",
    "# Path to image_names file\n",
    "imageNamesFilepath = os.path.join(fldDir, 'image_names.txt')\n",
    "\n",
    "# Check whether path to image_names file exists within \n",
    "# facial_landmark_data directory\n",
    "if os.path.exists(imageNamesFilepath):\n",
    "  # If image_names.txt exists, read it\n",
    "  with open(imageNamesFilepath) as d:\n",
    "    imageNames = [x.strip() for x in d.readlines()]\n",
    "else:\n",
    "  print('Pass path to facial_landmark_data as argument to this script')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down indices of all 33 points w.r.t. 70 points\n",
    "# Out of 70 points, we have to pick 33 points.\n",
    "# Here we are writing indices of those 33 points.\n",
    "# IMPORTANT: Numbers shown on image are natural numbers. They\n",
    "# start from 1 whereas indices in Python start from 0.\n",
    "# So to match these indices with facial landmark numbers on\n",
    "# sample image, you should add 1.\n",
    "points33Indices = [\n",
    "                   1, 3, 5, 8, 11, 13, 15,     # Jaw line\n",
    "                   17, 19, 21,                 # Left eyebrow\n",
    "                   22, 24, 26,                 # Right eyebrow\n",
    "                   30, 31,                     # Nose bridge\n",
    "                   33, 35,                     # Lower nose\n",
    "                   36, 37, 38, 39, 40, 41,     # Left eye\n",
    "                   42, 43, 44, 45, 46, 47,     # Right Eye\n",
    "                   48, 51, 54, 57              # Outer lip\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over all images. For each image, create path to 70-points annotation file(for reading) and 33-points annotation file(for writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numImages = len(imageNames)\n",
    "\n",
    "# Iterate over all image names\n",
    "for n, imageName in enumerate(imageNames):\n",
    "  # Just pretty printing the progress\n",
    "  print('{}/{} - {}'.format(n+1, numImages, imageName))\n",
    "\n",
    "  # path to image\n",
    "  imagePath = os.path.join(fldDir, imageName)\n",
    "  # We points annotation file has prefix _bv70.txt in end \n",
    "  # whereas image has .jpg. So we are creating the path to 70points\n",
    "  # annotation file by replacing .jpg of image path with _bv70.txt.\n",
    "  points70Path = os.path.splitext(imagePath)[0] + '_bv70.txt'\n",
    "  # Similarly for 30points annotation files.\n",
    "  points33Path = os.path.splitext(imagePath)[0] + '_bv33.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 70-points annotation file, select 33 points using indices that we picked earlier and save these points to a 33-points annotation file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Check if path to annotation file exists\n",
    "  if os.path.exists(points70Path):\n",
    "    # open file\n",
    "    with open(points70Path, 'r') as f:\n",
    "      # read all lines\n",
    "      points70 = f.readlines()\n",
    "      # select lines whose indices are in our points33Indices list\n",
    "      points33 = [points70[i] for i in points33Indices]\n",
    "      # open points33 file\n",
    "      with open(points33Path, 'w') as g:\n",
    "        # write 33 points to file\n",
    "        g.writelines(points33)\n",
    "\n",
    "  else:\n",
    "    print('Unable to find path:{}'.format(points70Path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Visualize Annotations</font>\n",
    "\n",
    "We have generated 33-points annotation files from 70-points files. It is really important to check whether these new annotation files are correct. We will visualize 33-points facial landmarks using script drawRectLandmarks.py. When we pass 33 as argument, script reads _bv33.txt files to load facial landmarks annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style = \"color:rgb(8,133,37)\">Python [Visual Annotations] [drawRectLandmarks.py]</font>\n",
    "\n",
    "Run drawRectLandmarks.py for 33-points:\n",
    "\n",
    "`python drawRectLandmarks.py path_to_facial_landmark_data 33`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Train-Test Data Preparation</font>\n",
    "\n",
    "To generate training and test XML files, we will run script createTrainTextXml. When we pass 33 as argument, script read _bv33.txt files to load facial landmarks annotations. This script will write <font style=\"color:rgb(8,133,37)\">training_with_face_landmarks.xml</font> and <font style=\"color:rgb(8,133,37)\">testing_with_face_landmarks.xml</font> to facial_landmark_data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style = \"color:rgb(8,133,37)\">Python [Create train and test XML files] [createTrainTestXml.py]</font>\n",
    "\n",
    "Run createTrainTestXml.py for 33-points,\n",
    "\n",
    "`python createTrainTestXml.py path_to_facial_landmark_data 33`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Training</font>\n",
    "\n",
    "Training code looks for files <font style=\"color:rgb(8,133,37)\">training_with_face_landmarks.xml</font> and <font style=\"color:rgb(8,133,37)\">testing_with_face_landmarks.xml</font> in facial_landmark_data folder. These files don’t have distinct names i.e. it is same for both 70-points and 33-points. By default facial_landmark_data folder has XML files for 70-points. Before you start training, make sure that you are using the correct files. Last argument (33 or 70) is used to name model file.\n",
    "\n",
    "We can train for 33-points landmark model using Python code.\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Python [Train Facial Landmark Detector] [trainFLD.py]</font>\n",
    "\n",
    "`python trainFLD.py path_to_facial_landmark_data 33`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">References and Further reading</font>\n",
    "\n",
    "1. [http://www.csc.kth.se/~vahidk/face_ert.html](http://www.csc.kth.se/~vahidk/face_ert.html)\n",
    "\n",
    "2. [https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/](https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/)\n",
    "\n",
    "3. [http://dlib.net/ml.html#shape_predictor_trainer](http://dlib.net/ml.html#shape_predictor_trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
