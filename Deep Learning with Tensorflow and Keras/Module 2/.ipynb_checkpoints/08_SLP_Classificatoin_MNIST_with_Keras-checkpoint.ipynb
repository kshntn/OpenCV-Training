{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "a5695ccc"
            },
            "source": [
                "<h1 style=\"font-size:30px;\">MNIST Digit Classification using Keras<\/h1>\n",
                "\n",
                "In this notebook, we will introduce several new concepts associated with the general problem of classification involving more than two classes. This is sometimes referred to as multinomial regression or softmax regression when the number of classes is more than two. Specifically, in this notebook, we will see how to classify handwritten digits from the MNIST database. The MNIST dataset is included in Tensorflow and can easily be imported and loaded, as we will see below. Using this dataset, we will demonstrate how to work with image data that represents the digits $[0,9]$ and how to develop a network architecture that includes ten neurons whose outputs represent the probability of the digits.\n",
                "\n",
                "<img src='https:\/\/learnopencv.com\/wp-content\/uploads\/2022\/01\/c4_02_MNIST_Digits.jpg' width=650 align='center'>\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Table of Contents\n",
                "* [1 Mathematical Foundation](#1-Mathematical-Foundation)\n",
                "* [2 Load the MNIST Dataset](#2-Load-the-MNIST-Dataset)\n",
                "* [3 Prepare the Image Data](#3-Prepare-the-Image-Data)\n",
                "* [4 Model Architecture](#4-Model-Architecture)\n",
                "* [5 Model Evaluation](#5-Model-Evaluation)\n",
                "* [6 Conclusion](#6-Conclusion)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1 Mathematical Foundation\n",
                "\n",
                "First, recall for **binary classification**, we developed a probabilistic interpretation for the output of the **sigmoid** activation function, $y'$, as follows:\n",
                "\n",
                "$$P(y\\ =\\ 1| x;\\theta) = y'$$\n",
                "\n",
                "Here $y'$ represented the probability of class 1 given the input $x$ is associated with class 1. And therefore, the probability that the input sample belongs class 0 (or the negative class) is:\n",
                "\n",
                "$$P(y\\ =\\ 0| x;\\theta) = 1 - y'$$\n",
                "\n",
                "We combined these two expressions into a single equation as shown below, where $y$ represents the ground truth (or label) for the class.\n",
                "\n",
                "$$p(y\\ |\\ x;\\theta) = (y')\\ ^y\\ (1 - y')^{1-y}$$\n",
                "\n",
                "In order to extend this to three or more classes, we are going to introduce the **softmax** activation function in the next section.\n",
                "\n",
                "### 1.1 Softmax Activation Function\n",
                "\n",
                "For classification problems involving more than two classes the target $y$ is a variable that ranges over more than two classes, and we, therefore, want to know the probability of $y$ being in each potential class $i$.\n",
                "\n",
                "$$ p(y\\ =\\ i\\ |\\ x) = y_i'$$\n",
                "\n",
                "As we will see further below, when we have three or more classes, the network architecture will contain an output neuron for each class whose output will be the predicted probability that the input $x$ is associated with a particular class, $i$. We can now use a generalization of the sigmoid activation function called the **softmax** function to compute $p(y\\ =\\ i\\ |\\ x)$. \n",
                "\n",
                "The softmax activation function will map each of the neuron inputs to a probability distribution, where each neuron output is a value in the range $(0, 1)$ with all values summing to 1. Assuming the number of possible classes is $k$, the following equation defines the **softmax** function, and the output for any particular neuron with the input $ z_i = w_i^Tx + b_i $ is defined as follows:\n",
                "\n",
                "$$ softmax(z_i)\\ = \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}} = y_i', \\ \\ 1<= i <= k $$\n",
                "\n",
                "Therefore, each output neuron ($i$) will compute a softmax score according to the above equation. Notice that the numerator is for a given class $i$, and the denominator normalizes each neuron's output into probabilities so that the inputs are mapped to the range $(0,1)$. So each output neuron computes a number in the range $(0,1)$, and the summation of the scores from all neurons is 1. The output of each neuron, $y'_i$, is interpreted as the probability that the input $x$ is associated with class $i$.\n",
                "\n",
                "### 1.2 Cross Entropy Loss Function\n",
                "\n",
                "The loss function used for multinomial regression is known as the **Cross Entropy Loss** function and is defined with the same motivation as binary cross entropy loss. Here, we want to maximize the probability that a given input corresponds to a given class $i$ which is the same as minimizing the negative log of the probability. The loss function for a single example $x$ is the sum of the logs of the $k$ output classes:\n",
                "\n",
                "$$ J(y') = -\\sum_{i=1}^{k} 1\\{y=i\\}\\ log\\ [\\ p(y\\ =\\ i|x)\\ ]$$\n",
                "\n",
                "$$ J(y') = -\\sum_{i=1}^{k} 1\\{y=i\\}\\ log\\ [ \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}}\\ ]$$\n",
                "\n",
                "$$ J(y') = -\\sum_{i=1}^{k} 1\\{y=i\\}\\ log\\ [ y'_i ]$$\n",
                "\n",
                "\n",
                "\n",
                "In the equation above, we make use of the indicator function $1\\{\\}$ which evaluates to 1 if the condition in the brackets is true and to 0 otherwise. So, the total loss is a summation across each of the output neurons. An easy way to get an intuition for why this makes sense is to consider two cases. First, consider the case where the neuron with the highest output is associated with the ground truth label for the input $x$. In that case, the total loss would be the negative log of a high probability number. For example, if the probability was 0.9 then the total loss for this case would be $-log(0.9) = .105$. Now consider a case where the correct class had a predicted output probability of .01. In that case, the total loss would be the negative log of a low probability number, $-log(0.01) = 4.61$.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2d5d8122"
            },
            "source": [
                "## 2 Load the MNIST Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "collapsed": true,
                "id": "c712db14"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import random\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras import layers\n",
                "from tensorflow.keras import Sequential\n",
                "from tensorflow.keras.layers import Dense\n",
                "from tensorflow.keras.datasets import mnist\n",
                "\n",
                "plt.rcParams['axes.titlesize'] = 16\n",
                "plt.rcParams['axes.labelsize'] = 14\n",
                "plt.rcParams['image.cmap'] = 'gray'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "SEED_VALUE = 42\n",
                "\n",
                "# Fix seed to make training deterministic.\n",
                "random.seed(SEED_VALUE)\n",
                "np.random.seed(SEED_VALUE)\n",
                "tf.random.set_seed(SEED_VALUE)\n",
                "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
                "\n",
                "# For GPU.\n",
                "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
                "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <font style=\"color:rgb(50,120,230)\">Load the MNIST dataset and split into train, validation and test<\/font>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "76cd566d",
                "outputId": "0511a613-a101-4e3a-f4e1-da69f48f2df1"
            },
            "outputs": [],
            "source": [
                "(X_train_all, y_train_all), (X_test, y_test) = mnist.load_data()\n",
                "\n",
                "X_val   = X_train_all[:10000]\n",
                "X_train = X_train_all[10000:]\n",
                "y_val   = y_train_all[:10000]\n",
                "y_train = y_train_all[10000:]\n",
                "\n",
                "print(X_train.shape)\n",
                "print(X_val.shape)\n",
                "print(X_test.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.imshow(X_train[0]);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "6bfe41ae"
            },
            "source": [
                "## 3 Prepare the Image Data\n",
                "\n",
                "We're now going to pre-process the image data by reshaping it from a 2D [28x28] array into a 1D [784x1] array. We will also normalize the intensity values to the range $[0, 1]$. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "collapsed": true,
                "id": "925cbd53"
            },
            "outputs": [],
            "source": [
                "X_train = X_train.reshape((X_train.shape[0], 28 * 28))\n",
                "X_train = X_train.astype(\"float32\") \/ 255\n",
                "\n",
                "X_test = X_test.reshape((X_test.shape[0], 28 * 28))\n",
                "X_test = X_test.astype(\"float32\") \/ 255\n",
                "\n",
                "X_val = X_val.reshape((X_val.shape[0], 28 * 28))\n",
                "X_val = X_val.astype(\"float32\") \/ 255"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "7d4e2d28"
            },
            "source": [
                "## 4 Model Architecture \n",
                "\n",
                "### 4.1 Single Layer, Multiple Output Architecture\n",
                "\n",
                "The network architecture shown below is similar to the previous architecture for binary classification but with some important differences. The key differences are summarized below:\n",
                "\n",
                "1. The image input data is pre-processed in a way that we have not yet discussed (more on this below).\n",
                "2. We now have 10 neurons to represent the ten different classes (digits: 0 to 9), instead of a single neuron as with binary classification. \n",
                "3. The activation function is a **softmax** activation rather than a sigmoid activation.\n",
                "4. The loss function is now **sparse categorical cross entropy**.\n",
                "\n",
                "Although the diagram looks quite a bit different from previous (single neuron) architectures, it is fundamentally very similar in terms of the processing that takes place during training and prediction.\n",
                "\n",
                "\n",
                "<img src='https:\/\/learnopencv.com\/wp-content\/uploads\/2022\/12\/c4_02_MNIST_network_updated.png' width=900 align='center'><br\/>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Input Feature Transformation\n",
                "\n",
                "Since we are now working with an image as the input, we need to find some logical way to represent the image data as a set of features. A naive approach that actually works quite well is to just assume that the pixel intensities **are** the features. And one way to transform the image data into a set of features that we can process is to flatten the 2D array into a 1D array. The 28x28 input image thus becomes a 1D array containing 784 features. Notice that we also normalize the pixel intensities to be in the range $[0, 1]$. This is very common when working with image data. This normalization that we perform manually there preclude the need to define a normalization layer in Keras.\n",
                "\n",
                "Also, just to be clear, using the pixel intensities as features is a naive approach that we are using intentionally here to keep things simple. As we will see in Module 3, we will learn about convolutional neural networks (CNNs), which use more advanced techniques for representing and processing image data in neural networks.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 Fully Connected (Dense) Layers\n",
                "\n",
                "The neural network architectures we have covered thus far in the course have used \"fully connected\" layers which are also referred to as \"dense\" layers. This is very common, but as the number of inputs and neurons in each layer becomes larger, the number of trainable parameters grows exponentially. The figure below shows two examples of fully connected layers. When depicting neural network architectures with fully connected layers, the connections are typically omitted with the understanding that 'dense' or 'fully connected' is assumed.\n",
                "\n",
                "<img src='https:\/\/learnopencv.com\/wp-content\/uploads\/2022\/01\/c4_02_dense_layers.png' width=700 align='center'>\n",
                "\n",
                "As we will see later in the course, when working with images, the number of parameters can become exceedingly large as the number of neurons and the number of layers in the network is increased. For example, it is not uncommon for state-of-the-art networks to contain millions of parameters. Larger networks hold the potential to exceed the performance of smaller networks, but that comes at the cost of much longer training times. In order to mitigate these issues, we will see that the data in the network is sometimes down-sampled at intermediate layers, which reduces the number of parameters. One approach that is used to down-sample data in CNNs is called 'pooling.' Another approach called \"dropout\", is a stochastic regularization technique that is used to reduce overfitting by randomly dropping a percentage of neurons from the network (along with their associated connections) which also reduces the number of trainable parameters in the network. We will cover these topics in more detail later in the course."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "6202e88e"
            },
            "source": [
                "### 4.4 Model Architecture Definition"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "id": "06ac2797",
                "outputId": "9501c149-fa3c-4530-9869-6278645ce6d2"
            },
            "outputs": [],
            "source": [
                "# Instantiate the model.\n",
                "model = tf.keras.Sequential()\n",
                "\n",
                "# Add the single neuron.\n",
                "model.add(Dense(10, input_shape=(784,), activation=\"softmax\"))\n",
                "\n",
                "# Display the model summary.\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "b0348e25"
            },
            "source": [
                "### 4.5 Compile the Model\n",
                "\n",
                "This step defines the optimizer and the loss function that will be used in the training loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "collapsed": true,
                "id": "cd922b3b"
            },
            "outputs": [],
            "source": [
                "model.compile(optimizer=\"adam\",\n",
                "              loss=\"sparse_categorical_crossentropy\",\n",
                "              metrics=[\"accuracy\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "6ed49f59"
            },
            "source": [
                "### 4.6 Train the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "collapsed": true,
                "id": "aacbcb95"
            },
            "outputs": [],
            "source": [
                "history = model.fit(X_train, \n",
                "                    y_train, \n",
                "                    epochs=50, \n",
                "                    batch_size=128, \n",
                "                    verbose=False,\n",
                "                    validation_data=(X_val, y_val));"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "id": "b42d522a",
                "outputId": "5cbab0af-031f-4f0f-ee55-f146a3b94684",
                "scrolled": false
            },
            "outputs": [],
            "source": [
                "history_dict = history.history\n",
                "loss_values = history_dict[\"loss\"]\n",
                "val_loss_values = history_dict[\"val_loss\"]\n",
                "epochs = range(1, len(loss_values) + 1)\n",
                "\n",
                "plt.figure(figsize=[15, 5])\n",
                "plt.plot(epochs, loss_values, \"g\", label=\"Training loss\")\n",
                "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
                "plt.title(\"Training and validation loss\")\n",
                "plt.xlabel(\"Epochs\")\n",
                "plt.ylabel(\"Loss\")\n",
                "plt.xlim(0, 50)\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()\n",
                "\n",
                "plt.figure(figsize=[15, 5])\n",
                "acc = history_dict[\"accuracy\"]\n",
                "val_acc = history_dict[\"val_accuracy\"]\n",
                "plt.plot(epochs, acc, \"g\", label=\"Training acc\")\n",
                "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
                "plt.title(\"Training and validation accuracy\")\n",
                "plt.xlabel(\"Epochs\")\n",
                "plt.ylabel(\"Accuracy\")\n",
                "plt.xlim(0, 50)\n",
                "plt.ylim(.8, 1.0)\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.7 Saving and Loading Models\n",
                "Saving and loading models is very convenient. This enables you to develop and train a model, save it to the file system and then load at some future time for use.  In this section we will cover the basic operations for saving and loading models. \n",
                "\n",
                "### <font style=\"color:rgb(50,120,230)\">Saving models<\/font>\n",
                "\n",
                "You can easily save a model using the `save()` method which will save the model to the file system in the 'SavedModel' format. This method creates a folder on the file system. Within this folder, the model architecture and training configuration (including the optimizer, losses, and metrics) are stored in `saved_model.pb`. The 'variables\/' folder  contains a standard training checkpoint that includes the weights of the model. We will delve into these details in later modules. For now, let's simply save the trained model and then we'll load it in the next code cell with a different name and continue using it in the remainder of the notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "# Using the save() method, the model will be saved to the file system in the 'SavedModel' format.\n",
                "model.save('MNIST_classifer_model')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### <font style=\"color:rgb(50,120,230)\">Loading models<\/font>\n",
                "Let's now load the model and continue using it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "from tensorflow.keras import models\n",
                "MNIST_classifer_model = models.load_model('MNIST_classifer_model')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2f27ef96"
            },
            "source": [
                "## 5 Model Evaluation\n",
                "\n",
                "### 5.1 Make Predictions on Sample Test Images\n",
                "We can now predict the results for all the test images, as shown in the code below. Here, we call the predict() method to retrieve all the predictions, and then we select a specific index from the test set and print out the predicted scores for each class. You can experiment with the code below by setting the test index to various values and see how the highest score is usually associated with the correct value indicated by the ground truth."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "id": "9f477add",
                "outputId": "e6813308-8149-4838-cb69-d08edbbbbe35"
            },
            "outputs": [],
            "source": [
                "predictions = MNIST_classifer_model.predict(X_test)\n",
                "index = 0 # up to 9999\n",
                "print('Ground truth for test digit: ',y_test[index])\n",
                "print('\\n');\n",
                "print('Predictions for each class:\\n')\n",
                "for i in range(10):\n",
                "    print('digit:', i, ' probability: ', predictions[index][i])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "dcc4e3e8"
            },
            "source": [
                "### 5.2 Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "collapsed": true,
                "id": "145de872"
            },
            "outputs": [],
            "source": [
                "predictions = MNIST_classifer_model.predict(X_test)\n",
                "predicted_labels = [np.argmax(i) for i in predictions]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "id": "74c5b9a3",
                "outputId": "a99e6cd4-5885-41d8-c8b4-d22bb3f9c877"
            },
            "outputs": [],
            "source": [
                "cm = tf.math.confusion_matrix(labels=y_test, predictions=predicted_labels)\n",
                "\n",
                "plt.figure(figsize=[15, 8])\n",
                "import seaborn as sn\n",
                "sn.heatmap(cm, annot=True, fmt='d', annot_kws={\"size\": 14})\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('Truth')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import confusion_matrix\n",
                "\n",
                "cm = confusion_matrix(y_true=y_test, y_pred=predicted_labels, normalize='true')\n",
                "\n",
                "plt.figure(figsize=[15, 8])\n",
                "import seaborn as sn\n",
                "sn.heatmap(cm, annot=True, annot_kws={\"size\": 14})\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('Truth')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ba5a6a81"
            },
            "source": [
                "### 5.3 Model Diagnostics: Interpretation of the Weights\n",
                "\n",
                "We can gain some additional insight as to why the trained weights make sense for detecting each of the nine digits by re-shaping the model weights back into a two-dimensional array. In the plots below, we're displaying the weights for three different neurons corresponding to digits 0, 1 and 2. The dot product between the input image and the weights (plus a bias term) for each of the neurons produces a number which is then converted to a probability via the softmax function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "id": "8c9ccae4",
                "outputId": "0829e60b-a968-4a1e-ad3b-6ffcb446f254",
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "weights = MNIST_classifer_model.layers[0].get_weights()[0]\n",
                "bias    = MNIST_classifer_model.layers[0].get_weights()[1]\n",
                "\n",
                "weights_0 = weights[:,0] + bias[0]\n",
                "weights_0 = np.reshape(weights_0, (28,28))\n",
                "\n",
                "weights_1 = weights[:,1] + bias[1]\n",
                "weights_1 = np.reshape(weights_1, (28,28))\n",
                "\n",
                "weights_2 = weights[:,2] + bias[2]\n",
                "weights_2 = np.reshape(weights_2, (28,28))\n",
                "\n",
                "plt.figure(figsize=(20, 7))\n",
                "plt.subplot(131); plt.title('Weights for Digit: 0'); plt.imshow(weights_0)\n",
                "plt.subplot(132); plt.title('Weights for Digit: 1'); plt.imshow(weights_1)\n",
                "plt.subplot(133); plt.title('Weights for Digit: 2'); plt.imshow(weights_2);"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(17,6))\n",
                "input_0 = np.reshape(X_test[3], (28,28))\n",
                "plt.subplot(131); plt.title('Input Digit: 0'); plt.imshow(input_0);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "f5fb8a09"
            },
            "source": [
                "### 6 Conclusions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "1007adba"
            },
            "source": [
                "In this notebook, we introduced multinomial classification, which is an extension to binary classification that uses the softmax activation function. We also introduced one (simple) approach for how to preprocess image data for use in a neural network. In the next module we will learn about Convolutional Neural Networks (CNNs) that are specifically designed to process image data. The following link contains a really nice interactive web-based animation of several well known CNN architectures, which is a great place to start getting familiar with them."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "https:\/\/tensorspace.org\/html\/playground\/lenet.html"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "colab": {
            "name": "week_02_15_SLP_Classificatoin_MNIST_with_Keras.ipynb",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3.8",
            "language": "python",
            "name": "python38"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text\/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.7"
        },
        "latex_envs": {
            "LaTeX_envs_menu_present": true,
            "autoclose": false,
            "autocomplete": true,
            "bibliofile": "biblio.bib",
            "cite_by": "apalike",
            "current_citInitial": 1,
            "eqLabelWithNumbers": true,
            "eqNumInitial": 1,
            "hotkeys": {
                "equation": "Ctrl-E",
                "itemize": "Ctrl-I"
            },
            "labels_anchors": false,
            "latex_user_defs": false,
            "report_style_numbering": false,
            "user_envs_cfg": false
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": [],
            "number_sections": false,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": [],
            "toc_section_display": true,
            "toc_window_display": false
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}