{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":72335,"databundleVersionId":7925022,"sourceType":"competition"}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font style=\"color:blue\">Project 1 - Part 1: Data Understanding & Pipeline Check</font>\n\n\nHere, we will slightly modify the steps that we used earlier to train Neural Networks:\n\n* Step 1 - Understand your problem\n* Step 2 - Dataset Exploration\n    * Step 2A - Get the data\n    * Step 2B - Explore & understand your data\n    * Step 2C - Create a sample data from the dataset\n* Step 3 - Data preparation\n* Step 4 - Train a simple model on sample data, and check the pipeline before training the full network\n* Step 5 - Train on full data\n* Step 6 - Improve your model\n* Step 7 - Generate Submission file\n\n\nIn this notebook, we will detail the steps 1 to 4, and do some coding along the way! You will implement Steps 5, 6 & 7 in the next notebook.","metadata":{}},{"cell_type":"markdown","source":"As of date `11/03/2024` there's an issue with the TensorFlow package that comes pre-installed with the Kaggle kernels.\nTo rectify it, execute `!pip install tensorflow==2.15.0.post1`. \n\nOnce installed, click the `Restart & Clear Cell Outputs` button in the **More settings** (kebab) menu on the right side of the `Factory reset` button.","metadata":{}},{"cell_type":"markdown","source":"This notebook carries **`25 points`** out of a total **`100`**. <br> \n<font style=\"color:red\">The sections in red carry marks.</font>\n\n\n#### Points Distribution - Maximum Points: 25\n\n\n<div align=\"center\">\n    <table>\n        <tr><td><h3>Number</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n        <tr><td><h3>1</h3></td> <td><h3>Explore Dataset</h3></td> <td><h3>6</h3></td> </tr>\n        <tr><td><h3>2</h3></td> <td><h3>Data Preparation</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>3</h3></td> <td><h3>Configurations</h3></td> <td><h3>4</h3></td> </tr>\n        <tr><td><h3>4</h3></td> <td><h3>Display Mistakes</h3></td><td><h3>10</h3></td> </tr>\n    </table>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# <font style=\"color:blue\">Step 1: Understand Your Problem </font>\n\nAs you already know, Image Classification helps classify an image based on its visual content. So, the model is supposed to look at the given image and predict which object is present in it. Obviously, the objects which it can predict depends on the objects you have trained it on.\n\n\nIn our problem, we want to classify an input image by distinguishing among  **4 animals - cow, elephant, horse and spider**. \n\n\n### <font style=\"color:green\">What Do We Need and How to Achieve It? </font>\n\n1. You need correctly-labeled images of each animal.\n2. Also, you need to train a network to understand the input image.","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nfrom PIL import Image\nimport tensorflow as tf\nfrom tensorflow.keras.utils import image_dataset_from_directory\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\nfrom dataclasses import dataclass\nimport platform\n\n# Text formatting\nbold = \"\\033[1m\"\nend = \"\\033[0m\"\n\nblock_plot=False\n\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font style=\"color:green\">Set Seeds</font>\n\nUse this function to set random seeds for CPU and GPU operations in order to make training deterministic and to ensure reproducibility.","metadata":{}},{"cell_type":"code","source":"def set_seeds():\n    # fix random seeds\n    SEED_VALUE = 42\n\n    random.seed(SEED_VALUE)\n    np.random.seed(SEED_VALUE)\n    tf.random.set_seed(SEED_VALUE)\n    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n    \n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        try:\n            # Currently, memory growth needs to be the same across GPUs\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n        except RuntimeError as e:\n            # Memory growth must be set before GPUs have been initialized\n            print(e)\n    \n#     physical_devices = tf.config.list_physical_devices(\"GPU\")\n#     try:\n#         tf.config.experimental.set_memory_growth(physical_devices[0], True)\n#     except:\n#         # Invalid device or cannot modify virtual devices once initialized.\n#         pass\n\n    return\n\nset_seeds()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a MirroredStrategy for distributed training.\n# This strategy effectively replicates the model's layers on each GPU or other available devices,\n# syncing their weights after each training step.\nDISTRIBUTE_STRATEGY = tf.distribute.MirroredStrategy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing the number of devices that are in sync with the MirroredStrategy.\n# This indicates how many replicas of the model are being trained in parallel.\nprint('Number of devices: {}'.format(DISTRIBUTE_STRATEGY.num_replicas_in_sync))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:blue\">Step 2: Dataset Exploration</font>","metadata":{}},{"cell_type":"markdown","source":"## <font style=\"color:blue\">Step 2A: Get the Data </font>\n\nLet’s use a dataset from kaggle. [**Check it out here**](https://www.kaggle.com/c/opencv-tensorflow-course-classication-project-1/data).\n\nWe have already separated the dataset into training, validation and test splits for you.","metadata":{}},{"cell_type":"markdown","source":"## <font style=\"color:blue\">Step 2B: Explore & Understand your data [6 Points]</font>\n","metadata":{}},{"cell_type":"markdown","source":"**Set Data Paths**","metadata":{}},{"cell_type":"code","source":"# If required, update the root_dir path according to the dataset path.\n\nroot_dir = r\"/kaggle/input/opencv-tf-project-1-image-classification-round-3/dataset\"\n\ntrain_dir = os.path.join(root_dir, \"Train\")\nvalid_dir = os.path.join(root_dir, \"Valid\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font style=\"color:green\">Folder Structure </font>","metadata":{}},{"cell_type":"code","source":"def list_folders(startpath):\n    for root, _, files in os.walk(startpath):\n        level = root.replace(startpath, '').count(os.sep)\n        indent = ' ' * 4 * (level)\n        print(f'{indent}{os.path.basename(root):<8}')\n\n\nlist_folders(root_dir)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font style=\"color:green\">List the Classes</font>\n\nIt simply prints the sub folders present under the training or validation folder.","metadata":{}},{"cell_type":"code","source":"print(f\"{bold}Training Classes:{end} \")\nfor i in os.listdir(train_dir):\n    print(i)\n    \nprint(\"------------\")\n\nprint(f\"{bold}Validation Classes:{end} \")\nfor j in os.listdir(valid_dir):\n    print(j)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font style=\"color:red\">Find the Number of Samples in Training and Validation Folders [2 Points]</font> ","metadata":{}},{"cell_type":"code","source":"num_train_files = 0\nnum_valid_files = 0\n\n### YOUR CODE HERE\n\n###\n\nprint(f\"{bold}Number of Training samples: {end}{num_train_files}\")\nprint(f\"{bold}Number of Validation samples: {end}{num_valid_files}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font style=\"color:red\">Display Some Samples and the Size of the Image[4 Points]</font>\n\n**Display one sample for each class**\n\nChoose randomly one sample from each class folder in the `Train` directory.\n\n1. Load image using `PIL.Image` or `matplotlib` or `opencv`.\n2. Print size/shape of the image.\n3. Display image file using matplotlib.","metadata":{}},{"cell_type":"markdown","source":"**A sample output:**\n\n<img src=\"https://learnopencv.com/wp-content/uploads/2022/05/c4-pa1-data_pipeline_check-sample_image.png\" width=\"40%\">","metadata":{}},{"cell_type":"code","source":"target = \"cow\"\n\n# Load image and print its shape/size.\n\n### YOUR CODE HERE\n\n###","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = \"elephant\"\n\n# Load image and print its shape/size.\n\n### YOUR CODE HERE\n\n###","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = \"horse\"\n\n# Load image and print its shape/size.\n\n### YOUR CODE HERE\n\n###","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = \"spider\"\n\n# Load image and print its shape/size.\n\n### YOUR CODE HERE\n\n###","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:red\">Step 3. Data Preparation [5 Points]</font>\n\n\nNow that you have seen how the data is organized, it's time to configure the train and valid generators to feed to the training pipeline.\n\n* For this notebook, you have to create a `data_augmentation_preprocess()` function that returns a `Sequential model` made up of different augmentation layers.\n* The augmentation pipeline returned from this function will be applied using the `Dataset.map()` method as part of data loading.\n* Using this approach, the dataset will yield batches of augmented images.\n\n* Starting from the **Training from Scratch** notebook, you can use:\n    * Option 1: [Make the preprocessing layers part of your model.](https://www.tensorflow.org/tutorials/images/data_augmentation#option_1_make_the_preprocessing_layers_part_of_your_model)\n    * Option 2: [Apply the preprocessing layers to your dataset.](https://www.tensorflow.org/tutorials/images/data_augmentation#option_2_apply_the_preprocessing_layers_to_your_dataset)","metadata":{}},{"cell_type":"markdown","source":"### <font style=\"color:red\">Task [3 points]</font>\n\n**Create a function `data_augmentation_preprocess()` to apply augmentations.**\n\n\n```python\ndef data_augmentation_preprocess():\n    # Combine multiple augmentations in a single processing pipeline.\n    data_augmentation_pipeline = ...\n    \n    return data_augmentation_pipeline\n```\n\n**Reference:** https://www.tensorflow.org/tutorials/images/data_augmentation#data_augmentation_2","metadata":{}},{"cell_type":"code","source":"### YOUR CODE HERE\n\n###","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font style=\"color:red\">Question [2 points]</font>\n\n**Spot and correct 2 mistakes present in the `get_data` function.**","metadata":{}},{"cell_type":"code","source":"def get_data(*, data_root, target_size=(224, 224), batch_size=32, data_augmentation=False):\n    \n    train_path = os.path.join(data_root, \"Train\")\n    train_dataset = image_dataset_from_directory(\n        train_path, \n        label_mode='categorical',\n        color_mode='rgb', \n        batch_size=batch_size, \n        image_size=target_size, \n        shuffle=True,\n    )\n\n    valid_path = os.path.join(data_root, \"Valid\")    \n    valid_dataset = image_dataset_from_directory(\n        valid_path, \n        label_mode='categorical',\n        color_mode='rgb', \n        batch_size=batch_size, \n        image_size=target_size, \n        shuffle=False, \n    )\n    \n    if data_augmentation: \n        data_augmentation_pipeline = data_augmentation_preprocess()\n        train_dataset = train_dataset.map(lambda x, y: (data_augmentation_pipeline(x), y))\n        valid_dataset = valid_dataset.map(lambda x, y: (data_augmentation_pipeline(x), y))\n            \n        \n    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)    \n    valid_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n    \n    return train_dataset, valid_dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:blue\">Step 4: Train a Simple Model</font>\n\n* It’s time to create the training pipeline and train a simple model (already provided) using the dataset provided. \n* We also provide most of the code in this section.","metadata":{}},{"cell_type":"markdown","source":"## <font style=\"color:red\">4.1. Configurations [4 Points]</font>\n\nIn this section, define the training configurations. Also, specify the batch size, data path, learning rate etc.\n\n\n### <font style=\"color:red\">Question [4 Points]</font>\n\n\nChange the configurations as given below:\n\n1. Set up the training pipeline with a batch size of `32`.\n2. Run the experiment then for `30` epochs. \n3. Change the learning rate to `0.01`.\n3. Use a data shape of `(224, 224, 3)` for training.","metadata":{}},{"cell_type":"markdown","source":"### <font style=\"color:green\">4.1.1. Training Configuration</font>","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass TrainingConfig:\n    # Defining the batch size for model training.\n    # The batch size is set to be some integer times the  number of devices in synchronization as per the distributed strategy.\n    # This means that the overall batch of data is divided equally across all the devices used in the distributed training.\n    # By scaling the batch size with the number of replicas (devices), each device processes a batch of size, in this case, 4.\n   \n    # This approach helps in efficient utilization of the computational power of all the devices involved in training.\n    BATCH_SIZE: int = 4 * DISTRIBUTE_STRATEGY.num_replicas_in_sync\n\n    EPOCHS: int = 2\n    LEARNING_RATE: float = 0.1\n\n    # For tensorboard logging and saving checkpoints\n    root_log_dir = os.path.join(\"Logs_Checkpoints\", \"Model_logs\")\n    root_checkpoint_dir = os.path.join(\"Logs_Checkpoints\", \"Model_checkpoints\")\n\n    # Current log and checkpoint directory.\n    log_dir = \"version_0\"\n    checkpoint_initial = \"version_0\"\n\n    # Use multiprocessing during training.\n    use_multiprocessing: bool = True if platform.system() == \"Linux\" else False\n        \n    # Number of workers to use for training.\n    num_workers: int = 4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font style=\"color:green\">4.1.2. Dataset Configuration</font>","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass DatasetConfig:\n    DATA_ROOT: str = r\"/kaggle/input/opencv-tf-project-1-image-classification-round-3/dataset\"\n    DATA_SHAPE: tuple = (128, 256, 3)\n    NUM_CLASSES: int = 4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:blue\">4.2. Callbacks</font>\n\n\nLet’s define the different callbacks we'll be using during training.","metadata":{}},{"cell_type":"code","source":"def get_callbacks(\n    training_config=TrainingConfig(),\n    monitor=\"val_loss\",\n    mode=\"min\",\n    save_weights_only=False,\n    save_best_only=True,\n):\n\n    # Initialize tensorboard callback for logging.\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n        log_dir=training_config.log_dir,\n        histogram_freq=20,\n        write_graph=True,\n        update_freq=\"epoch\",\n        write_images=True,\n    )\n\n    \n    checkpoint_filepath = training_config.checkpoint_path\n    \n    # Update file path if saving best model weights.\n    if save_weights_only:\n        checkpoint_filepath = os.path.join(checkpoint_filepath, \"model.ckpt\")\n    else:\n        checkpoint_filepath = os.path.join(checkpoint_filepath, \"model.keras\")\n\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n        filepath=checkpoint_filepath,\n        save_weights_only=save_weights_only,\n        monitor=monitor,\n        mode=mode,\n        save_best_only=save_best_only,\n        verbose=0,\n    )\n    \n    return [tensorboard_callback, model_checkpoint_callback]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:blue\">4.3. Logging Setup</font>\n\nThis function will be initializing directories so that they save tensorboard and model checkpoints for different training versions.","metadata":{}},{"cell_type":"code","source":"def setup_log_directory(training_config=TrainingConfig()):\n    '''Tensorboard Log, Model checkpoint directory Setup and Early stopping'''\n    \n    get_number = lambda path: path.replace(\".keras\", \"\").replace(\"version_\", \"\")\n    \n    if os.path.isdir(training_config.root_log_dir):\n        # Get all folders numbers in the root_log_dir\n        folder_numbers = [int(get_number(folder)) for folder in os.listdir(training_config.root_log_dir)]\n        \n        # Find the latest version number present in the log_dir\n        last_version_number = max(folder_numbers)\n\n        # New version name\n        version_name = f\"version_{last_version_number + 1}\"\n\n    else:\n        version_name = training_config.log_dir\n\n\n    # Update the training config default directory \n    training_config.log_dir        = os.path.join(training_config.root_log_dir,        version_name)\n    training_config.checkpoint_path = os.path.join(training_config.root_checkpoint_dir, version_name)\n\n    # Create new directory for saving new experiment version\n    os.makedirs(training_config.log_dir, exist_ok=True)\n    os.makedirs(training_config.root_checkpoint_dir, exist_ok=True)\n\n    print(f\"Logging at: {training_config.log_dir}\")\n    print(f\"Model Checkpoint at: {training_config.checkpoint_path}\")\n    \n    return training_config, version_name","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:blue\">4.4. Plot Loss and Accuracy</font>\n\nUse this function to plot loss and accuracy for training and validation results.","metadata":{}},{"cell_type":"code","source":"def plot_history(\n    train_loss=None,\n    val_loss=None,\n    train_metric=None,\n    val_metric=None,\n    colors=[\"blue\", \"green\"],\n    loss_legend_loc=\"upper center\",\n    acc_legend_loc=\"upper left\",\n    fig_size=(15, 10),\n):\n\n    plt.rcParams[\"figure.figsize\"] = fig_size\n    fig = plt.figure()\n    fig.set_facecolor(\"white\")\n\n    # Loss Plots\n    plt.subplot(2, 1, 1)\n\n    train_loss_range = range(len(train_loss))\n    plt.plot(\n        train_loss_range,\n        train_loss,\n        color=f\"tab:{colors[0]}\",\n        label=f\"Train Loss\",\n    )\n\n    valid_loss_range = range(len(val_loss))\n    plt.plot(\n        valid_loss_range,\n        val_loss,\n        color=f\"tab:{colors[1]}\",\n        label=f\"Valid Loss\",\n    )\n\n    plt.ylabel(\"Loss\")\n    plt.legend(loc=loss_legend_loc)\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.grid(True)\n    plt.title(\"Training and Validation Loss\")\n\n    # Accuracy Plots\n    plt.subplot(2, 1, 2)\n\n    train_metric_range = range(len(train_metric))\n    plt.plot(\n        train_metric_range,\n        train_metric,\n        color=f\"tab:{colors[0]}\",\n        label=f\"Train Accuracy\",\n    )\n\n    val_metric_range = range(len(val_metric))\n    plt.plot(\n        val_metric_range,\n        val_metric,\n        color=f\"tab:{colors[1]}\",\n        label=f\"Valid Accuracy\",\n    )\n\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.legend(loc=acc_legend_loc)\n    plt.grid(True)\n    plt.title(\"Training and Validation Accuracy\")\n\n    plt.show(block=block_plot)\n\n    return","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:blue\">4.5. Define Model</font>\n\nNow, we will define a dummy CNN model with the Rescaling layer and train it.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import Model, Input\nfrom tensorflow.keras.layers import (\n    Conv2D,\n    BatchNormalization,\n    MaxPooling2D,\n    Flatten,\n    Dense,\n    GlobalAveragePooling2D,\n    Rescaling,\n)\n\n\ndef get_model(num_classes=4, input_shape=(224, 224, 3), name=\"Dummy_Model\"):\n\n    inputs = Input(shape=input_shape)\n    \n    rescaled = Rescaling(1./255)(inputs)\n    \n    conv1 = Conv2D(8, 3, activation=\"relu\")(rescaled)\n    bn1   = BatchNormalization()(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(bn1)\n\n    conv2 = Conv2D(16, 3, activation=\"relu\")(pool1)\n    bn2 = BatchNormalization()(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(bn2)\n\n    conv3 = Conv2D(32, 3, activation=\"relu\")(pool2)\n    bn3 = BatchNormalization()(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(bn3)\n        \n    conv4 = Conv2D(64, 3, activation=\"relu\")(pool3)\n    bn4 = BatchNormalization()(conv4)\n    conv_out = MaxPooling2D(pool_size=(2, 2))(bn4)\n\n    flatten = Flatten()(conv_out)\n    output = Dense(num_classes, activation=\"softmax\")(flatten)\n\n    return Model(inputs=inputs, outputs=output, name=name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:blue\">4.6. Training Pipeline</font>\n\n\nIn this function, we bring together all the different functions we've defined so far.","metadata":{}},{"cell_type":"code","source":"def train_model(\n    dataset_config=DatasetConfig(),\n    training_config=TrainingConfig(),\n    data_augmentation=True,\n    print_summary=True,\n):\n\n    # Get training and validation datasets.\n    train_dataset, valid_dataset = get_data(\n        data_root=dataset_config.DATA_ROOT,\n        target_size=dataset_config.DATA_SHAPE[:2],\n        batch_size=training_config.BATCH_SIZE,\n        data_augmentation=data_augmentation,\n    )\n    \n    for images, labels in valid_dataset:\n        print(\"X Shape:\", images.shape, \"Y Shape:\", labels.shape)\n        break\n        \n\n    # # Get model\n    # model = get_model(num_classes=dataset_config.NUM_CLASSES, input_shape=dataset_config.DATA_SHAPE)\n\n    # # Initialize optimizer\n    # optimizer = tf.keras.optimizers.Adam(learning_rate=training_config.LEARNING_RATE)\n\n    # # Compile model\n    # model.compile(\n    #     loss=\"categorical_crossentropy\",\n    #     optimizer=optimizer,\n    #     metrics=[\"accuracy\"],\n    # )\n\n    # Start a context manager using the distributed strategy previously defined.\n    # This scope ensures that the operations defined within it are distributed across the available devices as per the strategy.\n    with DISTRIBUTE_STRATEGY.scope():\n        # Get the model by calling the 'get_model' function.\n        model = get_model(num_classes=dataset_config.NUM_CLASSES, input_shape=dataset_config.DATA_SHAPE)\n\n        # Compile the model. This step configures the model for training.\n        # 'loss' is set to 'categorical_crossentropy', which is a common choice for classification tasks.\n        # 'optimizer' is an Adam optimizer with a specific learning rate from the training configuration.\n        # 'metrics' is a list of metrics to be evaluated by the model during training and testing, here it's set to track 'accuracy'.\n        model.compile(\n            loss=\"categorical_crossentropy\",\n            optimizer=tf.keras.optimizers.Adam(learning_rate=training_config.LEARNING_RATE),\n            metrics=[\"accuracy\"],\n        )\n\n\n    # Print model summary\n    if print_summary:\n        model.summary()\n\n    # Get training callbacks\n    callbacks = get_callbacks(training_config)\n\n    # Train model\n    training_results = model.fit(\n        train_dataset,\n        validation_data=valid_dataset,\n        epochs=training_config.EPOCHS,\n        callbacks=callbacks,\n        workers=training_config.num_workers,\n        use_multiprocessing=training_config.use_multiprocessing\n    )\n\n    print(\"training_results keys:\", training_results.history.keys())\n\n    return model, training_results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:blue\">4.7. Training</font>","metadata":{}},{"cell_type":"code","source":"training_config = TrainingConfig()\n\n# Tensorboard Log and model checkpoint Setup.\ntraining_config, current_version_name = setup_log_directory(training_config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start training\n\ntrained_model, training_results = train_model(\n    dataset_config=DatasetConfig(),\n    training_config=training_config,\n    data_augmentation=False,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:blue\">4.8. Loss and Accuracy Plot</font>","metadata":{}},{"cell_type":"code","source":"loss = training_results.history[\"loss\"]\naccuracy = training_results.history[\"accuracy\"]\n\nval_loss = training_results.history[\"val_loss\"]\nval_accuracy = training_results.history[\"val_accuracy\"]\n\nplot_history(\n    train_loss=loss,\n    train_metric=accuracy,\n    val_loss=val_loss,\n    val_metric=val_accuracy,\n    loss_legend_loc=\"upper center\",\n    acc_legend_loc=\"upper left\",\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:blue\">Step 5: Sample Prediction</font>\n\nShow some sample predictions.","metadata":{}},{"cell_type":"markdown","source":"## <font style=\"color:blue\">5.1. Reload the Model</font>\n\nA function to help reload the model or the weights saved during training.","metadata":{}},{"cell_type":"code","source":"def recreate_model(path, load_weights=False, config=DatasetConfig(), **kwargs):\n    \"\"\"\n    For reloading trained model\n    Args:\n        path: current_version checkpoint path\n        load_weights: Whether the files saved at path contains model weights\n                      or the Entire model configuration\n        **kwargs: use to pass additional keyword arguments to the load_model method\n    Returns:\n        Reinitialized Trained Model\n    \"\"\"\n\n    if load_weights:\n        \n        # Create model architecture\n        model = get_model(num_classes=config.NUM_CLASSES, input_shape=config.DATA_SHAPE)\n\n        # Load trained model best weights.\n        path = os.path.join(path, \"model.ckpt\")\n        model.load_weights(path).expect_partial()\n    else:\n        path = os.path.join(path, \"model.keras\")\n        model = tf.keras.models.load_model(path, **kwargs)\n\n    model.summary()\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font style=\"color:green\">Loading trained model</font>","metadata":{}},{"cell_type":"code","source":"# Loading trained model\n\nload_version = current_version_name\ncheckpoint_path = os.path.join(training_config.root_checkpoint_dir, load_version) \n\ntrained_model = recreate_model(checkpoint_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:blue\">5.2. Running Inference</font>\n\nWe’ll be running dummy inference on validation data, and displaying 15 images.","metadata":{}},{"cell_type":"code","source":"def get_sample_predictions(*, model, dataset, total=15):\n\n    imgs = []\n    ground_truths = []\n    probs = []\n    predictions = []\n\n    idx_to_cls = {0: \"cow\", 1: \"elephant\", 2: \"horse\", 3: \"spider\"}\n\n    print(\"Generating Predictions...\")\n    for data, target in dataset:\n        model_predictions = model.predict_on_batch(data)\n        cls_predicted = np.argmax(model_predictions, axis=-1)\n        cls_probs = np.max(model_predictions, axis=-1)\n    \n        imgs.extend(data.numpy() / 255.)\n        ground_truths.extend(target.numpy())\n        predictions.extend(cls_predicted)\n        probs.extend(cls_probs)\n        \n        # Displaying only 15 images \n        if data.shape[0] >= total: \n            break\n\n    plt.style.use(\"default\")\n    plt.rcParams[\"figure.figsize\"] = (18, 9)\n    fig = plt.figure()\n    fig.set_facecolor(\"white\")\n\n    for idx in range(total):\n\n        plt.subplot(3, 5, idx + 1)\n        img = imgs[idx]\n        plt.imshow(img)\n\n        plt.title(f\"P:{idx_to_cls[predictions[idx]]}({probs[idx]:.2}), T:{idx_to_cls[ground_truths[idx]]}\")\n        plt.axis(\"off\")\n\n    fig.savefig(\"sample_predictions.png\")\n    plt.show(block=block_plot)\n    \n    del imgs, ground_truths, probs, predictions\n    return","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_config = DatasetConfig()\ntraining_config = TrainingConfig()\n\nvalid_path = os.path.join(data_config.DATA_ROOT, \"Valid\")\n\nvalid_dataset = tf.keras.utils.image_dataset_from_directory(\n    valid_path, \n    label_mode='int',\n    color_mode='rgb', \n    batch_size=training_config.BATCH_SIZE, \n    image_size=data_config.DATA_SHAPE[:2], \n    shuffle=True, # shuffling to show images from all classes\n)\n\nget_sample_predictions(model=trained_model, dataset=valid_dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:red\">Step 6. Displaying Mistakes [10 points]</font>\n\nThe above code displayed sample predictions. But correct predictions are of no use. So, write a similar function such that it displays only the mistakes made by the network.\n\n\n**You have to display 10 images wrongly predicted from the Validation set.**","metadata":{}},{"cell_type":"code","source":"### YOUR CODE HERE\n\n###","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Upon completing the project, <font style=\"color:red\">upload the notebook to the lab for grading and feedback.</font>**","metadata":{}},{"cell_type":"markdown","source":"**<font style=\"color:red\">Please do not make your notebooks public or publish them on the competition page. You only need to submit your notebook to the lab. This is to make sure that students don't copy each other.</font>**","metadata":{}}]}